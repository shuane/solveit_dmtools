# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_dhb.ipynb.

# %% auto 0
__all__ = ['doc', 'c', 'BackupChat']

# %% ../nbs/01_dhb.ipynb 1
doc = """**Backup Chat for SolveIt using dialoghelper and lisette**

Sometimes we may have a problem in SolveIt while Sonnet is down (E300), or maybe we want a different perspective.

This module helps us to leverage any other LLM that is available to LiteLLM by providing our own keys and the model name.

Usage: 
```python
from solveit_dmtools import dhb

# then in another cell
# bc = dhb.c() to search model names
bc = dhb.c("model-name")
bc("Hi")
```
"""
import json
import re
from dialoghelper.core import *
from lisette import *
from typing import Optional, Union
from ipykernel_helper import read_url
import inspect

class BackupChat(Chat):
    models = None
    vars_for_hist = None
    model = None

    def __init__(self,
                model: str = None,
                sp='',
                temp=0,
                search=False,
                tools: list = None,
                hist: list = None,
                ns: Optional[dict] = None,
                cache=False,
                cache_idxs: list = [-1],
                ttl=None,
                var_names: Union[list,str] = None,
                hide_msg:bool=False, # whether to hide the cell that includes a BackupChat.__call__
    ):
        if sp is None or sp == '':
            sp = """You're continuing a conversation from another session. Variables are marked as $`varname` and tools as &`toolname` in the context.

If you see references to variables or tools that might be relevant to your answer but aren't fully available, ask the user to indicate which ones they want to include by calling e.g their `bc.add_vars`, `bc.add_tools`, or `bc.add_vars_and_tools` methods (if they called their chat instance `bc`). Note that these 3 methods each take a list of names or a string containing space-delimited names.

Tool results from the earlier conversation may be truncated to about 100 characters. If you need complete information, you should ask the user to run the tool and store results in a variable then make that variable available using the chat object's add_vars method. You already have access to the read_url tool, but do confirm if you can read the URLs once because it may be expensive to access them.

You are not able to run other code so you cannot store your own variables or do that for me, instead you should give Python in fenced markdown in your responses. If giving code examples or similar, remember to use fenced markdown too.

Use a Socratic approach - guide through questions rather than direct answers - unless the user explicitly asks you to do something differently."""
        if self.models is None:
            self.models = self.get_litellm_models()
        if model is None:
            _m1 = input("Please enter part of a model name to pick your model. Remember you also need to have secret for their API key already defined in your secrets:")
            print("Please try again by using e.g. `bc = dhb.c('model_name')` with a model name in:")
            print('\n'.join([m for m in self.models if _m1 in m]))
            return None
        if model not in self.models:
            raise ValueError(f"Model {model} not found in LiteLLM models. Please check the model name or use a different model.")
        self.model = model
        self.hide_msg = hide_msg
        self.vars_for_hist = dict()
        if var_names is not None:
            self.add_vars(var_names)
        if tools is None:
            tools = [read_url]
        if ns is None:
            ns = inspect.currentframe().f_back.f_globals
        super().__init__(model=model, sp=sp, temp=temp, search=search, tools=tools, hist=hist, ns=ns, cache=cache, cache_idxs=cache_idxs, ttl=ttl)

    def get_litellm_models(self):
        url = "https://raw.githubusercontent.com/BerriAI/litellm/refs/heads/main/model_prices_and_context_window.json"
        data = read_url(url, as_md=False)
        models = json.loads(data)
        return [k for k in models.keys() if k != 'sample_spec']
    
    def __call__(self, 
                msg=None,
                prefill=None,
                temp=None,
                think=None,
                search=None,
                stream=False,
                max_steps=2,
                final_prompt='You have no more tool uses. Please summarize your findings. If you did not complete your goal please tell the user what further work needs to be done so they can choose how best to proceed.',
                return_all=False,
                var_names=None, # list of variable names to add to the chat
                **kwargs,
                ):
        msgs = [m for m in find_msgs() if m['pinned'] or not m['skipped']]
        last_msg = read_msg(-1)['msg']
        curr_msg = read_msg(0)['msg']
        if var_names: self.add_vars(var_names)
        self.hist = self._build_hist(msgs, last_msg=last_msg)
        start = len(self.hist)
        update_msg(msgid=curr_msg['id'], content="# " + curr_msg['content'].replace('\n', '\n# '), skipped=self.hide_msg, o_collapsed=1)
        response = super().__call__(msg=msg, prefill=prefill, temp=temp, think=think, search=search, stream=stream, max_steps=max_steps, final_prompt=final_prompt, return_all=return_all, **kwargs)
        output = self._new_msgs_to_output(start)
        add_msg(content=f"**Prompt ({self.model}):** {msg}", output=output, msg_type='prompt')
        return response

    def _build_hist(self, msgs:list, last_msg=None):
        if last_msg is None: curr = len(msgs)
        else:
            try: curr = next(i for i,m in enumerate(msgs) if m['id'] == last_msg['id'])
            except StopIteration: curr = len(msgs)
        hist = []
        for m in msgs[:curr+1]:
            eol = '\n'
            if m['msg_type'] == 'code': hist.append({'role': 'user', 'content': f"```python{eol}{m['content']}{eol}```{eol}Output: {m.get('output', '[]')}"})
            elif m['msg_type'] == 'note' or m['msg_type'] == 'raw': hist.append({'role': 'user', 'content': m['content']})
            elif m['msg_type'] == 'prompt':
                hist.append({'role': 'user', 'content': m['content']})
                if m.get('output'): hist.append({'role': 'assistant', 'content': m['output']})
        
        hist = hist + self._vars_as_msg() + [{'role': 'assistant', 'content': '.'}] # empty assistant msg to prevent flipping chat msg to look like prefill
        return hist

    def _vars_as_msg(self):
        if self.vars_for_hist and len(self.vars_for_hist.keys()):
            content = "Here are the requested variables:\n" + json.dumps(self.vars_for_hist)
            return [{'role': 'user', 'content': content}]
        else:
            return []

    def _new_msgs_to_output(self, start):
        new_msgs = self.hist[start+1:]
        parts = []
        for i, m in enumerate(new_msgs):
            if m.get('role') == 'assistant' and m.get('tool_calls'):
                for tc in m['tool_calls']:
                    result_msg = next((r for r in new_msgs if r.get('tool_call_id') == tc['id']), None)
                    if result_msg: parts.append(self._format_tool_details(tc['id'], tc['function']['name'], json.loads(tc['function']['arguments']), result_msg['content'], is_last_msg=(i == len(new_msgs)-1)))
            elif m.get('role') == 'assistant' and m.get('content'):
                content = m['content']
                if 'You have no more tool uses' not in content: parts.append(content)
        return '\n\n'.join(parts)
    
    def _trunc_tool_result(self, result, max_len=100, is_last_msg=False):
        if len(str(result)) <= max_len or is_last_msg: return result
        return str(result)[:max_len] + '<TRUNCATED>'
    
    def _format_tool_details(self, tool_id, func_name, args, result, is_last_msg=False):
        result_str = self._trunc_tool_result(result)
        tool_json = json.dumps({"id": tool_id, "call": {"function": func_name, "arguments": args}, "result": result_str}, indent=2)
        return f"<details class='tool-usage-details'>\n\n```json\n{tool_json}\n```\n\n</details>"    
    
    def add_vars(self, var_names:Union[list,str]=None):
        "Add variables to conversation as user message"
        if isinstance(var_names, str):
            var_names = var_names.split()
        if not isinstance(var_names, list):
            raise ValueError(f"var_names must be a string or list of strings, not {type(var_names)}")
        
        # Add each var to the self.vars_for_hist dictionary
        for v in var_names:
            self.vars_for_hist[v.strip()] = globals().get(v.strip(), 'NOT AVAILABLE')
    
    def add_tools(self, tool_names:Union[list,str]=None):
        "Add tools to the chat's tool list"
        if isinstance(tool_names, str):
            tool_names = tool_names.split()
        tools = [self.ns.get(t) for t in tool_names if self.ns.get(t)]
        self.tools = list(self.tools or []) + tools
    
    def add_vars_and_tools(self, var_names:Union[list,str]=None, tool_names:Union[list,str]=None):
        "Add both variables and tools to the chat's lists"
        self.add_tools(tool_names)
        self.add_vars(var_names)
                
c = BackupChat
