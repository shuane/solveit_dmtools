# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_dhb.ipynb.

# %% auto 0
__all__ = ['doc', 'c', 'BackupChat']

# %% ../nbs/01_dhb.ipynb 1
doc = """**Backup Chat for SolveIt using dialoghelper and lisette**

Sometimes we may have a problem in SolveIt while Sonnet is down (E300), or maybe we want a different perspective.

This module helps us to leverage any other LLM that is available to LiteLLM by providing our own keys and the model name.

Usage: 
```python
from solveit_dmtools import dhb

# then in another cell
# bc = dhb.c() to search model names
bc = dhb.c("model-name")
# then in another cell
bc("Hi")
```
"""

# %% ../nbs/01_dhb.ipynb 2
import json
import re
from dialoghelper.core import *
from lisette import *
from typing import Optional, Union
from ipykernel_helper import read_url
import inspect
from fastcore.tools import patch

class BackupChat(Chat):
    models = None
    vars_for_hist = None
    model = None

    def __init__(self,
                model: str = None,
                sp='',
                temp=0,
                search=False,
                tools: list = None,
                hist: list = None,
                ns: Optional[dict] = None,
                cache=False,
                cache_idxs: list = [-1],
                ttl=None,
                var_names: Union[list,str] = None,
                hide_msg:bool=False, # whether to hide the cell that includes a BackupChat.__call__
    ):
        if sp is None or sp == '':
            sp = """You're continuing a conversation from another session. Variables are marked as $`varname` and tools as &`toolname` in the context.

**Available Resources**

If you see references to variables or tools that might be relevant but aren't fully available, ask the user which ones they want to include by calling their `bc.add_vars`, `bc.add_tools`, or `bc.add_vars_and_tools` methods (if they called their chat instance `bc`). These methods accept either a list of names or a space-delimited string.

**Tool Usage Notes**

- Tool results from earlier conversations may be truncated to ~100 characters. If you need complete information, ask the user to run the tool and store results in a variable, then make that variable available using `bc.add_vars`.
- You have access to the `read_url` tool, but confirm before reading URLs as access may be expensive.

**Code Execution**

You cannot run code yourself or store variables. Instead, provide Python code in fenced markdown blocks. The user can execute these in their environment.

**Teaching Approach**

Use a Socratic method - guide through questions rather than providing direct answers - unless the user explicitly requests otherwise. When providing code examples:

- Keep code snippets brief (1-3 lines maximum) unless the user explicitly asks you to write more
- Encourage the user to implement solutions themselves
- Ask clarifying questions about their expertise and goals to customize your responses
"""
        if self.models is None:
            self.models = self.get_litellm_models()
        if model is None:
            _m1 = input("Please enter part of a model name to pick your model. Remember you also need to have secret for their API key already defined in your secrets:")
            print(f"Please try again by using e.g. `bc = dhb.c('model_name')` with a model name e.g. pick from these found by searching for '{_m1}':")
            # search case-insensitively and return models that match
            print('\n'.join([m for m in self.models if _m1.lower() in m.lower() or '###' in m]))
            return None
        if model not in self.models:
            raise ValueError(f"Model {model} not found in LiteLLM models. Please check the model name or use a different model.")
        self.model = model
        self.hide_msg = hide_msg
        self.vars_for_hist = dict()
        if var_names is not None:
            self.add_vars(var_names)
        if tools is None:
            tools = [read_url]
        if ns is None:
            ns = inspect.currentframe().f_back.f_globals
        super().__init__(model=model, sp=sp, temp=temp, search=search, tools=tools, hist=hist, ns=ns, cache=cache, cache_idxs=cache_idxs, ttl=ttl)

    def get_openrouter_ignored(self):
        url = "https://raw.githubusercontent.com/cheahjs/free-llm-api-resources/refs/heads/main/src/data.py"
        code = read_url(url, as_md=False)
        
        # Find the OPENROUTER_IGNORED_MODELS set definition
        pattern = r'OPENROUTER_IGNORED_MODELS\s*=\s*\{([^}]+)\}'
        match = re.search(pattern, code, re.DOTALL)
        models = []
        
        if match:
            # Extract the content and parse the strings
            content = match.group(1)
            models = re.findall(r'"([^"]+)"', content)
        return list(models)
    
    def fetch_openrouter_models(self, already_listed:list=None):
        r = read_url("https://openrouter.ai/api/v1/models", as_md=False)
        models = json.loads(r)['data']
        ignored_models = self.get_openrouter_ignored()
        ret_models = []
        for model in models:
            pricing = float(model.get("pricing", {}).get("completion", "1")) + float(
                model.get("pricing", {}).get("prompt", "1")
            )
            if pricing != 0 or ":free" not in model["id"] or model["id"].lower() in [im.lower() for im in ignored_models]:
                continue
            if not (already_listed and model["id"].lower() in [al.replace('openrouter/', '').lower() for al in already_listed]):
                ret_models.append(
                    {
                        "id": f"openrouter/{model['id']}",
                        "limits": {
                            "requests/minute": 20,
                            "requests/day": 50,
                        },
                    }
                )
        return ret_models
    
    def get_litellm_models(self):
        url = "https://raw.githubusercontent.com/BerriAI/litellm/refs/heads/main/model_prices_and_context_window.json"
        data = read_url(url, as_md=False)
        models = json.loads(data)
        already_listed = [k for k in models.keys() if k != 'sample_spec']
        return already_listed + [f"### The following ones are listed by OpenRouter but not LiteLLM (may still work)"] + sorted([orm['id'] for orm in self.fetch_openrouter_models(already_listed)])
   
    def add_vars(self, var_names:Union[list,str]=None):
        "Add variables to conversation as user message"
        if isinstance(var_names, str):
            var_names = var_names.split()
        if not isinstance(var_names, list):
            raise ValueError(f"var_names must be a string or list of strings, not {type(var_names)}")
        
        # Add each var to the self.vars_for_hist dictionary
        for v in var_names:
            self.vars_for_hist[v.strip()] = self.ns.get(v.strip(), 'NOT AVAILABLE')

# %% ../nbs/01_dhb.ipynb 3
@patch
def __call__(self:BackupChat, 
            msg=None,
            prefill=None,
            temp=None,
            think=None,
            search=None,
            stream=False,
            max_steps=2,
            final_prompt='You have no more tool uses. Please summarize your findings. If you did not complete your goal please tell the user what further work needs to be done so they can choose how best to proceed.',
            return_all=False,
            var_names=None, # list of variable names to add to the chat
            **kwargs,
            ):
    msgs = [m for m in find_msgs() if m['pinned'] or not m['skipped']]
    last_msg = read_msg(-1)['msg']
    curr_msg = read_msg(0)['msg']
    if var_names: self.add_vars(var_names)
    self.hist = self._build_hist(msgs, last_msg=last_msg)
    start = len(self.hist)
    update_msg(msgid=curr_msg['id'], content="# " + curr_msg['content'].replace('\n', '\n# '), skipped=self.hide_msg)
    response = Chat.__call__(self, msg=msg, prefill=prefill, temp=temp, think=think, search=search, stream=stream, max_steps=max_steps, final_prompt=final_prompt, return_all=return_all, **kwargs)
    output = self._new_msgs_to_output(start)
    # if no exceptions, collapse output
    update_msg(msgid=curr_msg['id'], o_collapsed=True)
    add_msg(content=f"**Prompt ({self.model}):** {msg}", output=output, msg_type='prompt')
    return response

@patch
def _build_hist(self:BackupChat, msgs:list, last_msg=None):
    if last_msg is None: curr = len(msgs)
    else:
        try: curr = next(i for i,m in enumerate(msgs) if m['id'] == last_msg['id'])
        except StopIteration: curr = len(msgs)
    hist = []
    for m in msgs[:curr+1]:
        eol = '\n'
        if m['msg_type'] == 'code': hist.append({'role': 'user', 'content': f"```python{eol}{m['content']}{eol}```{eol}Output: {m.get('output', '[]')}"})
        elif m['msg_type'] == 'note' or m['msg_type'] == 'raw': hist.append({'role': 'user', 'content': m['content']})
        elif m['msg_type'] == 'prompt':
            hist.append({'role': 'user', 'content': m['content']})
            if m.get('output'): hist.append({'role': 'assistant', 'content': m['output']})
    
    hist = hist + self._vars_as_msg() + [{'role': 'assistant', 'content': '.'}] # empty assistant msg to prevent flipping chat msg to look like prefill
    return hist

@patch
def _vars_as_msg(self:BackupChat):
    if self.vars_for_hist and len(self.vars_for_hist.keys()):
        content = "Here are the requested variables:\n" + json.dumps(self.vars_for_hist)
        return [{'role': 'user', 'content': content}]
    else:
        return []

@patch
def _new_msgs_to_output(self:BackupChat, start):
    new_msgs = self.hist[start+1:]
    parts = []
    for i, m in enumerate(new_msgs):
        if m.get('role') == 'assistant' and m.get('tool_calls'):
            for tc in m['tool_calls']:
                result_msg = next((r for r in new_msgs if r.get('tool_call_id') == tc['id']), None)
                if result_msg: parts.append(self._format_tool_details(tc['id'], tc['function']['name'], json.loads(tc['function']['arguments']), result_msg['content'], is_last_msg=(i == len(new_msgs)-1)))
        elif m.get('role') == 'assistant' and m.get('content'):
            content = m['content']
            if 'You have no more tool uses' not in content: parts.append(content)
    return '\n\n'.join(parts)

@patch
def _trunc_tool_result(self:BackupChat, result, max_len=100, is_last_msg=False):
    if len(str(result)) <= max_len or is_last_msg: return result
    return str(result)[:max_len] + '<TRUNCATED>'

@patch
def _format_tool_details(self:BackupChat, tool_id, func_name, args, result, is_last_msg=False):
    result_str = self._trunc_tool_result(result)
    tool_json = json.dumps({"id": tool_id, "call": {"function": func_name, "arguments": args}, "result": result_str}, indent=2)
    return f"<details class='tool-usage-details'>\n\n```json\n{tool_json}\n```\n\n</details>"

# %% ../nbs/01_dhb.ipynb 4
@patch
def add_tools(self:BackupChat, tool_names:Union[list,str]=None):
    "Add tools to the chat's tool list"
    if isinstance(tool_names, str):
        tool_names = tool_names.split()
    tools = [self.ns.get(t) for t in tool_names if self.ns.get(t)]
    self.tools = list(self.tools or []) + tools
    
@patch
def add_vars_and_tools(self:BackupChat, var_names:Union[list,str]=None, tool_names:Union[list,str]=None):
    "Add both variables and tools to the chat's lists"
    self.add_tools(tool_names)
    self.add_vars(var_names)

# %% ../nbs/01_dhb.ipynb 5
c = BackupChat
