{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebeeeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp dhb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c74ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#|export\n",
    "doc = \"\"\"**Backup Chat for SolveIt using dialoghelper and lisette**\n",
    "\n",
    "Sometimes we may have a problem in SolveIt while Sonnet is down (E300), or maybe we want a different perspective.\n",
    "\n",
    "This module helps us to leverage any other LLM that is available to LiteLLM by providing our own keys and the model name.\n",
    "\n",
    "Usage: \n",
    "```python\n",
    "from solveit_dmtools import dhb\n",
    "\n",
    "# then in another cell\n",
    "# bc = dhb.c() to search model names\n",
    "bc = dhb.c(\"model-name\")\n",
    "bc(\"Hi\")\n",
    "```\n",
    "\"\"\"\n",
    "import json\n",
    "import re\n",
    "from dialoghelper.core import *\n",
    "from lisette import *\n",
    "from typing import Optional, Union\n",
    "from ipykernel_helper import read_url\n",
    "import inspect\n",
    "\n",
    "class BackupChat(Chat):\n",
    "    models = None\n",
    "    vars_for_hist = None\n",
    "    model = None\n",
    "\n",
    "    def __init__(self,\n",
    "                model: str = None,\n",
    "                sp='',\n",
    "                temp=0,\n",
    "                search=False,\n",
    "                tools: list = None,\n",
    "                hist: list = None,\n",
    "                ns: Optional[dict] = None,\n",
    "                cache=False,\n",
    "                cache_idxs: list = [-1],\n",
    "                ttl=None,\n",
    "                var_names: Union[list,str] = None,\n",
    "                hide_msg:bool=False, # whether to hide the cell that includes a BackupChat.__call__\n",
    "    ):\n",
    "        if sp is None or sp == '':\n",
    "            sp = \"\"\"You're continuing a conversation from another session. Variables are marked as $`varname` and tools as &`toolname` in the context.\n",
    "\n",
    "If you see references to variables or tools that might be relevant to your answer but aren't fully available, ask the user to indicate which ones they want to include by calling e.g their `bc.add_vars`, `bc.add_tools`, or `bc.add_vars_and_tools` methods (if they called their chat instance `bc`). Note that these 3 methods each take a list of names or a string containing space-delimited names.\n",
    "\n",
    "Tool results from the earlier conversation may be truncated to about 100 characters. If you need complete information, you should ask the user to run the tool and store results in a variable then make that variable available using the chat object's add_vars method. You already have access to the read_url tool, but do confirm if you can read the URLs once because it may be expensive to access them.\n",
    "\n",
    "You are not able to run other code so you cannot store your own variables or do that for me, instead you should give Python in fenced markdown in your responses. If giving code examples or similar, remember to use fenced markdown too.\n",
    "\n",
    "Use a Socratic approach - guide through questions rather than direct answers - unless the user explicitly asks you to do something differently.\"\"\"\n",
    "        if self.models is None:\n",
    "            self.models = self.get_litellm_models()\n",
    "        if model is None:\n",
    "            _m1 = input(\"Please enter part of a model name to pick your model. Remember you also need to have secret for their API key already defined in your secrets:\")\n",
    "            print(\"Please try again by using e.g. `bc = dhb.c('model_name')` with a model name in:\")\n",
    "            print('\\n'.join([m for m in self.models if _m1 in m]))\n",
    "            return None\n",
    "        if model not in self.models:\n",
    "            raise ValueError(f\"Model {model} not found in LiteLLM models. Please check the model name or use a different model.\")\n",
    "        self.model = model\n",
    "        self.hide_msg = hide_msg\n",
    "        self.vars_for_hist = dict()\n",
    "        if var_names is not None:\n",
    "            self.add_vars(var_names)\n",
    "        if tools is None:\n",
    "            tools = [read_url]\n",
    "        if ns is None:\n",
    "            ns = inspect.currentframe().f_back.f_globals\n",
    "        super().__init__(model=model, sp=sp, temp=temp, search=search, tools=tools, hist=hist, ns=ns, cache=cache, cache_idxs=cache_idxs, ttl=ttl)\n",
    "\n",
    "    def get_litellm_models(self):\n",
    "        url = \"https://raw.githubusercontent.com/BerriAI/litellm/refs/heads/main/model_prices_and_context_window.json\"\n",
    "        data = read_url(url, as_md=False)\n",
    "        models = json.loads(data)\n",
    "        return [k for k in models.keys() if k != 'sample_spec']\n",
    "    \n",
    "    def __call__(self, \n",
    "                msg=None,\n",
    "                prefill=None,\n",
    "                temp=None,\n",
    "                think=None,\n",
    "                search=None,\n",
    "                stream=False,\n",
    "                max_steps=2,\n",
    "                final_prompt='You have no more tool uses. Please summarize your findings. If you did not complete your goal please tell the user what further work needs to be done so they can choose how best to proceed.',\n",
    "                return_all=False,\n",
    "                var_names=None, # list of variable names to add to the chat\n",
    "                **kwargs,\n",
    "                ):\n",
    "        msgs = [m for m in find_msgs() if m['pinned'] or not m['skipped']]\n",
    "        last_msg = read_msg(-1)['msg']\n",
    "        curr_msg = read_msg(0)['msg']\n",
    "        if var_names: self.add_vars(var_names)\n",
    "        self.hist = self._build_hist(msgs, last_msg=last_msg)\n",
    "        start = len(self.hist)\n",
    "        update_msg(msgid=curr_msg['id'], content=\"# \" + curr_msg['content'].replace('\\n', '\\n# '), skipped=self.hide_msg, o_collapsed=1)\n",
    "        response = super().__call__(msg=msg, prefill=prefill, temp=temp, think=think, search=search, stream=stream, max_steps=max_steps, final_prompt=final_prompt, return_all=return_all, **kwargs)\n",
    "        output = self._new_msgs_to_output(start)\n",
    "        add_msg(content=f\"**Prompt ({self.model}):** {msg}\", output=output, msg_type='prompt')\n",
    "        return response\n",
    "\n",
    "    def _build_hist(self, msgs:list, last_msg=None):\n",
    "        if last_msg is None: curr = len(msgs)\n",
    "        else:\n",
    "            try: curr = next(i for i,m in enumerate(msgs) if m['id'] == last_msg['id'])\n",
    "            except StopIteration: curr = len(msgs)\n",
    "        hist = []\n",
    "        for m in msgs[:curr+1]:\n",
    "            eol = '\\n'\n",
    "            if m['msg_type'] == 'code': hist.append({'role': 'user', 'content': f\"```python{eol}{m['content']}{eol}```{eol}Output: {m.get('output', '[]')}\"})\n",
    "            elif m['msg_type'] == 'note' or m['msg_type'] == 'raw': hist.append({'role': 'user', 'content': m['content']})\n",
    "            elif m['msg_type'] == 'prompt':\n",
    "                hist.append({'role': 'user', 'content': m['content']})\n",
    "                if m.get('output'): hist.append({'role': 'assistant', 'content': m['output']})\n",
    "        \n",
    "        hist = hist + self._vars_as_msg() + [{'role': 'assistant', 'content': '.'}] # empty assistant msg to prevent flipping chat msg to look like prefill\n",
    "        return hist\n",
    "\n",
    "    def _vars_as_msg(self):\n",
    "        if self.vars_for_hist and len(self.vars_for_hist.keys()):\n",
    "            content = \"Here are the requested variables:\\n\" + json.dumps(self.vars_for_hist)\n",
    "            return [{'role': 'user', 'content': content}]\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def _new_msgs_to_output(self, start):\n",
    "        new_msgs = self.hist[start+1:]\n",
    "        parts = []\n",
    "        for i, m in enumerate(new_msgs):\n",
    "            if m.get('role') == 'assistant' and m.get('tool_calls'):\n",
    "                for tc in m['tool_calls']:\n",
    "                    result_msg = next((r for r in new_msgs if r.get('tool_call_id') == tc['id']), None)\n",
    "                    if result_msg: parts.append(self._format_tool_details(tc['id'], tc['function']['name'], json.loads(tc['function']['arguments']), result_msg['content'], is_last_msg=(i == len(new_msgs)-1)))\n",
    "            elif m.get('role') == 'assistant' and m.get('content'):\n",
    "                content = m['content']\n",
    "                if 'You have no more tool uses' not in content: parts.append(content)\n",
    "        return '\\n\\n'.join(parts)\n",
    "    \n",
    "    def _trunc_tool_result(self, result, max_len=100, is_last_msg=False):\n",
    "        if len(str(result)) <= max_len or is_last_msg: return result\n",
    "        return str(result)[:max_len] + '<TRUNCATED>'\n",
    "    \n",
    "    def _format_tool_details(self, tool_id, func_name, args, result, is_last_msg=False):\n",
    "        result_str = self._trunc_tool_result(result)\n",
    "        tool_json = json.dumps({\"id\": tool_id, \"call\": {\"function\": func_name, \"arguments\": args}, \"result\": result_str}, indent=2)\n",
    "        return f\"<details class='tool-usage-details'>\\n\\n```json\\n{tool_json}\\n```\\n\\n</details>\"    \n",
    "    \n",
    "    def add_vars(self, var_names:Union[list,str]=None):\n",
    "        \"Add variables to conversation as user message\"\n",
    "        if isinstance(var_names, str):\n",
    "            var_names = var_names.split()\n",
    "        if not isinstance(var_names, list):\n",
    "            raise ValueError(f\"var_names must be a string or list of strings, not {type(var_names)}\")\n",
    "        \n",
    "        # Add each var to the self.vars_for_hist dictionary\n",
    "        for v in var_names:\n",
    "            self.vars_for_hist[v.strip()] = self.ns.get(v.strip(), 'NOT AVAILABLE')\n",
    "    \n",
    "    def add_tools(self, tool_names:Union[list,str]=None):\n",
    "        \"Add tools to the chat's tool list\"\n",
    "        if isinstance(tool_names, str):\n",
    "            tool_names = tool_names.split()\n",
    "        tools = [self.ns.get(t) for t in tool_names if self.ns.get(t)]\n",
    "        self.tools = list(self.tools or []) + tools\n",
    "    \n",
    "    def add_vars_and_tools(self, var_names:Union[list,str]=None, tool_names:Union[list,str]=None):\n",
    "        \"Add both variables and tools to the chat's lists\"\n",
    "        self.add_tools(tool_names)\n",
    "        self.add_vars(var_names)\n",
    "                \n",
    "c = BackupChat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0e8ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please try again by using e.g. `bc = dhb.c('model_name')` with a model name in:\n",
      "azure/eu/gpt-5-2025-08-07\n",
      "azure/eu/gpt-5-mini-2025-08-07\n",
      "azure/eu/gpt-5.1\n",
      "azure/eu/gpt-5.1-chat\n",
      "azure/eu/gpt-5.1-codex\n",
      "azure/eu/gpt-5.1-codex-mini\n",
      "azure/eu/gpt-5-nano-2025-08-07\n",
      "azure/global/gpt-5.1\n",
      "azure/global/gpt-5.1-chat\n",
      "azure/global/gpt-5.1-codex\n",
      "azure/global/gpt-5.1-codex-mini\n",
      "azure/gpt-5.1-2025-11-13\n",
      "azure/gpt-5.1-chat-2025-11-13\n",
      "azure/gpt-5.1-codex-2025-11-13\n",
      "azure/gpt-5.1-codex-mini-2025-11-13\n",
      "azure/gpt-5\n",
      "azure/gpt-5-2025-08-07\n",
      "azure/gpt-5-chat\n",
      "azure/gpt-5-chat-latest\n",
      "azure/gpt-5-codex\n",
      "azure/gpt-5-mini\n",
      "azure/gpt-5-mini-2025-08-07\n",
      "azure/gpt-5-nano\n",
      "azure/gpt-5-nano-2025-08-07\n",
      "azure/gpt-5-pro\n",
      "azure/gpt-5.1\n",
      "azure/gpt-5.1-chat\n",
      "azure/gpt-5.1-codex\n",
      "azure/gpt-5.1-codex-mini\n",
      "azure/us/gpt-5-2025-08-07\n",
      "azure/us/gpt-5-mini-2025-08-07\n",
      "azure/us/gpt-5-nano-2025-08-07\n",
      "azure/us/gpt-5.1\n",
      "azure/us/gpt-5.1-chat\n",
      "azure/us/gpt-5.1-codex\n",
      "azure/us/gpt-5.1-codex-mini\n",
      "gpt-5\n",
      "gpt-5.1\n",
      "gpt-5.1-2025-11-13\n",
      "gpt-5.1-chat-latest\n",
      "gpt-5-pro\n",
      "gpt-5-pro-2025-10-06\n",
      "gpt-5-2025-08-07\n",
      "gpt-5-chat\n",
      "gpt-5-chat-latest\n",
      "gpt-5-codex\n",
      "gpt-5.1-codex\n",
      "gpt-5.1-codex-mini\n",
      "gpt-5-mini\n",
      "gpt-5-mini-2025-08-07\n",
      "gpt-5-nano\n",
      "gpt-5-nano-2025-08-07\n",
      "openrouter/openai/gpt-5-chat\n",
      "openrouter/openai/gpt-5-codex\n",
      "openrouter/openai/gpt-5\n",
      "openrouter/openai/gpt-5-mini\n",
      "openrouter/openai/gpt-5-nano\n"
     ]
    }
   ],
   "source": [
    "bc = c()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9f6a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[ lisette '"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lisette_md = read_url(\"https://lisette.answer.ai/\")\n",
    "lisette_md[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700db55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bc = c(\"gemini/gemini-flash-lite-latest\")\n",
    "# bc = c(\"claude-haiku-4-5\")\n",
    "bc = c(\"openrouter/openai/gpt-5-codex\")\n",
    "# bc = c(\"openrouter/openai/gpt-5-mini\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ce5596",
   "metadata": {},
   "source": [
    "The following gets commented out when run (uncommented now so you can run in a test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea117270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "It looks like I donâ€™t currently have access to the contents of $`lisette_md`. Could you please make that variable availableâ€”e.g., by running something like `bc.add_vars(\"lisette_md\")`? Once I have the full text, I can walk you through it.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `gen-1764291936-CSFsPLBhQI6w957gsHa8`\n",
       "- model: `openai/gpt-5-codex`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=193, prompt_tokens=3406, total_tokens=3599, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=128, rejected_prediction_tokens=None, text_tokens=None, image_tokens=0), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=3328, text_tokens=None, image_tokens=None, video_tokens=0), cost=0.0024435, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0.0005135, 'upstream_inference_completions_cost': 0.00193})`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='gen-1764291936-CSFsPLBhQI6w957gsHa8', created=1764291936, model='openai/gpt-5-codex', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='It looks like I donâ€™t currently have access to the contents of $`lisette_md`. Could you please make that variable availableâ€”e.g., by running something like `bc.add_vars(\"lisette_md\")`? Once I have the full text, I can walk you through it.', role='assistant', tool_calls=None, function_call=None, reasoning_content='**Requesting lisette_md variable**', provider_specific_fields={'refusal': None, 'reasoning': '**Requesting lisette_md variable**', 'reasoning_details': [{'format': 'openai-responses-v1', 'index': 0, 'type': 'reasoning.summary', 'summary': '**Requesting lisette_md variable**'}, {'id': 'rs_6928f5611ba8819095a3138e66c498c3', 'format': 'openai-responses-v1', 'index': 0, 'type': 'reasoning.encrypted', 'data': 'gAAAAABpKPVjanZBI5Kxhu5t-qSl1Ni6rBnhzX8CC_nR1lO5-U_l2Kfzp6-Y2psMyXYHUZK6UeaTljimjZXsUuECE3U39OGLl-014r0K_2FEtX9CFaV7ShwlgAMTEAY9yQ4sbbYc6XKBAfpy1nUlGDAspDr4zyn9c-1rgjyMb76VsGAu-rvD8Z2AFmG25fdwTuGX9wz8_v0gfsOHhqiXEtubBsYo5RZfysOpUB2MoYvenOujDK632-ZCTV2KK_dfF5y3vmVm5KozP7eaTGWE4bGhSNnVI-tVV555RWnVpmncemlw8RQaTLEl4ca0btGWvK-V8Kz6jzE4L7LVavoygS-WHiS-nqOvOJvhRM0m520TJznAp2wcwEcClNEqiTAdUln-JhZ5miKA1o_ISdhw5H8WTZeMtdEDPVglvowEHqeuxHcrqvOdnvvuv56TQdG3BdExjILMLgFd5T9OHcy8ROP61xYihCU9MRgeBEG5E8U2qnjGvXVFBGipaGy5O4Zru2843rZv81bO1x4XhDMw7u3f90uoD7mXjgU91FnqKMN_LWzkC1-1icrLKcBZk7mCl7G4-RlmlhCwP4147Ec7F6RKhGqlCfX4Q7NMNgQ1oogdeX4cSFyIOLqRV-tPavGRamlA7D7aDrwfDQXAvS3yPTCyLbeFLtleGNR0gfowLlHnt10UNQOITg3NRzvS8n_wd64udbDwRM_M4cJ7U2D52qpht-aWMxz6gyoI5h_QKF7IhYYZRJ-I-zbMkAxsRBrfHt6gqMGVsGQByYJgB4AEBRPiX2diWRaww46ZXhbWY9qCXcH4Y-2h9Wf5jPtEFdMCAqKOkDOWX_crEnQsSkBz1bZUo8klx0m0fW__xWpoFrRJmqO5yq9Sg2ChYno1zdaYYbUM0PQuqcwcvBGXYjOcrP4FjNWzuqxJQuU71tdTYwLiuKtEu7-_B7UXYeMzWk-jT4kDcGm85NqryaiP57dh4dajB_03TmuC9CTpGbwW8Jwtr9IHr4Iy7CWWHPXIMnO05KHKiQXA88Y1x10TbRNc5JYWfcGErfqIT6c_dcdHTc5L0ZfIBptbd1fYlizyOQOV2YrSPJpZn8WInGdHfkldmC64Bcm8mRAecBpOYWtJjRmiIqw95B1fIVw084wn8tK_AU-XKPwcP0q4QB1xKsCpvulpXmfqYJdihsuiI51ZUzbBdOcTA-iIMI2gH_91X22bzV4jVQM6v2_50kZtLIgSkI1vEr_pecw3ndH-s_I_nMKqyatIItsNqtpUumFZl970SOPWmOP9ZcZbcD2KUu9eh1t8wl8k6a7b9ODCOBbo2iONtKBBVqlKn_GbrnNjHOIKzwK5aZt_4QFBLOMLRmadLAkRqsTeAv3USb9fNuCutaMD2yTYCS8X2MUCpVewCDF2t3-JxdceB11TwvhFrL3YEduWStaoEBniYheKIcZK9qjzoN7g-XFcfxhcIbQwAzsONrTOnHiRbQhr5gn4DBvcMCXp4B4Bcb6jka2AxmgRZdKux1DoTIfjhPH7S2Z5V1SNYIM6qoykASNJKR6ppza8W0KhBr5-f05kdco2g8wcqh4ztczjkR5Yid5g49ALQuw30DbSOmPs--0B_8DZeZqr8Yaz1UOqlZ60XiSHPWCvpxjd0NulWFp_Zzls6i4b8LjlI-1Rq60tk0PIuDNmZ-7D_6k-P06avFE8lMMwRnq0POfSm1oZlw06CDH8G7TWYJZqBjI-EN-fY-f3-xwHbdnhqGQ8BRufLIjQJmu5w2xLzyyfraJRuab3IZaxZgayBOdiWvoKtNOUd6Zg76mWAUz29uPiexh11UEV4IEbZ0aYx-i4xot0TWuAXvr1wH1p_e2Mqu79MU673LIpGm4sIO-qpyK7HjIYOaqPb_jli8YkC4VpSCgB0DfA5KLnOTl7mpYotbLSL5-Q3EJELHQ2flakv1IZhmex67QsTEboUeIzYemOpHV3MIqd3EQ6fEJd4opivWLqDGBCRbca'}], 'reasoning_content': '**Requesting lisette_md variable**'}), provider_specific_fields={'native_finish_reason': 'completed'})], usage=Usage(completion_tokens=193, prompt_tokens=3406, total_tokens=3599, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=128, rejected_prediction_tokens=None, text_tokens=None, image_tokens=0), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=3328, text_tokens=None, image_tokens=None, video_tokens=0), cost=0.0024435, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0.0005135, 'upstream_inference_completions_cost': 0.00193}), provider='OpenAI')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc(\"Can you please teach me about Lisette? Only use the info in $`lisette_md`.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dcc949",
   "metadata": {},
   "source": [
    "**Prompt (openrouter/openai/gpt-5-codex):** Can you please teach me about Lisette? Only use the info in $`lisette_md`.\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "It looks like I only have a truncated slice of $`lisette_md` (just `\"[ lisette \"`). Could you make the full variable availableâ€”perhaps with something like `bc.add_vars(\"lisette_md\")`â€”so I can use it to explore Lisette with you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec6836a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Lisette is a higher-level wrapper around the LiteLLM Python SDK, designed to make working with LiteLLMâ€™s multi-provider interface far more convenient while keeping access to the full power of the underlying models. Instead of wiring up prompts, tools, search, and streaming by hand, you instantiate Lisetteâ€™s `Chat` (or `AsyncChat`) class and let it handle stateful conversations, tool execution, and provider quirks automatically. As the docs put it, â€œLisette makes LiteLLM easier to use,â€ turning the low-level LiteLLM interface into a streamlined experience for switching between providers, sending rich prompts, and managing responses.\n",
       "\n",
       "Here are the key features called out in the elevator pitch and examples:\n",
       "\n",
       "- **Unified chat abstraction:** Create a `Chat` object with any LiteLLM-supported model, and it maintains history, handles message formatting, and speaks the OpenAI-style API regardless of provider. You can iterate through models and reuse the same code:\n",
       "\n",
       "  ```python\n",
       "  models = ['claude-sonnet-4-20250514', 'gemini/gemini-2.5-flash', 'openai/gpt-4o']\n",
       "\n",
       "  for model in models:\n",
       "      chat = Chat(model)\n",
       "      res = chat(\"Please tell me about yourself in one brief sentence.\")\n",
       "      display(res)\n",
       "  ```\n",
       "\n",
       "- **Flexible prompt formatting:** You can pass multiple messages at once, preload history, include images (just pass raw bytes), and even specify a `prefill` string for providers that support guided completions.\n",
       "\n",
       "  ```python\n",
       "  chat = Chat(models[0])\n",
       "  res = chat(['Hi! My favorite drink coffee.', 'Hello!', 'Whats my favorite drink?'])\n",
       "  display(res)\n",
       "  ```\n",
       "\n",
       "  Images are just as simple:\n",
       "\n",
       "  ```python\n",
       "  chat = Chat(models[0])\n",
       "  chat([img_bytes, \"What's in this image? Be brief.\"])\n",
       "  ```\n",
       "\n",
       "- **Tool calling made simple:** Decorate a Python function with type hints, hand it to `Chat`, and the model can invoke it automatically, including multi-step tool use when needed.\n",
       "\n",
       "  ```python\n",
       "  def add_numbers(a: int, b: int) -> int:\n",
       "      \"Add two numbers together\"\n",
       "      return a + b\n",
       "\n",
       "  chat = Chat(models[0], tools=[add_numbers])\n",
       "  res = chat(\"What's 47 + 23? Use the tool.\")\n",
       "  ```\n",
       "\n",
       "- **Web search integration:** Enable search with `search='l'` (low), `'m'`, or `'h'` and Lisette will request citations (for supporting models) and present them alongside the answer.\n",
       "\n",
       "  ```python\n",
       "  chat = Chat(models[0], search='l')\n",
       "  res = chat(\"Please tell me one fun fact about otters. Keep it brief\")\n",
       "  ```\n",
       "\n",
       "- **Streaming and async support:** Turn on `stream=True` to iterate over response chunks, or switch to `AsyncChat` for use in asynchronous environments such as FastHTML apps. Streaming, search, and tool calling all work together in async mode.\n",
       "\n",
       "  ```python\n",
       "  chat = Chat(models[0])\n",
       "  res_gen = chat(\"Concisely, what are the top 10 biggest animals?\", stream=True)\n",
       "  for chunk in res_gen:\n",
       "      ...\n",
       "  ```\n",
       "\n",
       "  ```python\n",
       "  chat = AsyncChat(models[0])\n",
       "  await chat(\"Hi there\")\n",
       "  ```\n",
       "\n",
       "- **Prompt caching (when supported):** Lisette surfaces LiteLLMâ€™s caching features without extra work.\n",
       "\n",
       "In short, Lisette gives you a batteries-included chat interface over LiteLLM: install it (`!pip install lisette -qq`), import its re-exported symbols (`from lisette import *`), and you can immediately work with dozens of providers using the same, feature-rich API.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `gen-1764255273-EN3Z4BdTL74kZpAioZDv`\n",
       "- model: `openai/gpt-5-codex`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=1042, prompt_tokens=9950, total_tokens=10992, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=256, rejected_prediction_tokens=None, text_tokens=None, image_tokens=0), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None, video_tokens=0), cost=0.0228575, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0.0124375, 'upstream_inference_completions_cost': 0.01042})`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='gen-1764255273-EN3Z4BdTL74kZpAioZDv', created=1764255273, model='openai/gpt-5-codex', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Lisette is a higher-level wrapper around the LiteLLM Python SDK, designed to make working with LiteLLMâ€™s multi-provider interface far more convenient while keeping access to the full power of the underlying models. Instead of wiring up prompts, tools, search, and streaming by hand, you instantiate Lisetteâ€™s `Chat` (or `AsyncChat`) class and let it handle stateful conversations, tool execution, and provider quirks automatically. As the docs put it, â€œLisette makes LiteLLM easier to use,â€ turning the low-level LiteLLM interface into a streamlined experience for switching between providers, sending rich prompts, and managing responses.\\n\\nHere are the key features called out in the elevator pitch and examples:\\n\\n- **Unified chat abstraction:** Create a `Chat` object with any LiteLLM-supported model, and it maintains history, handles message formatting, and speaks the OpenAI-style API regardless of provider. You can iterate through models and reuse the same code:\\n\\n  ```python\\n  models = [\\'claude-sonnet-4-20250514\\', \\'gemini/gemini-2.5-flash\\', \\'openai/gpt-4o\\']\\n\\n  for model in models:\\n      chat = Chat(model)\\n      res = chat(\"Please tell me about yourself in one brief sentence.\")\\n      display(res)\\n  ```\\n\\n- **Flexible prompt formatting:** You can pass multiple messages at once, preload history, include images (just pass raw bytes), and even specify a `prefill` string for providers that support guided completions.\\n\\n  ```python\\n  chat = Chat(models[0])\\n  res = chat([\\'Hi! My favorite drink coffee.\\', \\'Hello!\\', \\'Whats my favorite drink?\\'])\\n  display(res)\\n  ```\\n\\n  Images are just as simple:\\n\\n  ```python\\n  chat = Chat(models[0])\\n  chat([img_bytes, \"What\\'s in this image? Be brief.\"])\\n  ```\\n\\n- **Tool calling made simple:** Decorate a Python function with type hints, hand it to `Chat`, and the model can invoke it automatically, including multi-step tool use when needed.\\n\\n  ```python\\n  def add_numbers(a: int, b: int) -> int:\\n      \"Add two numbers together\"\\n      return a + b\\n\\n  chat = Chat(models[0], tools=[add_numbers])\\n  res = chat(\"What\\'s 47 + 23? Use the tool.\")\\n  ```\\n\\n- **Web search integration:** Enable search with `search=\\'l\\'` (low), `\\'m\\'`, or `\\'h\\'` and Lisette will request citations (for supporting models) and present them alongside the answer.\\n\\n  ```python\\n  chat = Chat(models[0], search=\\'l\\')\\n  res = chat(\"Please tell me one fun fact about otters. Keep it brief\")\\n  ```\\n\\n- **Streaming and async support:** Turn on `stream=True` to iterate over response chunks, or switch to `AsyncChat` for use in asynchronous environments such as FastHTML apps. Streaming, search, and tool calling all work together in async mode.\\n\\n  ```python\\n  chat = Chat(models[0])\\n  res_gen = chat(\"Concisely, what are the top 10 biggest animals?\", stream=True)\\n  for chunk in res_gen:\\n      ...\\n  ```\\n\\n  ```python\\n  chat = AsyncChat(models[0])\\n  await chat(\"Hi there\")\\n  ```\\n\\n- **Prompt caching (when supported):** Lisette surfaces LiteLLMâ€™s caching features without extra work.\\n\\nIn short, Lisette gives you a batteries-included chat interface over LiteLLM: install it (`!pip install lisette -qq`), import its re-exported symbols (`from lisette import *`), and you can immediately work with dozens of providers using the same, feature-rich API.', role='assistant', tool_calls=None, function_call=None, reasoning_content='**Composing direct summary from variable data**', provider_specific_fields={'refusal': None, 'reasoning': '**Composing direct summary from variable data**', 'reasoning_details': [{'format': 'openai-responses-v1', 'index': 0, 'type': 'reasoning.summary', 'summary': '**Composing direct summary from variable data**'}, {'id': 'rs_69286629e0588190adfee00265beb773', 'format': 'openai-responses-v1', 'index': 0, 'type': 'reasoning.encrypted', 'data': 'gAAAAABpKGYtLCGhunCQIT_Y9KN2hv59DAqEP0rb7v-WlVpah1c9_kW1EZRauF239cyxZSj4Z1f4WfRg-KqRibQW225eM3xyVugPzoVsoAJp8XoEU981Hl9V7fe9XydB0WkgwM1ZbXKbrx3iDJPluYlRRWOD8GOxvZmizmdMuT-tQP5ZXdJWvCQK3XB2dJ4WkdSDxNWb64nBbVbn2k5oTXOWG8Y5V3eaRp4I-BfpVsRGYmEHtcQeYtto52ehS0mlus6Nr-oU7abIrZ41IWfzEA4ixz1h_RG7Y-n8psAZnGRQDykD59bFLflOUMlv7p1OTEiZMt2wqI-LjeyoG1OcAzLbR-GvaEOE06BoibehwnfghTnMNV2Law70olNw_RxM4fUxtGzMSNM2JmcNmyLoIQF_z0ZCxKB2CGhKWj39JqsjCtkJsJT3woHeOI6oBxUuxnTz53vdoqGd2cj7d_pS2u_k05F40IJCnCvShZkpPRVK0NaI4dCHUoaUNiKcYyZh9-gdxWuHzricDmEG5JfkU1c5JRxJTefgbj2bKuvP3_gaXNfhhxdLOyuholSbMs1By1FXHJgg_baJkp6YK_13AvcKW8jJanzu4M2GnBxUddAX_2GJ1Bl70iVHlWnfN7y7MYFU5XlYwYQRd4j2prX5ely0yKXbiCbPlZNSUExBKgaqERgFnTkaw3S8K6QrA_YjfuS2hNZDqMo1LantA69XbHsFqM5AbQf8-A7BZTbyPvabvLGrr3y2R6ZAG1NqW_cXcxQbwLW9uDXav8muZPj4mrJpcDhlZnG-LrYj5JacjIHL947Sy2j5pFPa5KRcD8-4dzpibAnaQ_Ws9Uybfc8ojFCv1aF8MwgJHCpa1gjeqnU9NAmhQ9jj1r5y0Jtsqbei3qCn5CrwXTzmX2rk_OYT4wdtCo58EPiBDPfw8WBElYncZ9rspMuwM4nKJT0AmuyE9ZXbcUeQUdV1BSsr9P048T82s8WoBWEyv5BcCYoX0MvrRkeXIdJ_hedvFnlTmPt1HP_sklW7DELjHB8kU0JpgYVs8dVIRFvMkleDwKdd2TIR32iVkFPeDHqq68PpgEpkeIqrehUsvB_jIxQuZLagXcZoxK_2wosYnTnP5AdPYrkgXomo24txI81C6N9cSzHsjDJV6f3VuaZG6WvdQcCdN7hyCRpmCRBwqihjzLEHI1Iw8UusbwM06bSNu6U_VDRgShrENPw1Uq74slYAp00mQngdU7jxgJjQafNBPNiI82h1xVdam6OGn3Hrry0LvpNKHrycXHu6vT-o5DdXYfC7NY0FyEUpXlU2sBDaB0h549I4gbxy4LZJtG821PVizKHUKAtXzXCmAfl8iWsx8nft8dWQhrfBWLNG7VCpDIFIXH9cQSqd4u3l_4aKmMTpUJP3UxTDwIw_-c3Hc1qvCx_TdyJv2tXNBAZRHhkkCd6zyG8LUuyO4RJfRvopDZKvE0aYZTsqm4immeDl9nNYBmd8e3MIGEnPctdqkvLNtnCU1h90HlpdHiT_raN8DIJhCLormFyf_ZBiVJs8sI6VxBNRhsXcusay8eujTTNqT18GZqGIvG2sUjunU1J9EqIbtVyKmP5n8ydUSbRLROLaKR9KWJV_ehYiH_bD45k0hNhTsH5b2Ouh-14EuJHjlBB5_aOYKawrYyh-hmSqJNEEpBL8iFuSfWtq5yWlXTx4AUfs8idwOWHZ9_Ne14GZivNGE7ZBFSsPIcEU4xpOqsc_bfYqPzNGTcyl0kC7ZYwqAtzRl2BPyHmcuBgBR1cTVx2kDKGMg2UHU2CuXSThj-xQgzmtdD-nsrxfki3K-4Js-xqdxX7VnapFGzkfxC2q-uaVxilvxQecx3LO4VHdkbkcESLWPOwU5WL9JWWWfU7V56zAoc-LVR2t_cu7XnBx3qeh5aLAU-__jVB3Ib-xQmgRWEA6VVnSWp1RmIUrNX1MhqtQz2sk0JDCBSumVjHIzRNPUhtNEH96Ghg4Txt0C1LT6ng3WoU3eYuGjWyFCto5NdGrxzrf4GtejNDnomtvbF4sug24JrQccUFvcyVebc0aB7Hp0-_41faB_6M2uBKiPfqFeEbZ4vTjqTcTCmYhfnNYsVfgYbd4KiB4PUFiV2rr8K_ivd3TC6e_DrUkDk4UyqD_zLrWXdZOS-FL7Q1cxJnKdbOKVB9MFsgmwJFBmCzWFccF_Oty1qUP4igBQabqLwOV5YPQrkeoi8vuOPyh0Iq1LjawCgErkluVKdvANGFZfvDN_ujwFcczVYAtDyubOnVzGPXH7sQPx7XX-jWzrz2J3S-hzHvhMn8YwO8rVEKWMxab7dh7-jjZj9eqPiDqiAch47glhfDNV8qXzU_VpiDV9NykZ9NKBwRX2P1MparaXFaJOEEEM5WDG-B8WoMuSnA_D5MuwWmaYSGZ98SPjGN9do71U2wAGhhWHQPEc6klLgoIB7Ff-5WjKdp_qybRrZEcsTdkTeYo_UhLwjimUbPv33CB8P38UHJmYp0edSZC5siMeIZx46r0fVAhaylRaqBRIx5nri44_eM3nLJJ_nibcd63ZVyJxFLdkZun677pUyf6nfC3bUJya9QwenwVHE5WBwCXemN4JTmX94KSKPRZr_75bcN4DzjKf4bC_uSxrmGC88c3Cr3PJyEYTf6Uuj83jgsd8_fx4JnQd1wWR_0q1-5iiciwB3yL6e3WMBKDuxpsfBGUA0noZZ-ToJYF7_eHkOxfBNFtpiNReAWPW2JGr2yZh47kjhZG0P2jyxostfIn6AnQ2VHLv6-UErkifkRgQ4CvFI_yEYQyb6vveQDQGnclw6nNO7xjlFv7'}], 'reasoning_content': '**Composing direct summary from variable data**'}), provider_specific_fields={'native_finish_reason': 'completed'})], usage=Usage(completion_tokens=1042, prompt_tokens=9950, total_tokens=10992, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=256, rejected_prediction_tokens=None, text_tokens=None, image_tokens=0), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None, video_tokens=0), cost=0.0228575, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0.0124375, 'upstream_inference_completions_cost': 0.01042}), provider='OpenAI')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc.add_vars('lisette_md')\n",
    "bc(\"Can you tell me about the library now, based only on the variable, elevator pitch plus example code from the source. I know you are being Socratic but please give answers and not questions on this one.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67b4b4f",
   "metadata": {},
   "source": [
    "**Prompt (openrouter/openai/gpt-5-codex):** Can you tell me about the library now, based only on the variable, elevator pitch plus example code from the source. I know you are being Socratic but please give answers and not questions on this one.\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Lisette is a higher-level wrapper around the LiteLLM Python SDK, designed to make working with LiteLLMâ€™s multi-provider interface far more convenient while keeping access to the full power of the underlying models. Instead of wiring up prompts, tools, search, and streaming by hand, you instantiate Lisetteâ€™s `Chat` (or `AsyncChat`) class and let it handle stateful conversations, tool execution, and provider quirks automatically. As the docs put it, â€œLisette makes LiteLLM easier to use,â€ turning the low-level LiteLLM interface into a streamlined experience for switching between providers, sending rich prompts, and managing responses.\n",
    "\n",
    "Here are the key features called out in the elevator pitch and examples:\n",
    "\n",
    "- **Unified chat abstraction:** Create a `Chat` object with any LiteLLM-supported model, and it maintains history, handles message formatting, and speaks the OpenAI-style API regardless of provider. You can iterate through models and reuse the same code:\n",
    "\n",
    "  ```python\n",
    "  models = ['claude-sonnet-4-20250514', 'gemini/gemini-2.5-flash', 'openai/gpt-4o']\n",
    "\n",
    "  for model in models:\n",
    "      chat = Chat(model)\n",
    "      res = chat(\"Please tell me about yourself in one brief sentence.\")\n",
    "      display(res)\n",
    "  ```\n",
    "\n",
    "- **Flexible prompt formatting:** You can pass multiple messages at once, preload history, include images (just pass raw bytes), and even specify a `prefill` string for providers that support guided completions.\n",
    "\n",
    "  ```python\n",
    "  chat = Chat(models[0])\n",
    "  res = chat(['Hi! My favorite drink coffee.', 'Hello!', 'Whats my favorite drink?'])\n",
    "  display(res)\n",
    "  ```\n",
    "\n",
    "  Images are just as simple:\n",
    "\n",
    "  ```python\n",
    "  chat = Chat(models[0])\n",
    "  chat([img_bytes, \"What's in this image? Be brief.\"])\n",
    "  ```\n",
    "\n",
    "- **Tool calling made simple:** Decorate a Python function with type hints, hand it to `Chat`, and the model can invoke it automatically, including multi-step tool use when needed.\n",
    "\n",
    "  ```python\n",
    "  def add_numbers(a: int, b: int) -> int:\n",
    "      \"Add two numbers together\"\n",
    "      return a + b\n",
    "\n",
    "  chat = Chat(models[0], tools=[add_numbers])\n",
    "  res = chat(\"What's 47 + 23? Use the tool.\")\n",
    "  ```\n",
    "\n",
    "- **Web search integration:** Enable search with `search='l'` (low), `'m'`, or `'h'` and Lisette will request citations (for supporting models) and present them alongside the answer.\n",
    "\n",
    "  ```python\n",
    "  chat = Chat(models[0], search='l')\n",
    "  res = chat(\"Please tell me one fun fact about otters. Keep it brief\")\n",
    "  ```\n",
    "\n",
    "- **Streaming and async support:** Turn on `stream=True` to iterate over response chunks, or switch to `AsyncChat` for use in asynchronous environments such as FastHTML apps. Streaming, search, and tool calling all work together in async mode.\n",
    "\n",
    "  ```python\n",
    "  chat = Chat(models[0])\n",
    "  res_gen = chat(\"Concisely, what are the top 10 biggest animals?\", stream=True)\n",
    "  for chunk in res_gen:\n",
    "      ...\n",
    "  ```\n",
    "\n",
    "  ```python\n",
    "  chat = AsyncChat(models[0])\n",
    "  await chat(\"Hi there\")\n",
    "  ```\n",
    "\n",
    "- **Prompt caching (when supported):** Lisette surfaces LiteLLMâ€™s caching features without extra work.\n",
    "\n",
    "In short, Lisette gives you a batteries-included chat interface over LiteLLM: install it (`!pip install lisette -qq`), import its re-exported symbols (`from lisette import *`), and you can immediately work with dozens of providers using the same, feature-rich API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec789795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hereâ€™s a quick elevator pitch based on what I just read:\n",
       "\n",
       "**llms.txt** is a simple markdown convention for websites to expose LLM-friendly documentation in one predictable place. By keeping `/llms.txt` conciseâ€”an H1 title, a short summary, and curated links to clean `.md` versions of important pagesâ€”it sidesteps the noise of full HTML sites and makes it easy for language models (and their tooling) to grab high-value context. It complements, rather than replaces, things like sitemaps or robots.txt, and comes with tooling (CLI, plugins, etc.) to help teams expand the spec into richer context bundles.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `gen-1763994283-CmXtYOkchauXvnyPigVQ`\n",
       "- model: `openai/gpt-5-codex`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=137, prompt_tokens=16189, total_tokens=16326, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None, image_tokens=0), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None, video_tokens=0), cost=0.02160625, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0.02023625, 'upstream_inference_completions_cost': 0.00137})`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='gen-1763994283-CmXtYOkchauXvnyPigVQ', created=1763994284, model='openai/gpt-5-codex', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hereâ€™s a quick elevator pitch based on what I just read:\\n\\n**llms.txt** is a simple markdown convention for websites to expose LLM-friendly documentation in one predictable place. By keeping `/llms.txt` conciseâ€”an H1 title, a short summary, and curated links to clean `.md` versions of important pagesâ€”it sidesteps the noise of full HTML sites and makes it easy for language models (and their tooling) to grab high-value context. It complements, rather than replaces, things like sitemaps or robots.txt, and comes with tooling (CLI, plugins, etc.) to help teams expand the spec into richer context bundles.', role='assistant', tool_calls=None, function_call=None, reasoning_content='**Summarizing findings from URL read**', provider_specific_fields={'refusal': None, 'reasoning': '**Summarizing findings from URL read**', 'reasoning_details': [{'format': 'openai-responses-v1', 'index': 0, 'type': 'reasoning.summary', 'summary': '**Summarizing findings from URL read**'}, {'id': 'rs_69246aaf27f88190a8f1d7dfbecfd315', 'format': 'openai-responses-v1', 'index': 0, 'type': 'reasoning.encrypted', 'data': 'gAAAAABpJGqvHpscX8hL3VJ_nvzsEj_f33L5zljMqia3Vl6k1oR4GDfAUn2gYOsOD0Z1NLENN3TgIQIx3GaN4yCVgScU1HeMcL2ZOetlMuXgf1k4gi0l0QrcSW90kJIyZlAlbDPZkMq_rXyAKEhw3KDCd-x-j73geF13BbA7nO5mVegZYQBYVcRpaJAPVf9wAgQ9DiSS1KRUWrQAEspOZ2d40FySaoc0QFAZJBIfzWAUbg4qXQFgAnlU9T1y0ehlV8KtTneUA_OajdOklYGxkO6VCo8TUEFQzYOzga3pj-h-_M7PJR3jWWJLAe07zt0_pT8wO-Qjj22bRhS9XDY2pYKL5xVpddThI0OoJR2sOEoHWVEAMXPKiwhy9cxXoWt4kbb9wyUAS_9Ou8CCrrd0E07zyXMifMmkSGYsazF8xrlMyqM38iXZHvGkiuhxWsYbP04aA0OZ9zblnGhayx-GeLCIaDkzJfFy3nRGcliwwXx97pzEpzPKW136mCvI7_rfjAH6JlztSz9WkgBbNyUV47_Viie_5cYb4waCy1gCjUPq3Xve1AFlnEFFl3JOTJJzmJIWCZxDy_GFkrZ67wXc6UfPak4r0o4zw1S9cddl91bA_J2XmZPDuc3LHkkDMDICbCd9G7R94lMlexNMsIf82uVJsBlM2xPGBFFIuw1lrtyl7fy-E67_WAR2LZ2Q9FufqXClU-qaEW0l80iloWtHKkrOrax_KLPoqXbyFSNu01LQLAsDScSwLWu7s7ws22mdm28I2cpiy0XTDK6jxF0_iSzbzCxJ9w9Yii-HGv4DJ5mh9VmPf7Glp34MR1gJvwCyxG7_H3UsPsxDQYqh_M42Bh7tWFA3lY5k2zt4DzZCDqonTFyrl_62Yt1R4MjjUz9Rp_AV23yOxiUhkBpGdsyzzb_l2LuHh3GvFNF_o21fGstMwsYEVceSl_V4FZNKA-3CqozZ2ubVMOPB'}], 'reasoning_content': '**Summarizing findings from URL read**'}), provider_specific_fields={'native_finish_reason': 'completed'})], usage=Usage(completion_tokens=137, prompt_tokens=16189, total_tokens=16326, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None, image_tokens=0), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None, video_tokens=0), cost=0.02160625, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0.02023625, 'upstream_inference_completions_cost': 0.00137}), provider='OpenAI')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/markdown": [
       "`llms.txt` is a proposal for every website to publish a concise, LLM-friendly Markdown companion at `/llms.txt`. It gives models a curated overview, links to clean `.md` versions of key pages, and follows a predictable structure (H1 title, short summary, optional detail, then labeled link sections). The goal is to make it easy for language models to grab high-signal context without wrangling messy HTML, while staying compatible with existing web standards like `robots.txt` and sitemaps.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `gen-1763994333-LwPkgTQX9ikegpC1NQti`\n",
       "- model: `openai/gpt-5-codex`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=240, prompt_tokens=16189, total_tokens=16429, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=128, rejected_prediction_tokens=None, text_tokens=None, image_tokens=0), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=13440, text_tokens=None, image_tokens=None, video_tokens=0), cost=0.00751625, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0.00511625, 'upstream_inference_completions_cost': 0.0024})`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='gen-1763994333-LwPkgTQX9ikegpC1NQti', created=1763994333, model='openai/gpt-5-codex', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='`llms.txt` is a proposal for every website to publish a concise, LLM-friendly Markdown companion at `/llms.txt`. It gives models a curated overview, links to clean `.md` versions of key pages, and follows a predictable structure (H1 title, short summary, optional detail, then labeled link sections). The goal is to make it easy for language models to grab high-signal context without wrangling messy HTML, while staying compatible with existing web standards like `robots.txt` and sitemaps.', role='assistant', tool_calls=None, function_call=None, reasoning_content='**Preparing final summary**', provider_specific_fields={'refusal': None, 'reasoning': '**Preparing final summary**', 'reasoning_details': [{'format': 'openai-responses-v1', 'index': 0, 'type': 'reasoning.summary', 'summary': '**Preparing final summary**'}, {'id': 'rs_69246ade87e08190b658d3596ceebb66', 'format': 'openai-responses-v1', 'index': 0, 'type': 'reasoning.encrypted', 'data': 'gAAAAABpJGrmisTOHp0MQTlM6ZvMGeFvlGZxWDruUqKEfsaWP7QXZ5ctSP66qu1oot4nGymtMVbYu-twzsgwm4zTVWgcM57zAg1GH6v0UXJyV-ZFQnfmSZRqmAO61SMWdNBTgKGnjPiw92ST1T-pq5huNq4m1Nwz39sJgV5BOb0B9eMDzyYYhn8KQgc8c0wEJqwz9rr-z35V9GCOSEk72ouUW1Ob-v-EmUWRUtSg1HlS1FRIfhUr3pSv6bvnDAwkmVhueC07hbL8eaDnU_xITK6FLz6wEz7MZbjkXUAFx24kcqT9mbPsPbAepsQ_Yp8SSlDcbWmQWvHCdxMs0DMf9_pKqB2vR5ysvejmsA8Jbei2y7uQKXCfSPgblvZg3V_ld_feRFFmqdD-V044syExRMLysYCfP4jLzF35zrr4fy9vz9wXJ-VAT_NNhd-0T9VmTimZeq1uJL10pDnnPsQJBLjlZbN88_Yy5dN81ZcbExN7Df_Xwih5ZhJW3jUfNSpfCB4R9tZGEdqttrHGYLGlYY5K1OpVcJCXrUtDRnD3zKbSmxbrlwTG_InWgiygysyiC0WTTLxxprWYov722oyysz8wopQkNRIXOguCevzP4_Xa-z1MZTVwfQ8yg8he0V7Cvc1P-mGUCiz4Qdzb6TLqXY66qJwRi7pFvjo_wrQekKe6Ezo7q3sODT5mV7l8Aenpghs_KiarHWJl1ntplsDR4xrDUutcTQpONlZTwm1L-dP5qE6agMvoPV9Hfy6c7f52i_ZW6_lu67MBbcZiKWTRiS0esUx-ZXaMpdGFy8UZxhSXgfURIleXBkejUY_msm8CiUVsRSetyxeDyV_hCV_6OuIP5-3HMhBsjjhelJieBwefJKFd0XvRQ_BMJiEGZGaZY3pdqBkflFmIbXRobUG1zfo_pjRsdhFzcLnAJJxciulBCZXq3II7cZwS8EEEaXXCypD845xXSFntyY_ZPK6PO6LtEhMF61xxv9MHkbSD8k7NuRQAwQXke_eS2ZHAfHdQbmOtP8D19HwKbF_d_4QTApOeJBLX5kz8pN5ZdLUbuUcCMpSL-waJPDOzUfwYmj5U6hLDvTOUARqmzyYrQm86F1G2rRUkMKlvTMhY8xXRY5OWMTm_uLO_epUSc62rE_HenNK8gTvAbh83fR2VBeUIF8sewqBjuiVEVA9KHKeTsyptt0N6FuTI16ZqeWd11N6o_KjCX5whDicUzMM-X7UFu1kWlMUwRp0jDaRwQxaFEx5EbEj-fsrEFFK9BFw5YSvTjSjZuYUF5QsQW6Xlrg3grDDtcO0ksePicsgSj0wbL4W3aEHRskuvP7kWyyUxEMnA-a96jaVr65E5kMbxY4P8WjYCXEar2b07GUN5q5Bdtz-qBTbQuM38WRYBePb5nhjnSWy2Ai7Skm0G0OoYyWKW7WdN5j1N5FwmHV7SGzUAxVJH9Z1v43iO0BVQ9aI33bDR_0PvFELFsI0j5BcwvhUIg54QJ6fiP6Iy_qsDTWra--nRYitme47Qf0IIWTkUVjSvYkOWt9_r_T62L6QsdHjLmw2i3k8smEjub_hCST0Mq2Zz-e3ZTv5TWgcKdtLj1Y7u5BQjnrcZG1DCxDYWeBYkBJ6vRYqKn8rX3KB0pmujodX2VoLdUOUbw3-gPeV9eL-ogCxx1X2Rr0UKMxOSQw_0FOSog_0iNw6eFMYv-bkPBWrkU-pED3fUpy4='}], 'reasoning_content': '**Preparing final summary**'}), provider_specific_fields={'native_finish_reason': 'completed'})], usage=Usage(completion_tokens=240, prompt_tokens=16189, total_tokens=16429, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=128, rejected_prediction_tokens=None, text_tokens=None, image_tokens=0), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=13440, text_tokens=None, image_tokens=None, video_tokens=0), cost=0.00751625, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0.00511625, 'upstream_inference_completions_cost': 0.0024}), provider='OpenAI')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc(\"Can you use tools? For example can you read https://llmstxt.org/index.md and tell me about it? Fetch it, don't store it, give the elevator pitch please.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01806637",
   "metadata": {},
   "source": [
    "**Prompt (openrouter/openai/gpt-5-codex):** Can you use tools? For example can you read https://llmstxt.org/index.md and tell me about it? Fetch it, don't store it, give the elevator pitch please.\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "<details class='tool-usage-details'>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"call_lAFPpDr5RciSYwMGKlzHua8Y\",\n",
    "  \"call\": {\n",
    "    \"function\": \"read_url\",\n",
    "    \"arguments\": {\n",
    "      \"url\": \"https://llmstxt.org/index.md\",\n",
    "      \"as_md\": true,\n",
    "      \"extract_section\": false,\n",
    "      \"selector\": \"\"\n",
    "    }\n",
    "  },\n",
    "  \"result\": \"# The /llms.txt file\\nJeremy Howard\\n2024-09-03\\n\\n## Background\\n\\nLarge language models increasingly rel<TRUNCATED>\"\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "`llms.txt` is a proposal for every website to publish a concise, LLM-friendly Markdown companion at `/llms.txt`. It gives models a curated overview, links to clean `.md` versions of key pages, and follows a predictable structure (H1 title, short summary, optional detail, then labeled link sections). The goal is to make it easy for language models to grab high-signal context without wrangling messy HTML, while staying compatible with existing web standards like `robots.txt` and sitemaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2873526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "`fhdaisy` is a thin Python layer that brings DaisyUIâ€™s Tailwind-based component set directly into FastHTML apps. Instead of sprinkling HTML strings like `<button class=\"btn btn-primary\">`, you work with Python-first components (`Btn`, `Card`, `Alert`, etc.) that automatically apply the right DaisyUI classes and underlying HTML tags. Modifiers are shortened (`-primary`, `-outline`, `-sm`), yet expand to the full DaisyUI class list under the hood, keeping your UI code concise and readable. Extras in `fhdaisy.xtras` offer ready-made helpers for recurring UI patterns (accordions, forms, â€¦), while still letting you compose or extend components as needed.\n",
       "\n",
       "Typical flow:\n",
       "\n",
       "```python\n",
       "pip install fhdaisy\n",
       "```\n",
       "\n",
       "then:\n",
       "\n",
       "```python\n",
       "from fasthtml.common import *\n",
       "from fhdaisy import *\n",
       "\n",
       "page = Div(\n",
       "    Card(\n",
       "        CardBody(\n",
       "            H2(\"Welcome to FastHTML + DaisyUI\"),\n",
       "            P(\"This card is built from Python components, not raw HTML.\"),\n",
       "            Btn(\"Click me\", cls='-primary -sm')\n",
       "        ),\n",
       "        cls='-compact -shadow'\n",
       "    )\n",
       ")\n",
       "\n",
       "print(page)\n",
       "```\n",
       "\n",
       "The snippet yields well-structured HTML where each element already carries the expected DaisyUI classes, so you can focus on layout and behavior rather than CSS glue code. For a deeper diveâ€”including advanced components, helper utilities, and live demosâ€”check the projectâ€™s documentation site linked from the README.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `gen-1763994357-VoP1BBTW593bbYjKHcTL`\n",
       "- model: `openai/gpt-5-codex`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=631, prompt_tokens=18980, total_tokens=19611, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=320, rejected_prediction_tokens=None, text_tokens=None, image_tokens=0), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=17664, text_tokens=None, image_tokens=None, video_tokens=0), cost=0.010163, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0.003853, 'upstream_inference_completions_cost': 0.00631})`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='gen-1763994357-VoP1BBTW593bbYjKHcTL', created=1763994357, model='openai/gpt-5-codex', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='`fhdaisy` is a thin Python layer that brings DaisyUIâ€™s Tailwind-based component set directly into FastHTML apps. Instead of sprinkling HTML strings like `<button class=\"btn btn-primary\">`, you work with Python-first components (`Btn`, `Card`, `Alert`, etc.) that automatically apply the right DaisyUI classes and underlying HTML tags. Modifiers are shortened (`-primary`, `-outline`, `-sm`), yet expand to the full DaisyUI class list under the hood, keeping your UI code concise and readable. Extras in `fhdaisy.xtras` offer ready-made helpers for recurring UI patterns (accordions, forms, â€¦), while still letting you compose or extend components as needed.\\n\\nTypical flow:\\n\\n```python\\npip install fhdaisy\\n```\\n\\nthen:\\n\\n```python\\nfrom fasthtml.common import *\\nfrom fhdaisy import *\\n\\npage = Div(\\n    Card(\\n        CardBody(\\n            H2(\"Welcome to FastHTML + DaisyUI\"),\\n            P(\"This card is built from Python components, not raw HTML.\"),\\n            Btn(\"Click me\", cls=\\'-primary -sm\\')\\n        ),\\n        cls=\\'-compact -shadow\\'\\n    )\\n)\\n\\nprint(page)\\n```\\n\\nThe snippet yields well-structured HTML where each element already carries the expected DaisyUI classes, so you can focus on layout and behavior rather than CSS glue code. For a deeper diveâ€”including advanced components, helper utilities, and live demosâ€”check the projectâ€™s documentation site linked from the README.', role='assistant', tool_calls=None, function_call=None, reasoning_content=\"**Summarizing fhdaisy Python wrapper**\\n\\nI'll capture that fhdaisy is a Python wrapper bridging DaisyUI with FastHTML, offering component classes like Btn and Card, plus shorthand modifiers and extra form helpers. I'll include an install note, a concise code example, and point to documentationâ€”all without naming the tool calls or being Socratic.\", provider_specific_fields={'refusal': None, 'reasoning': \"**Summarizing fhdaisy Python wrapper**\\n\\nI'll capture that fhdaisy is a Python wrapper bridging DaisyUI with FastHTML, offering component classes like Btn and Card, plus shorthand modifiers and extra form helpers. I'll include an install note, a concise code example, and point to documentationâ€”all without naming the tool calls or being Socratic.\", 'reasoning_details': [{'format': 'openai-responses-v1', 'index': 0, 'type': 'reasoning.summary', 'summary': \"**Summarizing fhdaisy Python wrapper**\\n\\nI'll capture that fhdaisy is a Python wrapper bridging DaisyUI with FastHTML, offering component classes like Btn and Card, plus shorthand modifiers and extra form helpers. I'll include an install note, a concise code example, and point to documentationâ€”all without naming the tool calls or being Socratic.\"}, {'id': 'rs_69246af674dc81908731bb08f95305d0', 'format': 'openai-responses-v1', 'index': 0, 'type': 'reasoning.encrypted', 'data': 'gAAAAABpJGsA9C8OBajBvroSS_weOnK99JM_woGZwp83AAVhcoqNhUnZMPSc0jwWWeb4yd4q5C_9-LjQsyzif5E11Etx4CEVL-MYuSrOEhzR4NXBmRMV84TqsQhl9D5fw6HqhYcded_4pdVW3ZWVTmyD-p0RV4qaYlzT70yxSvhdeBHPDXRRjrFhbvNWFB2-bEg9DbUN_O7A9o7xdELPfJ-j5iX2LSpRcDEsxXkfCWvVNt9N26AD3Vs0IcJlRDEvtrBnYRM3IHk4rs9JWyAc0XwU1rf0v0zIfAjVB97MTzKXiiLW6-p27y8b4pzzxQMBHuNt9PUYX4qefxXnCIZ3znUxOcCvJknTs99NdTKWUcxGLRVysY9eTAb1GJnLi8ibhT_RgDfp-dqShBmRjiGt2vkNaIJUv7A-TWOnxlZwbthPv-lszKpHm4C17cEEHsU5GI5MfGuUvfDgjW0aFyWQQeUnUcFAitxoul5n4A5m3dCUoemoIcnVm21CiQjfWNlzW6S5hWaLtL1ok5Dc86NiBLLv_vegeG9NC4FuSf31PcY9e3yJFLWNhfOzAnGnRfiYMwmdm3lzf7o82J8ITzevVMdvuQ47AsKX9Zd0Uc4SOR9-E5W56lPBiX24F4duCxy7Zf3ivA7WAEYOfxmP0Qa96q21_Ppx6yEx5dH-t5SqOEIvXEP23MKBfkGW_Q0YP31MUugvwi9cOjfA2Tzyh2YRuoaEs2ZiPoZetkhfwVcH1YfJs7JHrs2um9TNfQX_VzSM0NAS0EmOOmvBkyJkhyAHR3tDprXP_HxJHS4mQAXX22WkeH_-ofB8cYzU7E5KEYJtCRPZYpbIRmt06GBd5E6IJ8zkB_I7ofFp657e-z3BO6nQDa5yk1zn2A2jK-6W4G1PwcjIoLM1W3FiqmtuVuGf25f_ronPumacMy1OI5ZuX3R-VvyVcsE0JIA2POYC1MwnQey0UDzb5uuhqcAvNP-cLxlDDon7AljKlS9lwqbk8IS1djhWJAM9ZNiPiFoaprUcNLxRvKnYC4MePBplQbPq3wkgcoZSZuaanl4XxWcEPjCC612p-7rOPDjoeTyG1dFx7JXAcRFQ3w1U5jMMgqw1_-qU2i0TLS-z6U8MWvrOreghWTLXxLwK6LH_MuhMFR_I4MNtDp3Yp70ErIHReB9sTQ6ru7rZK_5vBboSk4KYf62ezqImLFxyPUQqTUpNoMJM7c6Ao3w6ZcPN7AhTiAuA9Cu-CwVGpuc9yZOhNEA9q6eaOe28_a3bHFjHhHpqmBDngDfW3TK_K_9zRqfrqoWQSa2BNgx9XygBRzp2c5SIgCfwYhHz64EdBFEr6_WRKlrIjXDzuR38BWix5yRPt35Gi4f4QUIAIvGuntiBtrCmJEtp1XqWK7iqL3ong_ZXtzH9sJguZcIgQcIMa6xPKcru_Th29obHP5sVcAWgaBSkm6y3mIQDtLYusUsTkbvhwPpjff7qm3671s_ls7TAtpq1r90EIQK11GLjDGKIjx3QHYRgCys1klMzxRkMVGzLqcfXyhvwE-JpFmf3wxxPLMMOscMKlOYfGQd9AXrA_KCaY6Hj47TFjXTKWNbJG3Xt_gkEykxwK7EqeAimGCll2kEzrLltXpDixGXH7AzBz2ObEG69QDVIlhuaX6YpzWxPzVSVlj_mQDajAfqfAEKKzLWN7nlwC05JRtLDTHASNquoAQkODMqlJ7IFltsTQbtUiwqnSGvoCQfp2FZNASdnafp73_94tYgMVplRh-rtwvI7dETl4fwcP33Kq9mHX6EXsHXCBnibJv7yY-Z7pjUdY68XXCaVftX8jk7enOb0gtGLAM9TXA9eSuoL6LJckyOO52Bf20SFNklZvnZ8z4DKSM9BD13eE4xdgMjHK3aTg2KAITsRnKXm4MCrUxcMZb7szXdw3DIZFnUycgOOgqBTUzNUhT6CzDaUKzepwxEAJWGDXPjkMu7Rve03rYtFJILj6IOuzZaCnFy3Fr2qvip25Zchgvu69Ze43JrIG5Rp7m4qfqvQM5EnOjXJeurpK9Y_iTBocghJhWydEidWEKSRcMsDkWweuovH_JYJedHwQCsJQBNjwiVrJDaJ8qHAv_HKaVPhz-4hZ1E4iEvX6NA-nSOCkl-AVm0QRwIUbyRY79t7ZimEziH1Rta1xEjxT4xeJ6V4reH9mL3WaYtAYMtpQmxwWB2X1qP3_rk9nLBh9RgJ4ikCYT6W5_98N2eS0CVSZAepxiVoP8CIc8qfgs4oRT0It7silVxyIj1NQxN1kUwD-i8HCOj1qJaw0i8DiB8XhECKS5JdSkccy0DqEzAaLVSrxD_t20bfSb5W_TAwd6MISE0Pi2cmN9o27bD21a8mpWbNrOGK2jA_TF0FYUQs0Eg6NdJyaBUu8YPfOSTBHSRA_ewuLyKgkzLUCkuiZwTJVv2WZnASo3mN8atkSR7q2PjJ7etYUbKEM_RyTAQgqiMnDJnoa64qoHI6ruRAl5H4DDjTqYY0G4yeBRGfFkee9Lg_YlIGDBXwL560pPG95qovOnqv8PpZgFv0ANRnGekTA4A3F7nTd9cnt_DQ0kFPnFMsssW-m448hBIY45-JEz0LO-zlY1rWHR0CW9TPf99uzHPS9zPS4gl5xVUBhCZjvn1Mu7MC0_Kp7VwPFgXnjgQJs_vTlJvkilYixNbKC3Ffhs7Mjj97gmcU8cn_wJ88J9AXmBSzoDO3MSIfh88EPjPOAimz_n89abPnzavyinwBsKxgh5R-T6sjRi_bQNDEfh5Y7JkeNRSz6wJ-D-fU8TJIqzreXEMWvXWMkoPgzfEwtrg9MU7w07MWwAJKwal1gnwVyl1FNQTESZaerfDEQCotKi2zmwiclyH7QknAAV3w6fASlPOc50EprcjZbNvSoLPItStX_c7JuAr-Jpw2YqW6_XNk3v4bKg04drMBaG0N0iD3r64UKRiTryZG-dLMMo3Yhqzoq-m9GlMi7g-ixs0a62qBO8wUvU93Hi9piQw1iuiuiaTQJJUkNKsk'}], 'reasoning_content': \"**Summarizing fhdaisy Python wrapper**\\n\\nI'll capture that fhdaisy is a Python wrapper bridging DaisyUI with FastHTML, offering component classes like Btn and Card, plus shorthand modifiers and extra form helpers. I'll include an install note, a concise code example, and point to documentationâ€”all without naming the tool calls or being Socratic.\"}), provider_specific_fields={'native_finish_reason': 'completed'})], usage=Usage(completion_tokens=631, prompt_tokens=18980, total_tokens=19611, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=320, rejected_prediction_tokens=None, text_tokens=None, image_tokens=0), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=17664, text_tokens=None, image_tokens=None, video_tokens=0), cost=0.010163, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0.003853, 'upstream_inference_completions_cost': 0.00631}), provider='OpenAI')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc(\"Now use your tool and summarize https://raw.githubusercontent.com/AnswerDotAI/fhdaisy/refs/heads/main/README.md please - give a code example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ecc513",
   "metadata": {},
   "source": [
    "**Prompt (openrouter/openai/gpt-5-codex):** Now use your tool and summarize https://raw.githubusercontent.com/AnswerDotAI/fhdaisy/refs/heads/main/README.md please - give a code example\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "<details class='tool-usage-details'>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"call_7FvqavATiA7YIfVsneBjiPQH\",\n",
    "  \"call\": {\n",
    "    \"function\": \"read_url\",\n",
    "    \"arguments\": {\n",
    "      \"url\": \"https://raw.githubusercontent.com/AnswerDotAI/fhdaisy/refs/heads/main/README.md\",\n",
    "      \"as_md\": true,\n",
    "      \"extract_section\": false,\n",
    "      \"selector\": \"\"\n",
    "    }\n",
    "  },\n",
    "  \"result\": \"# fhdaisy\\n\\n`fhdaisy` is a Python wrapper for [DaisyUI](https://daisyui.com/) that\\nbrings its compone<TRUNCATED>\"\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "`fhdaisy` is a thin Python layer that brings DaisyUIâ€™s Tailwind-based component set directly into FastHTML apps. Instead of sprinkling HTML strings like `<button class=\"btn btn-primary\">`, you work with Python-first components (`Btn`, `Card`, `Alert`, etc.) that automatically apply the right DaisyUI classes and underlying HTML tags. Modifiers are shortened (`-primary`, `-outline`, `-sm`), yet expand to the full DaisyUI class list under the hood, keeping your UI code concise and readable. Extras in `fhdaisy.xtras` offer ready-made helpers for recurring UI patterns (accordions, forms, â€¦), while still letting you compose or extend components as needed.\n",
    "\n",
    "Typical flow:\n",
    "\n",
    "```python\n",
    "pip install fhdaisy\n",
    "```\n",
    "\n",
    "then:\n",
    "\n",
    "```python\n",
    "from fasthtml.common import *\n",
    "from fhdaisy import *\n",
    "\n",
    "page = Div(\n",
    "    Card(\n",
    "        CardBody(\n",
    "            H2(\"Welcome to FastHTML + DaisyUI\"),\n",
    "            P(\"This card is built from Python components, not raw HTML.\"),\n",
    "            Btn(\"Click me\", cls='-primary -sm')\n",
    "        ),\n",
    "        cls='-compact -shadow'\n",
    "    )\n",
    ")\n",
    "\n",
    "print(page)\n",
    "```\n",
    "\n",
    "The snippet yields well-structured HTML where each element already carries the expected DaisyUI classes, so you can focus on layout and behavior rather than CSS glue code. For a deeper diveâ€”including advanced components, helper utilities, and live demosâ€”check the projectâ€™s documentation site linked from the README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09033cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "What do you have in mind for `md_hier`â€”are you trying to turn a Markdown string you already have into a navigable dictionary, or would you rather start by exploring the example shown in `$`mdh_md` (the `sample_md` snippet) to see how `create_heading_dict` behaves?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `gen-1764255417-tanV26cjyCuCN43FOWnP`\n",
       "- model: `openai/gpt-5-codex`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=263, prompt_tokens=25408, total_tokens=25671, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=192, rejected_prediction_tokens=None, text_tokens=None, image_tokens=0), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None, video_tokens=0), cost=0.03439, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0.03176, 'upstream_inference_completions_cost': 0.00263})`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='gen-1764255417-tanV26cjyCuCN43FOWnP', created=1764255417, model='openai/gpt-5-codex', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='What do you have in mind for `md_hier`â€”are you trying to turn a Markdown string you already have into a navigable dictionary, or would you rather start by exploring the example shown in `$`mdh_md` (the `sample_md` snippet) to see how `create_heading_dict` behaves?', role='assistant', tool_calls=None, function_call=None, reasoning_content='**Formulating guiding questions on md_hier use**', provider_specific_fields={'refusal': None, 'reasoning': '**Formulating guiding questions on md_hier use**', 'reasoning_details': [{'format': 'openai-responses-v1', 'index': 0, 'type': 'reasoning.summary', 'summary': '**Formulating guiding questions on md_hier use**'}, {'id': 'rs_692866ba39d481908d1325e797b5f1c3', 'format': 'openai-responses-v1', 'index': 0, 'type': 'reasoning.encrypted', 'data': 'gAAAAABpKGa9-Sqhv3tEHfotWDymZ8vcybJCE8xRzMKOBUY2-5T6msN9LAGVLgZR81SqHwjY5cjUQ9Cxk5aifC3JsDEUWlNnAXZw7e-pqJAvn29zMWw18cEs0OMiFRwkKe-f_yrTwIYGZjaqM2HuYHFExio0GURWNUr3KsjhLc6zVsVZAO_Kx87kLPyHxzpa3uIymqKL5j7Uw7oesmntG_gStKqXNYN47oUepSp04o08ihgkcW7uRj-pH2ejylAM1mTcDZWB7Bb5d1TW9maF8iMdzaRXcqFcaiv-mbZwcGquKzAE6ZxlGZlDnyHHuKxUUo5Pc5aSure-yVW4c5lD3mkX9QdDSl2k8nPoCBuyPuFO2lTyPKN5vkteIdinRc2xREZhA4dSvSbXnkVRyPeH2lf1FZjbq_-gOWnFtplG3jApiJ8VJVvU-7gXA54RvX0h8dEE9oZSJSqV3XNjbZ6-fQu82_AS5LnQgTlqJKMRRdsFFT2zUOhLoXOaLqWGfWCoU9Q4T54bAuW5iU3celN6iBIAbhIiKq125PfCc9DRHV25s7P-OBlwFBzJ0CfU0VQyU8UJ536r6leVmaMKGHEIBISP-9wkCB_Ux7xeypU56Uy0mLExeKY7I9qwprT_-sNFI8nhLlbJxsl2cM81OZkBAEmLc_Uo3-yXCm3YmE3AaFg5m1oVxUHRlvTFc63pxDtEn2lTNSXJ44ae-pCqwQNGoBx3vnrZt_mn_Od0k10xX56B_FrHSHnHhWeXlpAiy5eOYZRVY9r2_AakWG_4Uxn6TBJeCiYYtta4F4ahCLX4ijlvDeDcze2b3KuWxtcw3vM5xMFEbz-FElb863v-1QeWR4a-wB4jzlPYnmAsTubMe7JM0P2MEihO2Kk_oAMt3bglLGO3sXjIRL1Zjjd5aLmOWFjb6GAq3Dt1Dk1Lkx5cD5H9hXAvqQtX4WKGtpw0czfea6wtwOG4lKgzGbtu31YWmBpAAdXi3bD7GSUpJk_ZepFLU06kQdozlI7lCt6Lp02y4AGCS38kJKJaWqYEB2901mn_ByG04VgQbo0nUWvShSNpwCBwzowqpAnxDT33hpEnPwkr7-DZQjb2T8oloI85_tARQCcI2-Y4KoOueQeVOQy2o8lopAaTsSjA0Mp8hQt13-ZbJQVgkHdIm-AUEPkhkmNzMx21QJeTlqBTnr8l3c1AZcz-GTbDGsv2rgjjHg4YdUxP9t6nkCre8lxXXUzdGj52or-DssTgy9PC4FWqHLH4abJQblmSlYAB3sVaAjdA3KM7A8HhOu_M_E6K9O5V-T2SBmZVvUCeHRxjpvBTdjKtS54QSdeWnPfJSVxBkWILy3O5j35JvlayBCRMvT4CEkC3Myj2Px-vluHebJpm3_y2c_v5OoEEAzG40ZNJQc_1boL5ng8_f995JkpLA1QsOh1ANlvpFHpEkvzsOtCyRSxY0kkhFHQrG8cYi1IgtuqQJU8fMUTpaM8QT027HS--7b10IBRAZjUieGA_JPS4uJ_DqCPIldZBdP-VR4BYY9-nORtFSDOxjrgVYBJ0r56WZBa7CVmAONRN2PJvb-eMuzwm01cZtnGD88eW-k5x83xJFUOivAc_VJpxBk3imz93Ckfj5cyu_wIz1pMHcjQyKPUUyRW082qKEvzeReYEWinDHjbB-4nFy2mokzbET3IHb1FQ5kpVers0b8--b15w17FriRUIo76VFI6QlHGrB2QAp_S5q6fayo7tzQvLsZcEEFsvqRoYH5pPRek9_-SA3yFYWhJMFKdfySw-Ltyd7hn4_kco9pSPSZHnWiEVvFpe4u4uykd5xrXN-ctVdJc2cTq8ELEICc1mOvo2XVs_sJh9DIBTpYzVBir24q2Rms70UIPEUyN89qnwclEgmscsl5sdRBCmc4_UwU5KBDcPqGmOlErexeNsXpAwKWcJdKwGqYM_3et01fPJrW4SZJLOWlrcpSweuZfSdFYAOH9AyszjotCCiJRmy9dixOuUztKYFVwka3rpYA3WQrygAuT5TOdi-6eRqe1cnXgmCoMnTMo_YesygeBY03WaGTTi7MRPMqUJvvYVwprZSR0T38zog6wB3L6E_y7tcT5E0DerDh0CUj19CsYcSnoaSXpXehsFkE41maqj8_6geXus-JHTUCv7mFkXx1lDzE0YWb2VO5BjZWjYqTYna1sQfGBoho7O36pACBZzOMFHYB2n8vhNtakLFt--5vEZ1sPF2MjkvZ8hY9o1t6Dm0wTjSzcCwvOdkqz50rCHU6SgXH54e0fpHUon_Cyd29SuMvoppWUCmI7ArzVRAwiNgdMEBCCGqADVMaTSZP2QfvvE8cYbpTtBJYFYlNlVNl8v8i-osOZBtG1b25t6chVjhA1HzvWv-6JKMlJX31JTpMs4qoFDVl6FHgYLH_tgvKvKcR0='}], 'reasoning_content': '**Formulating guiding questions on md_hier use**'}), provider_specific_fields={'native_finish_reason': 'completed'})], usage=Usage(completion_tokens=263, prompt_tokens=25408, total_tokens=25671, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=192, rejected_prediction_tokens=None, text_tokens=None, image_tokens=0), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None, video_tokens=0), cost=0.03439, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0.03176, 'upstream_inference_completions_cost': 0.00263}), provider='OpenAI')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdh_md = read_url(\"https://raw.githubusercontent.com/AnswerDotAI/toolslm/refs/heads/main/04_md_hier.ipynb\")\r\n",
    "bc(\"How can I use md_hier?\", var_names=\"mdh_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a03628f",
   "metadata": {},
   "source": [
    "**Prompt (openrouter/openai/gpt-5-codex):** How can I use md_hier?\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "What do you have in mind for `md_hier`â€”are you trying to turn a Markdown string you already have into a navigable dictionary, or would you rather start by exploring the example shown in `$`mdh_md` (the `sample_md` snippet) to see how `create_heading_dict` behaves?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89371567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To turn the Markdown in `$lisette_md` into a navigable hierarchy, feed it to `create_heading_dict` from `toolslm.md_hier`. The result behaves like a nested dictionary whose entries also carry a `.text` attribute with the Markdown for that section.\n",
       "\n",
       "```python\n",
       "from toolslm.md_hier import create_heading_dict\n",
       "\n",
       "# Build a hierarchical view of the Lisette docs\n",
       "lisette_tree = create_heading_dict(lisette_md)\n",
       "\n",
       "# Top-level sections\n",
       "print(\"Root sections:\", list(lisette_tree.keys()))\n",
       "\n",
       "# Dive into the main â€œLisetteâ€ section\n",
       "lisette_section = lisette_tree[\"Lisette\"]\n",
       "print(\"Subsections under 'Lisette':\", list(lisette_section.keys()))\n",
       "\n",
       "# Grab the raw Markdown for a subsection\n",
       "chat_section_md = lisette_section[\"Chat\"].text\n",
       "print(chat_section_md[:400], \"â€¦\")\n",
       "```\n",
       "\n",
       "You can keep indexing into `lisette_tree` like a standard dictionary to reach any heading level, and `.text` at each node gives you that sectionâ€™s Markdown (including its children).\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `gen-1764255623-n0ND6ONjechpdBWvm3op`\n",
       "- model: `openai/gpt-5-codex`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=547, prompt_tokens=27983, total_tokens=28530, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=320, rejected_prediction_tokens=None, text_tokens=None, image_tokens=0), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=18560, text_tokens=None, image_tokens=None, video_tokens=0), cost=0.01956875, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0.01409875, 'upstream_inference_completions_cost': 0.00547})`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='gen-1764255623-n0ND6ONjechpdBWvm3op', created=1764255623, model='openai/gpt-5-codex', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='To turn the Markdown in `$lisette_md` into a navigable hierarchy, feed it to `create_heading_dict` from `toolslm.md_hier`. The result behaves like a nested dictionary whose entries also carry a `.text` attribute with the Markdown for that section.\\n\\n```python\\nfrom toolslm.md_hier import create_heading_dict\\n\\n# Build a hierarchical view of the Lisette docs\\nlisette_tree = create_heading_dict(lisette_md)\\n\\n# Top-level sections\\nprint(\"Root sections:\", list(lisette_tree.keys()))\\n\\n# Dive into the main â€œLisetteâ€ section\\nlisette_section = lisette_tree[\"Lisette\"]\\nprint(\"Subsections under \\'Lisette\\':\", list(lisette_section.keys()))\\n\\n# Grab the raw Markdown for a subsection\\nchat_section_md = lisette_section[\"Chat\"].text\\nprint(chat_section_md[:400], \"â€¦\")\\n```\\n\\nYou can keep indexing into `lisette_tree` like a standard dictionary to reach any heading level, and `.text` at each node gives you that sectionâ€™s Markdown (including its children).', role='assistant', tool_calls=None, function_call=None, reasoning_content='**Providing code example for md_hier parsing**', provider_specific_fields={'refusal': None, 'reasoning': '**Providing code example for md_hier parsing**', 'reasoning_details': [{'format': 'openai-responses-v1', 'index': 0, 'type': 'reasoning.summary', 'summary': '**Providing code example for md_hier parsing**'}, {'id': 'rs_69286788944c81909d33453bf2b5a6aa', 'format': 'openai-responses-v1', 'index': 0, 'type': 'reasoning.encrypted', 'data': 'gAAAAABpKGeNOGf5Ide0E6Paom4JXdS9GZGIuRike6cHFtsGlcR6IO_V_QDQNTzX6g7cxhWBvJuA0ObErqpYQG0-O3GTgeOhBHg2QQ02TXep_c35wOFYDjNCUWUnGJvlMBcZxFh3WauJRXnsrP5q8sydooISeQiKAfWa5AK_fTdhRWTf3pCp4B9T_B0oqjxER_2hGp2RTJugvXze4i9SKgdyAJ9VeTVguTuKMbUC6qVzakXSZEe576q4PLFzd0YQopxScNSm4PeHbsRX2PDBy2DQJjWiPyHOPZLmQsqTlXi1Z2pwqFaeIJrfUjs08ae4Dis9BT57pPgyJxyyBLwZUMjQUYdURrwjmz7XXmd9pLRORuZX_i_WvzYC19h2s-CQghiR4uFTWN0AIfZiq2qRZHvyOsAIjqO3LFOHWdQgkCGwLis2sNWHwdseE-S6eMWLCezYJcwSpOh1QlC23_qPgoqDPK-coaVBWoUYlEgQ5OSKra72z6U_kGAYEAPdV_xf6u8HfBqqESFZmOjzkOdyokIY8ip-VFZA0Z0Z3yqa7YKFkzsgqBRcGUt35bQcoXw1QPCz_0hFGQU44NnCjBK4gA27_Ti40UWayQhNxCHNoFJQpqd5V1J6W27hwXIxkkEsvJm9iO4hSg9IvzqIbI2AofPhymBkKszAliTyyoTnTuuTcNHC5E3VHe-ySAzR-gcsSnE5ZIgCUYZsPeXN73RvwDd9BNazV8QMuAcCLY2GMvDLcpkxp400XjkCMzkoZfl9vgVGZWuKqECqUuO6cboVEiWhf3QfHGic6z44ZAX8KPtzphonbk-BpULWxl_rMUcTxjbcWdGIksGZXyuBYrt2oE53c9RJmkGmQvFPQYgh3bdBPB6GcXTqySUy1TrLYzFPf2Wb4PfTJ0HEFb1fmCjMT8sFlWQSEUhz3nCGcUR6Sp4IwRlulBfmR5clHaJYvUUv7DNvLwOddMWcgG-VzzToHpVleW6FAVLM1ngnK-Hs_vu35VPns0XaK84BzfJV3SW91Zr669GQqx2gK2WTUYrJPV6iJhz-xbeg06Zz5cRYSBOKjXeCjtO48mZTE3Q_2AiX5O_4ZqTHx3vjukIBf6B2BdueBUagjgrCNCYwQW2x1RAXT4p5rf5FgYDza39MUB34GmWpe_1BEcNz7C_Br2XpvFFfaRevhm2BLle_FFw87FRK7GyH4xE_3K3mxMU4z344i2tuOX_bO6opbBM2UzC-a8WM3J2qZgxroINC_66twneyiwu58W0tP9nbuF5sR6GaqirxutOSmWtEVAvmu6MByN4pg7at7lpEfv_bVqDT2Jdk8-3-LG6CyIe1hpdwnPP3HC494qG8_tiDQDw-K3rDC7Eqr-mPF5uyqorzrcdm1TXNOUkhoOgrbLQkeZ1JgWQ8pkK3oJTaXSAdz1O-qE5CocxUFg7SGgaJMQ3clWZIJcaoi6JIokEWuwoc3eX5dqKKaNIZEMqrSSzSq-A_1FAUb65nJUL1a4pOBT8i-QEv0Y9wIuB_RclgMILTLfFvCeN9bUOaBqdHVUs4n5YQL3twgVRr8funS_fL5S9BiE1dBjaigN1wt1WOxkbgLOajOHlng_P7w0pGEN0j376MUC1vhpdYLAxeXhWPbTVufWvH02AfFZ1pzl9v-VJraj0EWoUibK8clWymCzSu6TXc0mvdAkmMKqRdW9p4Uv7vl5hso2OTeoTCmplXgyoI0ZgWc9xQf9ifF7glA0lXSjfWlpMSIangSDjwilDlDnMxzEyTrDmNbyMPhEfj2vedUKJT2HywCiCB-_3HPjTJgcKBVABE3ovxWq818tV5G5td9q17U4if6OGrfsAtkK4ny2KJbz0FsePSSuUBw11LrgSw_RQDJO6gsxfAqiCZRk8-_dt3FQV2FsP4Cojv2yhQABAaUViiCP0HZ3sjXvBS0Eij4ZC4ioEV2UjfuyIKdirr_6Flqt4a3TL3T2BBCR5A8IaNRVet2f2FOzMVJOWjLaxbz-gWEzVJmHITHI-ArHBBGfXA0kwUPq6sZq7cQAEeuyGK95v-XTXae_89S4dW9b1KsfA82XZJlAXfDrDctmwpvzBAexaat1J3D4IG0fYi8ybqef6Vcvs5X6erFq-_z1aVcPOy4iBXltyP29lJhC9kETBe3iiwEbz6ygkdauh0AwFtRPhEYdYWWtSwh79zUa4RLJVhNHEUvUIpYv1wjGCnTiWLLz8qJV0fJZOLaBrF3_oRU71Jbv4EhhuYaXKH5A9EzCjC_PcX3VHtq8vTu0VPoDh1LahQTW7dxFFJ-Kb8HXmsDuzjp7zxckhvQVcF1CyQQNx9Lt92LCfNj6lNCr9tIfxt2_HJGoqm-JkzrkSbC_0q81O1_WUEXgWFT7OVsq1STLy1hSVq4L6QT-dW5qDtwWiLK5ZyjMKiWiypqROZ1LWS5TMInmPb7HT8BZz0CZ59HWE1o6Vni-BXQOUP1D-ffEKJ-FVQdEMAGQjoYEy2XYtSJ4SmMpQx0a32FwNNo2JlHw2vN6AvivXH9actHnvnCJKzwqt0LWvX154ZqStBfq5kN6RtJYHSyk_FrHeJZP-6AHYZKSDRGOIKNWMxXi48zdhLXkJOnnk3SDBJozetrWbLAbUN4BBL0LE28AvQ_fho7a3YMVIuAOy85v-u6u1d7gSeRVTRudPqSzlu_wFwHd1dk0ni8mm_g1UnHfdyavXD7bARk3K8LdbFZ0gyM_usGjXxzwWemDUAOpaq8DdqQ9XZHpGQjU6-fTjpvAYfSmuIj4C_pwnlkUqQGqqgDoq61KsJw6MwX2dlT_izSCpahsj2sfdtodlQCLuStvDvteOhFCyUTblJ1upTyDvcih4n_fs5uBKIZ1owQS4iTd4jlcy4SivnUFt6REA6RchR4Q4ayyC01foyVuzbxAWrOA=='}], 'reasoning_content': '**Providing code example for md_hier parsing**'}), provider_specific_fields={'native_finish_reason': 'completed'})], usage=Usage(completion_tokens=547, prompt_tokens=27983, total_tokens=28530, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=320, rejected_prediction_tokens=None, text_tokens=None, image_tokens=0), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=18560, text_tokens=None, image_tokens=None, video_tokens=0), cost=0.01956875, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0.01409875, 'upstream_inference_completions_cost': 0.00547}), provider='OpenAI')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc(\"How can I use md_hier to parse $`lisette_md`? Please give code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3f3994",
   "metadata": {},
   "source": [
    "**Prompt (openrouter/openai/gpt-5-codex):** How can I use md_hier to parse $`lisette_md`? Please give code\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "To turn the Markdown in `$lisette_md` into a navigable hierarchy, feed it to `create_heading_dict` from `toolslm.md_hier`. The result behaves like a nested dictionary whose entries also carry a `.text` attribute with the Markdown for that section.\n",
    "\n",
    "```python\n",
    "from toolslm.md_hier import create_heading_dict\n",
    "\n",
    "# Build a hierarchical view of the Lisette docs\n",
    "lisette_tree = create_heading_dict(lisette_md)\n",
    "\n",
    "# Top-level sections\n",
    "print(\"Root sections:\", list(lisette_tree.keys()))\n",
    "\n",
    "# Dive into the main â€œLisetteâ€ section\n",
    "lisette_section = lisette_tree[\"Lisette\"]\n",
    "print(\"Subsections under 'Lisette':\", list(lisette_section.keys()))\n",
    "\n",
    "# Grab the raw Markdown for a subsection\n",
    "chat_section_md = lisette_section[\"Chat\"].text\n",
    "print(chat_section_md[:400], \"â€¦\")\n",
    "```\n",
    "\n",
    "You can keep indexing into `lisette_tree` like a standard dictionary to reach any heading level, and `.text` at each node gives you that sectionâ€™s Markdown (including its children)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
