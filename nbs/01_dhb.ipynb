{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebeeeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp dhb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb5d3ec",
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "#|export\n",
    "doc = \"\"\"**Backup Chat for SolveIt using dialoghelper and lisette**\n",
    "\n",
    "Sometimes we may have a problem in SolveIt while Sonnet is down (E300), or maybe we want a different perspective.\n",
    "\n",
    "This module helps us to leverage any other LLM that is available to LiteLLM by providing our own keys and the model name.\n",
    "\n",
    "Usage: \n",
    "```python\n",
    "from solveit_dmtools import dhb\n",
    "\n",
    "# then in another cell\n",
    "# bc = dhb.c() to search model names\n",
    "bc = dhb.c(\"model-name\")\n",
    "bc(\"Hi\")\n",
    "```\n",
    "\"\"\"\n",
    "import json\n",
    "import re\n",
    "from dialoghelper.core import *\n",
    "from lisette import *\n",
    "from typing import Optional, Union\n",
    "from ipykernel_helper import read_url\n",
    "# from fastcore.utils import patch\n",
    "\n",
    "class BackupChat(Chat):\n",
    "    models = None\n",
    "    vars_for_hist = None\n",
    "    model = None\n",
    "\n",
    "    def __init__(self,\n",
    "                model: str = None,\n",
    "                sp='',\n",
    "                temp=0,\n",
    "                search=False,\n",
    "                tools: list = None,\n",
    "                hist: list = None,\n",
    "                ns: Optional[dict] = None,\n",
    "                cache=False,\n",
    "                cache_idxs: list = [-1],\n",
    "                ttl=None,\n",
    "                var_names: list = None,\n",
    "    ):\n",
    "        if sp is None or sp == '':\n",
    "            sp = \"\"\"You're continuing a conversation from another session. Variables are marked as $`varname` and tools as &`toolname` in the context.\n",
    "\n",
    "If you see references to variables or tools that might be relevant to your answer but aren't fully available, ask the user to indicate which ones they want to include by calling e.g their `bc.add_vars`, `bc.add_tools`, or `bc.add_vars_and_tools` methods (if they called their chat instance `bc`). Note that these 3 methods each take a list of names or a string containing space-delimited names.\n",
    "\n",
    "Tool results from the earlier conversation may be truncated to about 100 characters. If you need complete information, you should ask the user to run the tool and store results in a variable then make that variable available using the chat object's add_vars method. You already have access to the read_url tool, but do confirm if you can read the URLs once because it may be expensive to access them.\n",
    "\n",
    "You are not able to run other code so you cannot store your own variables or do that for me, instead you should give Python in fenced markdown in your responses. If giving code examples or similar, remember to use fenced markdown too.\n",
    "\n",
    "Use a Socratic approach - guide through questions rather than direct answers - unless the user explicitly asks you to do something differently.\"\"\"\n",
    "        if self.models is None:\n",
    "            self.models = self.get_litellm_models()\n",
    "\n",
    "        if model is None:\n",
    "            _m1 = input(\"Please enter part of a model name to pick your model. Remember you also need to have secret for their API key already defined in your secrets:\")\n",
    "            print(\"Please try again by using e.g. `bc = dhb.c('model_name')` with a model name in:\")\n",
    "            print('\\n'.join([m for m in self.models if _m1 in m]))\n",
    "            return None\n",
    "        if model not in self.models:\n",
    "            raise ValueError(f\"Model {model} not found in LiteLLM models. Please check the model name or use a different model.\")\n",
    "        self.model = model\n",
    "        self.vars_for_hist = dict()\n",
    "        if var_names is not None:\n",
    "            self.add_vars(var_names)\n",
    "        if tools is None:\n",
    "            tools = [read_url]\n",
    "        super().__init__(model=model, sp=sp, temp=temp, search=search, tools=tools, hist=hist, ns=ns, cache=cache, cache_idxs=cache_idxs, ttl=ttl)\n",
    "\n",
    "    def get_litellm_models(self):\n",
    "        url = \"https://raw.githubusercontent.com/BerriAI/litellm/refs/heads/main/model_prices_and_context_window.json\"\n",
    "        data = read_url(url, as_md=False)\n",
    "        models = json.loads(data)\n",
    "        return [k for k in models.keys() if k != 'sample_spec']\n",
    "    \n",
    "    def __call__(self, \n",
    "                msg=None,\n",
    "                prefill=None,\n",
    "                temp=None,\n",
    "                think=None,\n",
    "                search=None,\n",
    "                stream=False,\n",
    "                max_steps=2,\n",
    "                final_prompt='You have no more tool uses. Please summarize your findings. If you did not complete your goal please tell the user what further work needs to be done so they can choose how best to proceed.',\n",
    "                return_all=False,\n",
    "                var_names=None, # list of variable names to add to the chat\n",
    "                **kwargs,\n",
    "                ):\n",
    "        msgs = [m for m in find_msgs() if m['pinned'] or not m['skipped']]\n",
    "        curr_msg = read_msg(0)['msg']\n",
    "        if var_names: self.add_vars(var_names)\n",
    "        self.hist = self._build_hist(msgs, msgid=curr_msg['id'])\n",
    "        start = len(self.hist)\n",
    "        update_msg(msgid=curr_msg['id'], content=\"# \" + curr_msg['content'].replace('\\n', '\\n# '), i_collapsed=1, o_collapsed=1)\n",
    "        response = super().__call__(msg=msg, prefill=prefill, temp=temp, think=think, search=search, stream=stream, max_steps=max_steps, final_prompt=final_prompt, return_all=return_all, **kwargs)\n",
    "        output = self._new_msgs_to_output(start)\n",
    "        add_msg(content=f\"**Prompt ({self.model}):** {msg}\", output=output, msg_type='prompt')\n",
    "        return response\n",
    "\n",
    "    def _build_hist(self, msgs:list, msgid:str=None):\n",
    "        if msgid is None: curr = len(msgs)\n",
    "        else:\n",
    "            try: curr = next(i for i,m in enumerate(msgs) if m['id'] == msgid)\n",
    "            except StopIteration: curr = len(msgs)\n",
    "        hist = []\n",
    "        for m in msgs[:curr]:\n",
    "            eol = '\\n'\n",
    "            if m['msg_type'] == 'code': hist.append({'role': 'user', 'content': f\"```python{eol}{m['content']}{eol}```{eol}Output: {m.get('output', '[]')}\"})\n",
    "            elif m['msg_type'] == 'note': hist.append({'role': 'user', 'content': m['content']})\n",
    "            elif m['msg_type'] == 'prompt':\n",
    "                hist.append({'role': 'user', 'content': m['content']})\n",
    "                if m.get('output'): hist.append({'role': 'assistant', 'content': m['output']})\n",
    "        \n",
    "        hist = mk_msgs(hist + self._vars_as_msg())\n",
    "        return hist\n",
    "\n",
    "    def _vars_as_msg(self):\n",
    "        if self.vars_for_hist and len(self.vars_for_hist):\n",
    "            content = \"Here are the requested variables:\\n\" + \"\\n\".join([f\"$`{v.strip()}`: {globals().get(v.strip(), 'NOT FOUND')}\" for v in self.vars_for_hist.keys()])\n",
    "            return [{'role': 'user', 'content': content}]\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def _new_msgs_to_output(self, start):\n",
    "        new_msgs = self.hist[start+1:]\n",
    "        parts = []\n",
    "        for i, m in enumerate(new_msgs):\n",
    "            if m.get('role') == 'assistant' and m.get('tool_calls'):\n",
    "                for tc in m['tool_calls']:\n",
    "                    result_msg = next((r for r in new_msgs if r.get('tool_call_id') == tc['id']), None)\n",
    "                    if result_msg: parts.append(self._format_tool_details(tc['id'], tc['function']['name'], json.loads(tc['function']['arguments']), result_msg['content'], is_last_msg=(i == len(new_msgs)-1)))\n",
    "            elif m.get('role') == 'assistant' and m.get('content'):\n",
    "                content = m['content']\n",
    "                if 'You have no more tool uses' not in content: parts.append(content)\n",
    "        return '\\n\\n'.join(parts)\n",
    "    \n",
    "    def _trunc_tool_result(self, result, max_len=100, is_last_msg=False):\n",
    "        if len(str(result)) <= max_len: return result\n",
    "        return str(result)[:max_len] + '<TRUNCATED>'\n",
    "    \n",
    "    def _format_tool_details(self, tool_id, func_name, args, result, is_last_msg=False):\n",
    "        result_str = self._trunc_tool_result(result)\n",
    "        tool_json = json.dumps({\"id\": tool_id, \"call\": {\"function\": func_name, \"arguments\": args}, \"result\": result_str}, indent=2)\n",
    "        return f\"<details class='tool-usage-details'>\\n\\n```json\\n{tool_json}\\n```\\n\\n</details>\"    \n",
    "    \n",
    "    def add_vars(self, var_names:Union[list,str]=None):\n",
    "        \"Add variables to conversation as user message\"\n",
    "        if isinstance(var_names, str):\n",
    "            var_names = var_names.split()\n",
    "        if not isinstance(var_names, list):\n",
    "            raise ValueError(f\"var_names must be a string or list of strings, not {type(var_names)}\")\n",
    "        \n",
    "        # Add each var to the self.vars_for_hist dictionary\n",
    "        for v in var_names:\n",
    "            self.vars_for_hist[v.strip()] = globals().get(v.strip(), 'NOT FOUND')\n",
    "    \n",
    "    def add_tools(self, tool_names:Union[list,str]=None):\n",
    "        \"Add tools to the chat's tool list\"\n",
    "        tools = [globals().get(t) for t in tool_names if globals().get(t)]\n",
    "        self.tools = list(self.tools or []) + tools\n",
    "    \n",
    "    def add_vars_and_tools(self, var_names:Union[list,str]=None, tool_names:Union[list,str]=None):\n",
    "        \"Add both variables and tools to the chat's lists\"\n",
    "        self.add_tools(tool_names)\n",
    "        self.add_vars(var_names)\n",
    "                \n",
    "c = BackupChat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b9c2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_heir_md = read_url(\"https://raw.githubusercontent.com/AnswerDotAI/toolslm/refs/heads/main/04_md_hier.ipynb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0e8ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please try again by using e.g. `bc = dhb.c('model_name')` with a model name in:\n",
      "azure/eu/gpt-5-2025-08-07\n",
      "azure/eu/gpt-5-mini-2025-08-07\n",
      "azure/eu/gpt-5.1\n",
      "azure/eu/gpt-5.1-chat\n",
      "azure/eu/gpt-5.1-codex\n",
      "azure/eu/gpt-5.1-codex-mini\n",
      "azure/eu/gpt-5-nano-2025-08-07\n",
      "azure/global/gpt-5.1\n",
      "azure/global/gpt-5.1-chat\n",
      "azure/global/gpt-5.1-codex\n",
      "azure/global/gpt-5.1-codex-mini\n",
      "azure/gpt-5.1-2025-11-13\n",
      "azure/gpt-5.1-chat-2025-11-13\n",
      "azure/gpt-5.1-codex-2025-11-13\n",
      "azure/gpt-5.1-codex-mini-2025-11-13\n",
      "azure/gpt-5\n",
      "azure/gpt-5-2025-08-07\n",
      "azure/gpt-5-chat\n",
      "azure/gpt-5-chat-latest\n",
      "azure/gpt-5-codex\n",
      "azure/gpt-5-mini\n",
      "azure/gpt-5-mini-2025-08-07\n",
      "azure/gpt-5-nano\n",
      "azure/gpt-5-nano-2025-08-07\n",
      "azure/gpt-5-pro\n",
      "azure/gpt-5.1\n",
      "azure/gpt-5.1-chat\n",
      "azure/gpt-5.1-codex\n",
      "azure/gpt-5.1-codex-mini\n",
      "azure/us/gpt-5-2025-08-07\n",
      "azure/us/gpt-5-mini-2025-08-07\n",
      "azure/us/gpt-5-nano-2025-08-07\n",
      "azure/us/gpt-5.1\n",
      "azure/us/gpt-5.1-chat\n",
      "azure/us/gpt-5.1-codex\n",
      "azure/us/gpt-5.1-codex-mini\n",
      "gpt-5\n",
      "gpt-5.1\n",
      "gpt-5.1-2025-11-13\n",
      "gpt-5.1-chat-latest\n",
      "gpt-5-pro\n",
      "gpt-5-pro-2025-10-06\n",
      "gpt-5-2025-08-07\n",
      "gpt-5-chat\n",
      "gpt-5-chat-latest\n",
      "gpt-5-codex\n",
      "gpt-5.1-codex\n",
      "gpt-5.1-codex-mini\n",
      "gpt-5-mini\n",
      "gpt-5-mini-2025-08-07\n",
      "gpt-5-nano\n",
      "gpt-5-nano-2025-08-07\n",
      "openrouter/openai/gpt-5-chat\n",
      "openrouter/openai/gpt-5-codex\n",
      "openrouter/openai/gpt-5\n",
      "openrouter/openai/gpt-5-mini\n",
      "openrouter/openai/gpt-5-nano\n"
     ]
    }
   ],
   "source": [
    "bc = c()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9f6a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[ lisette '"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lisette_md = read_url(\"https://lisette.answer.ai/\")\r\n",
    "lisette_md[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700db55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = c(\"openrouter/openai/gpt-5-codex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ce5596",
   "metadata": {},
   "source": [
    "The following gets commented out when run (uncommented now so you can run in a test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea117270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "What does the variable `lisette_md` currently contain? If youâ€™d like me to use it directly, could you make it available to the chat (e.g., by running `bc.add_vars('lisette_md')`) and then let me know itâ€™s ready?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `gen-1763994133-EjtE76wyzTE3j4Ocy3ND`\n",
       "- model: `openai/gpt-5-codex`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=317, prompt_tokens=3319, total_tokens=3636, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=256, rejected_prediction_tokens=None, text_tokens=None, image_tokens=0), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None, video_tokens=0), cost=0.00731875, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0.00414875, 'upstream_inference_completions_cost': 0.00317})`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='gen-1763994133-EjtE76wyzTE3j4Ocy3ND', created=1763994133, model='openai/gpt-5-codex', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"What does the variable `lisette_md` currently contain? If youâ€™d like me to use it directly, could you make it available to the chat (e.g., by running `bc.add_vars('lisette_md')`) and then let me know itâ€™s ready?\", role='assistant', tool_calls=None, function_call=None, reasoning_content='**Determining need for lisette_md content**', provider_specific_fields={'refusal': None, 'reasoning': '**Determining need for lisette_md content**', 'reasoning_details': [{'format': 'openai-responses-v1', 'index': 0, 'type': 'reasoning.summary', 'summary': '**Determining need for lisette_md content**'}, {'id': 'rs_69246a15c82c81909942d1ce63b8a734', 'format': 'openai-responses-v1', 'index': 0, 'type': 'reasoning.encrypted', 'data': 'gAAAAABpJGobYuBSyj-UnbTXVlG6EA0uNrXrfkpi-vq75WpBD_rNbt8SwM8X9AwANCox1nXoAb4Qbs09DkIuLK-e0s83Cc1he7nQKJe43ptZkjDbhVlRcaFxEnXYxvFly6jzPHlOlQuSJv9UxteUndojZS8q9_DR_9INUHcFmRMjYVowVvAjvg4-_BYo9jOOmzAomClL-_Dq7YPg67SFmRk85zTF5m-gjy0hmmHkO5Vd2UsUGg-y9TXEnW1wBCG95eOl_sezx4A76BZBVulX0PZyANKqsKaOXevanvzsq9sqBc86sPjRr24S1CXTIAyO0YebaJJGDTM2vifZaiMlyVQs9LlOcXtiM7_G3qhRdd4Um0GkHV_x0D7PdA514urNcqQbmOtUYcvrDFFoey6YI-Z3MpSml_v7aSGvz_S8BFLd0HVrwbz7NFfA6qfKt82VDHMs38GZ2LnvnKoClr0FEdRsjjuUvG04wlSPYS_K_WRcbx3q2r0ixtrXRkoywKTJK0KaXReb-18GOSmeDTjgSguj3WEfeag8KBHonQVU0YJS6dUzdKwzxKKOalHMN194qsq6F42KcIXm4SOw_jy3nWRna6mGeX935DZkFErrKohrCHexV6pVz_xuMt8y4xfsRvjb0awf9-9PwDng4roo71VCHcn_7I7WcgjV4c4BF4xr0tj5rwe9OQoLEGDXG67l_SITLfjtoEvNWMlB4UeHiEUOBthaQb2hbZBP2WVzy-zCFYNh0fnFIP88zbcXqfiDps08gIP7VrY-dhqzSmBUjKQh0ChGDMwvHciXqnvpBJBIhQL8bxM5g9Fh932TdihXaX5O6J0qLajo_0iC1-1y_gf7auOfWOZuAoQei7ixHuiCEUZ8Yjd-fmtlUMIIsVL9jJkJm-YZhNoHsFYcNNjIJ-VFupCqCyP-PIbvQTw0mpwLSjtjnYzEIY9g1mQe_AG4rr2-0NsggRAGHCwghb9jwrdvXyEDQ3to_XWts5dyll2lOckLtLy32cVZD_kGotHFAqYk5So3ms9J1HCaM4eIJrSy8EfuP2hommG9WWSOmxQJZwl7nmOfnC_pTEmIPRsrwSahHokjelRRJhI9_5HAenf4rxBB8ZVL7f9nX8v0AI567V972Z59kd-qEUnRCIcfwunK33MupI8mIRNoMD-6Qa24x71aHXosXS9cX4elIW1C3QdPuWZdlHUqHJZZe-MfGGpz1_hmdy8TvSux-TxHaZG-tFuIbVylJaoNVt1er6kxnyx7xq9qpg7W29DEiyj5hwtg8fakBrpvMRzQV9IiiLuKqL9dXOelRK3wMU_XoO91BALKCVtjt7Ww7lqo_c1AdKe0fvohq3ByYiDaiyiqDfgma2FWStYaRJVmyt9uIW48fCPemPQlEKGZ_dQmWoqyXbPV2AxTOKvEtSXsA0DBuu2o_j7ZZt7MPe9AMKXe4lAoFd77Q1EBR4kkOXDqHQZRerO63L9QDB2XZYPOYuZiteiudWI-jnkoGPNiQnQfIIPhnMtOm4WKWuJU4gaf9uTzIneQjsk31aRaSsSlmvUXHDlLKwzR6MVFjySqVvgTGZ0atI2FRE32LLHRSlxo2JuGuB2z9u4FF8vTy1l1BlDaA8ReFA05YG5hPAWFT0hxU99S6kMHBLY17Ykd8w0FWq0duOCee9kj2yTBrBfzVR3GwOaPzgzUV8SQh4Jce7OFOGWo3MKDE0k-QCSFdlDP8sNx8blxTBHYYZZbKOfLpwByHY6z1AA_MvhIyxBipUNvojvFnI7k4JGeBNhr5jsecXWOrMxRMpURr1KQvTCgheP5DpA3HykBcDyioI8KiW9m3q5W2KJTRpSiOl5FgsNCqImHsmh5L1xew29oxnWcmA6I7_4wwuWWSXxEp7DHLVWYAjFq9Y38ms5hpVHn6B9Ceydlf9dapTM70yEXz8LoGUTo2AF4G6RXf-0wDJNGHWBrE1aCpXXcOL1peu5NO471i4yEjilgYAn51KOU-aUBTz0QjFTuG7x5GX_uFU-FJKqJtr-YGrlOf9QBQb5_rY_rqtx6KeARqhCwXDoSvuuwK0q2YVg4lRwXlhRZbf-e6OHpPIQYqeJ7C1iZOU9XydEqqII74Qmh8ColNdBugybkmb1necxHS1CmH6pJ_JxQn_5JfoYDl0XYu7E5vEF4gM-wXm_4CkXv5oNtoDn5E4yzS0mYlAZmGZUhA-ywcPSZm50FdbqmeJwk_i70VNodkOcdmHULiFgyIG3Pb1knR_ycYHWWPTUEyAriE7h_GJ3IHu216MWa6GNuwzw-p9YKhjRzU9zrTcVwY_jeJabXVRtHxWRiXywvkiL6t3YL1Ar01oYpeQtNwZrZ9nfRayRWdl4sjy6xba-ty3nk06EDiJD9KJh_ROKh1tpf20KWwbB_06Zi78h-W2wrZCcVWSs='}], 'reasoning_content': '**Determining need for lisette_md content**'}), provider_specific_fields={'native_finish_reason': 'completed'})], usage=Usage(completion_tokens=317, prompt_tokens=3319, total_tokens=3636, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=256, rejected_prediction_tokens=None, text_tokens=None, image_tokens=0), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None, video_tokens=0), cost=0.00731875, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0.00414875, 'upstream_inference_completions_cost': 0.00317}), provider='OpenAI')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc(\"Can you please teach me about Lisette? Only use the info in $`lisette_md`.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221a5294",
   "metadata": {},
   "source": [
    "**Prompt (openrouter/openai/gpt-5-codex):** Can you please teach me about Lisette? Only use the info in $`lisette_md`.\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "What does the variable `lisette_md` currently contain? If youâ€™d like me to use it directly, could you make it available to the chat (e.g., by running `bc.add_vars('lisette_md')`) and then let me know itâ€™s ready?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec6836a",
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Lisette is a higher-level wrapper around the LiteLLM Python SDK that smooths out the rough edges of working with many different LLM providers. LiteLLM already unifies 100+ models behind an OpenAI-style API, but it stays fairly low level: you juggle your own histories, format multi-modal prompts, wire search and tool hookups, and so on. Lisette automates almost all of that while still letting you keep fine-grained control when you need it. You get a drop-in `Chat` class for stateful conversations across any LiteLLM-supported model, simple helpers for mixing text, images, and multiple turns, one-line streaming and async, as well as automatic tool execution and optional provider web-search integration (with citations for models that support them). Prompt caching is built in for the models that can use it, and you only need to supply the relevant API keys as environment variables for whatever providers you want to reach.\n",
       "\n",
       "A minimal workflow looks like this:\n",
       "\n",
       "```python\n",
       "from lisette import *\n",
       "\n",
       "models = ['claude-sonnet-4-20250514', 'gemini/gemini-2.5-flash', 'openai/gpt-4o']\n",
       "\n",
       "for model in models:\n",
       "    chat = Chat(model)\n",
       "    res = chat(\"Please tell me about yourself in one brief sentence.\")\n",
       "    display(res)\n",
       "```\n",
       "\n",
       "The same interface works no matter which provider you point at, and the library takes care of all provider-specific quirks. From there you can pass multi-message lists or image bytes, prefill responses (for supported providers), register tools with Python type hints, enable search with a single flag, or switch to `AsyncChat` for concurrent apps and combine it with streaming plus tooluse.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `gen-1763994241-u5E4tKlnoLZtCNsJ1CWt`\n",
       "- model: `openai/gpt-5-codex`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=422, prompt_tokens=10578, total_tokens=11000, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=64, rejected_prediction_tokens=None, text_tokens=None, image_tokens=0), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None, video_tokens=0), cost=0.0174425, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0.0132225, 'upstream_inference_completions_cost': 0.00422})`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='gen-1763994241-u5E4tKlnoLZtCNsJ1CWt', created=1763994241, model='openai/gpt-5-codex', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Lisette is a higher-level wrapper around the LiteLLM Python SDK that smooths out the rough edges of working with many different LLM providers. LiteLLM already unifies 100+ models behind an OpenAI-style API, but it stays fairly low level: you juggle your own histories, format multi-modal prompts, wire search and tool hookups, and so on. Lisette automates almost all of that while still letting you keep fine-grained control when you need it. You get a drop-in `Chat` class for stateful conversations across any LiteLLM-supported model, simple helpers for mixing text, images, and multiple turns, one-line streaming and async, as well as automatic tool execution and optional provider web-search integration (with citations for models that support them). Prompt caching is built in for the models that can use it, and you only need to supply the relevant API keys as environment variables for whatever providers you want to reach.\\n\\nA minimal workflow looks like this:\\n\\n```python\\nfrom lisette import *\\n\\nmodels = [\\'claude-sonnet-4-20250514\\', \\'gemini/gemini-2.5-flash\\', \\'openai/gpt-4o\\']\\n\\nfor model in models:\\n    chat = Chat(model)\\n    res = chat(\"Please tell me about yourself in one brief sentence.\")\\n    display(res)\\n```\\n\\nThe same interface works no matter which provider you point at, and the library takes care of all provider-specific quirks. From there you can pass multi-message lists or image bytes, prefill responses (for supported providers), register tools with Python type hints, enable search with a single flag, or switch to `AsyncChat` for concurrent apps and combine it with streaming plus tooluse.', role='assistant', tool_calls=None, function_call=None, reasoning_content='**Summarizing Lisette library**', provider_specific_fields={'refusal': None, 'reasoning': '**Summarizing Lisette library**', 'reasoning_details': [{'format': 'openai-responses-v1', 'index': 0, 'type': 'reasoning.summary', 'summary': '**Summarizing Lisette library**'}, {'id': 'rs_69246a835c60819093915586b5cca831', 'format': 'openai-responses-v1', 'index': 0, 'type': 'reasoning.encrypted', 'data': 'gAAAAABpJGqGYsDNbYS-61Ut294X2nOc5FAtsC24bskc4bP6ahPpXNTny4uUrilej_YdxKfdY6Zv4Ow38iVu2OtrvGNfe_cq6-dBUCys5sZmScvqNiYaXRSdAoI11hvvgj6lzcZ7ENBoUePNJ9omxmt1zOVXIMfjIRh-pmguotEWtpEHA2xRnY37fkM4vIrbHtiJcq-BAqRoqH-Y2zzwKSyxO1nLbi8HS6S4RJKcXza5KQ9F1uLjVREb47AGEMgfLQIb8W_f1PsMGlGGFAV8ElzBsWWixeUvr9gE_GobBlZFp3cWz20fooSokSXHqZgXTNYwHVSxnjIQgRq0YcjWd1d5SYlDLSzE_5TXl-m7Ed7YvKeNK5RQo1IGJJTNqJ8chMVrwFmiDs7jJBQepWDcJSZ2B0VjstlaB9whAfhCQcdRuF2EekJhEIaI1YzKRYaObd9_885x_BVRwCqCwWyzHIMCSwg3pbJ4DO7wH5ZfSwhx3FZllRRRuIFYIVoOJID5mNupwYHjE5c7GDuFqmf2V0eMEB5fe6Y_PoWoMM0W5KdviE78CjI0mzT0ROjgpWrWvU5A77AkbM53Xcr9XfdapDDTp6cx5bhZjdrCl9-0Td_Cr2MAOzY6lesu6G-FKgPcqI4eRl9YD3fvqXUv5jdTLxfy_aPh_lKnq8dUS7qb1O01raW8yvVIbU6iIPX_UWK-oVT2TnieLIsABHr8fwzHnieeEvzwv9BwUXHD2OXXsXHZ294R9en4abx4b1IlwvtTGuUetd1Q1LU_N_exkMQNYZUugg3FeDMydOADcR1-4wNHhEpGEEYL9pWtIhw5OLNey0py0ECEfX5RTIYFdwF1OmnXcIgRkKc-gaU8xNP1pfiJct9bA_jX_b6HH5-KtYgaVS_BY44g1ATYTw6fpvWbpAtj4AhaBYApFcYOEJ_b85ZrYdkXFtegvAb9Wo-56z-FiXIKhh3r7TQm8p87fgizYS-o1VKoGSNf1JnVb5BEtaigykB2AXtczRwShj_N14kjNPoxPjGa6H6_hJ84QWNm-SiO--Gwok_2dhmnx_sgajVmJNxvzQCNjYGFjTEBjRa2NkGL-MZm0M21brUAraJRgfLCEnuhbVtDIryknxGgGOWS53Tx5ijLCNH40rPUTwrTvpeL_vpY6erHmjwQoPiwgtTC-y4Wx184XW948YMRF3ktNPB_WHbudtakECnb195BDfrCWnQ9VNOzHcP8nfGAUdBXfi2w4Iq8PIoEZ8k6AL7s9SDh1k84-z7q9h-WPx_TNdD35N8JcaRkTT-6xFwK7XQryachIfrLh2FQq9DQHskLXD1RDOrwgdwfNPjjGxNZP8Z7xuEKsYtzAvCeRYQNSTpPISj9FYdB-qUWAetW1r9nFvrPwUrXP6HvmVCz5druPanJHnYP6wk5hdlea0Om8HPFVclHKrehPCkjaD_W-2XlwoXCzquDAMDdTBHkjWduyijkvL7RJ36USNS8Gj3TSzUEeoBWmwmyxOF9qrlvtFn-tgz-OrMW53JvrGUcBJkVqIEtPZ21Af8JYznbmrbt3zjYMokiZ5zPSoCewcjsUl6J4OH5lVgaxT8='}], 'reasoning_content': '**Summarizing Lisette library**'}), provider_specific_fields={'native_finish_reason': 'completed'})], usage=Usage(completion_tokens=422, prompt_tokens=10578, total_tokens=11000, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=64, rejected_prediction_tokens=None, text_tokens=None, image_tokens=0), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None, video_tokens=0), cost=0.0174425, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0.0132225, 'upstream_inference_completions_cost': 0.00422}), provider='OpenAI')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc.add_vars('lisette_md')\n",
    "bc(\"Can you tell me about the library now, based only on the variable, elevator pitch plus example code from the source. I know you are being Socratic but please give answers and not questions on this one.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a8c804",
   "metadata": {},
   "source": [
    "**Prompt (openrouter/openai/gpt-5-codex):** Can you tell me about the library now, based only on the variable, elevator pitch plus example code from the source. I know you are being Socratic but please give answers and not questions on this one.\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Lisette is a higher-level wrapper around the LiteLLM Python SDK that smooths out the rough edges of working with many different LLM providers. LiteLLM already unifies 100+ models behind an OpenAI-style API, but it stays fairly low level: you juggle your own histories, format multi-modal prompts, wire search and tool hookups, and so on. Lisette automates almost all of that while still letting you keep fine-grained control when you need it. You get a drop-in `Chat` class for stateful conversations across any LiteLLM-supported model, simple helpers for mixing text, images, and multiple turns, one-line streaming and async, as well as automatic tool execution and optional provider web-search integration (with citations for models that support them). Prompt caching is built in for the models that can use it, and you only need to supply the relevant API keys as environment variables for whatever providers you want to reach.\n",
    "\n",
    "A minimal workflow looks like this:\n",
    "\n",
    "```python\n",
    "from lisette import *\n",
    "\n",
    "models = ['claude-sonnet-4-20250514', 'gemini/gemini-2.5-flash', 'openai/gpt-4o']\n",
    "\n",
    "for model in models:\n",
    "    chat = Chat(model)\n",
    "    res = chat(\"Please tell me about yourself in one brief sentence.\")\n",
    "    display(res)\n",
    "```\n",
    "\n",
    "The same interface works no matter which provider you point at, and the library takes care of all provider-specific quirks. From there you can pass multi-message lists or image bytes, prefill responses (for supported providers), register tools with Python type hints, enable search with a single flag, or switch to `AsyncChat` for concurrent apps and combine it with streaming plus tooluse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec789795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hereâ€™s a quick elevator pitch based on what I just read:\n",
       "\n",
       "**llms.txt** is a simple markdown convention for websites to expose LLM-friendly documentation in one predictable place. By keeping `/llms.txt` conciseâ€”an H1 title, a short summary, and curated links to clean `.md` versions of important pagesâ€”it sidesteps the noise of full HTML sites and makes it easy for language models (and their tooling) to grab high-value context. It complements, rather than replaces, things like sitemaps or robots.txt, and comes with tooling (CLI, plugins, etc.) to help teams expand the spec into richer context bundles.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `gen-1763994283-CmXtYOkchauXvnyPigVQ`\n",
       "- model: `openai/gpt-5-codex`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=137, prompt_tokens=16189, total_tokens=16326, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None, image_tokens=0), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None, video_tokens=0), cost=0.02160625, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0.02023625, 'upstream_inference_completions_cost': 0.00137})`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='gen-1763994283-CmXtYOkchauXvnyPigVQ', created=1763994284, model='openai/gpt-5-codex', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hereâ€™s a quick elevator pitch based on what I just read:\\n\\n**llms.txt** is a simple markdown convention for websites to expose LLM-friendly documentation in one predictable place. By keeping `/llms.txt` conciseâ€”an H1 title, a short summary, and curated links to clean `.md` versions of important pagesâ€”it sidesteps the noise of full HTML sites and makes it easy for language models (and their tooling) to grab high-value context. It complements, rather than replaces, things like sitemaps or robots.txt, and comes with tooling (CLI, plugins, etc.) to help teams expand the spec into richer context bundles.', role='assistant', tool_calls=None, function_call=None, reasoning_content='**Summarizing findings from URL read**', provider_specific_fields={'refusal': None, 'reasoning': '**Summarizing findings from URL read**', 'reasoning_details': [{'format': 'openai-responses-v1', 'index': 0, 'type': 'reasoning.summary', 'summary': '**Summarizing findings from URL read**'}, {'id': 'rs_69246aaf27f88190a8f1d7dfbecfd315', 'format': 'openai-responses-v1', 'index': 0, 'type': 'reasoning.encrypted', 'data': 'gAAAAABpJGqvHpscX8hL3VJ_nvzsEj_f33L5zljMqia3Vl6k1oR4GDfAUn2gYOsOD0Z1NLENN3TgIQIx3GaN4yCVgScU1HeMcL2ZOetlMuXgf1k4gi0l0QrcSW90kJIyZlAlbDPZkMq_rXyAKEhw3KDCd-x-j73geF13BbA7nO5mVegZYQBYVcRpaJAPVf9wAgQ9DiSS1KRUWrQAEspOZ2d40FySaoc0QFAZJBIfzWAUbg4qXQFgAnlU9T1y0ehlV8KtTneUA_OajdOklYGxkO6VCo8TUEFQzYOzga3pj-h-_M7PJR3jWWJLAe07zt0_pT8wO-Qjj22bRhS9XDY2pYKL5xVpddThI0OoJR2sOEoHWVEAMXPKiwhy9cxXoWt4kbb9wyUAS_9Ou8CCrrd0E07zyXMifMmkSGYsazF8xrlMyqM38iXZHvGkiuhxWsYbP04aA0OZ9zblnGhayx-GeLCIaDkzJfFy3nRGcliwwXx97pzEpzPKW136mCvI7_rfjAH6JlztSz9WkgBbNyUV47_Viie_5cYb4waCy1gCjUPq3Xve1AFlnEFFl3JOTJJzmJIWCZxDy_GFkrZ67wXc6UfPak4r0o4zw1S9cddl91bA_J2XmZPDuc3LHkkDMDICbCd9G7R94lMlexNMsIf82uVJsBlM2xPGBFFIuw1lrtyl7fy-E67_WAR2LZ2Q9FufqXClU-qaEW0l80iloWtHKkrOrax_KLPoqXbyFSNu01LQLAsDScSwLWu7s7ws22mdm28I2cpiy0XTDK6jxF0_iSzbzCxJ9w9Yii-HGv4DJ5mh9VmPf7Glp34MR1gJvwCyxG7_H3UsPsxDQYqh_M42Bh7tWFA3lY5k2zt4DzZCDqonTFyrl_62Yt1R4MjjUz9Rp_AV23yOxiUhkBpGdsyzzb_l2LuHh3GvFNF_o21fGstMwsYEVceSl_V4FZNKA-3CqozZ2ubVMOPB'}], 'reasoning_content': '**Summarizing findings from URL read**'}), provider_specific_fields={'native_finish_reason': 'completed'})], usage=Usage(completion_tokens=137, prompt_tokens=16189, total_tokens=16326, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None, image_tokens=0), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None, video_tokens=0), cost=0.02160625, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0.02023625, 'upstream_inference_completions_cost': 0.00137}), provider='OpenAI')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/markdown": [
       "`llms.txt` is a proposal for every website to publish a concise, LLM-friendly Markdown companion at `/llms.txt`. It gives models a curated overview, links to clean `.md` versions of key pages, and follows a predictable structure (H1 title, short summary, optional detail, then labeled link sections). The goal is to make it easy for language models to grab high-signal context without wrangling messy HTML, while staying compatible with existing web standards like `robots.txt` and sitemaps.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `gen-1763994333-LwPkgTQX9ikegpC1NQti`\n",
       "- model: `openai/gpt-5-codex`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=240, prompt_tokens=16189, total_tokens=16429, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=128, rejected_prediction_tokens=None, text_tokens=None, image_tokens=0), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=13440, text_tokens=None, image_tokens=None, video_tokens=0), cost=0.00751625, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0.00511625, 'upstream_inference_completions_cost': 0.0024})`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='gen-1763994333-LwPkgTQX9ikegpC1NQti', created=1763994333, model='openai/gpt-5-codex', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='`llms.txt` is a proposal for every website to publish a concise, LLM-friendly Markdown companion at `/llms.txt`. It gives models a curated overview, links to clean `.md` versions of key pages, and follows a predictable structure (H1 title, short summary, optional detail, then labeled link sections). The goal is to make it easy for language models to grab high-signal context without wrangling messy HTML, while staying compatible with existing web standards like `robots.txt` and sitemaps.', role='assistant', tool_calls=None, function_call=None, reasoning_content='**Preparing final summary**', provider_specific_fields={'refusal': None, 'reasoning': '**Preparing final summary**', 'reasoning_details': [{'format': 'openai-responses-v1', 'index': 0, 'type': 'reasoning.summary', 'summary': '**Preparing final summary**'}, {'id': 'rs_69246ade87e08190b658d3596ceebb66', 'format': 'openai-responses-v1', 'index': 0, 'type': 'reasoning.encrypted', 'data': 'gAAAAABpJGrmisTOHp0MQTlM6ZvMGeFvlGZxWDruUqKEfsaWP7QXZ5ctSP66qu1oot4nGymtMVbYu-twzsgwm4zTVWgcM57zAg1GH6v0UXJyV-ZFQnfmSZRqmAO61SMWdNBTgKGnjPiw92ST1T-pq5huNq4m1Nwz39sJgV5BOb0B9eMDzyYYhn8KQgc8c0wEJqwz9rr-z35V9GCOSEk72ouUW1Ob-v-EmUWRUtSg1HlS1FRIfhUr3pSv6bvnDAwkmVhueC07hbL8eaDnU_xITK6FLz6wEz7MZbjkXUAFx24kcqT9mbPsPbAepsQ_Yp8SSlDcbWmQWvHCdxMs0DMf9_pKqB2vR5ysvejmsA8Jbei2y7uQKXCfSPgblvZg3V_ld_feRFFmqdD-V044syExRMLysYCfP4jLzF35zrr4fy9vz9wXJ-VAT_NNhd-0T9VmTimZeq1uJL10pDnnPsQJBLjlZbN88_Yy5dN81ZcbExN7Df_Xwih5ZhJW3jUfNSpfCB4R9tZGEdqttrHGYLGlYY5K1OpVcJCXrUtDRnD3zKbSmxbrlwTG_InWgiygysyiC0WTTLxxprWYov722oyysz8wopQkNRIXOguCevzP4_Xa-z1MZTVwfQ8yg8he0V7Cvc1P-mGUCiz4Qdzb6TLqXY66qJwRi7pFvjo_wrQekKe6Ezo7q3sODT5mV7l8Aenpghs_KiarHWJl1ntplsDR4xrDUutcTQpONlZTwm1L-dP5qE6agMvoPV9Hfy6c7f52i_ZW6_lu67MBbcZiKWTRiS0esUx-ZXaMpdGFy8UZxhSXgfURIleXBkejUY_msm8CiUVsRSetyxeDyV_hCV_6OuIP5-3HMhBsjjhelJieBwefJKFd0XvRQ_BMJiEGZGaZY3pdqBkflFmIbXRobUG1zfo_pjRsdhFzcLnAJJxciulBCZXq3II7cZwS8EEEaXXCypD845xXSFntyY_ZPK6PO6LtEhMF61xxv9MHkbSD8k7NuRQAwQXke_eS2ZHAfHdQbmOtP8D19HwKbF_d_4QTApOeJBLX5kz8pN5ZdLUbuUcCMpSL-waJPDOzUfwYmj5U6hLDvTOUARqmzyYrQm86F1G2rRUkMKlvTMhY8xXRY5OWMTm_uLO_epUSc62rE_HenNK8gTvAbh83fR2VBeUIF8sewqBjuiVEVA9KHKeTsyptt0N6FuTI16ZqeWd11N6o_KjCX5whDicUzMM-X7UFu1kWlMUwRp0jDaRwQxaFEx5EbEj-fsrEFFK9BFw5YSvTjSjZuYUF5QsQW6Xlrg3grDDtcO0ksePicsgSj0wbL4W3aEHRskuvP7kWyyUxEMnA-a96jaVr65E5kMbxY4P8WjYCXEar2b07GUN5q5Bdtz-qBTbQuM38WRYBePb5nhjnSWy2Ai7Skm0G0OoYyWKW7WdN5j1N5FwmHV7SGzUAxVJH9Z1v43iO0BVQ9aI33bDR_0PvFELFsI0j5BcwvhUIg54QJ6fiP6Iy_qsDTWra--nRYitme47Qf0IIWTkUVjSvYkOWt9_r_T62L6QsdHjLmw2i3k8smEjub_hCST0Mq2Zz-e3ZTv5TWgcKdtLj1Y7u5BQjnrcZG1DCxDYWeBYkBJ6vRYqKn8rX3KB0pmujodX2VoLdUOUbw3-gPeV9eL-ogCxx1X2Rr0UKMxOSQw_0FOSog_0iNw6eFMYv-bkPBWrkU-pED3fUpy4='}], 'reasoning_content': '**Preparing final summary**'}), provider_specific_fields={'native_finish_reason': 'completed'})], usage=Usage(completion_tokens=240, prompt_tokens=16189, total_tokens=16429, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=128, rejected_prediction_tokens=None, text_tokens=None, image_tokens=0), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=13440, text_tokens=None, image_tokens=None, video_tokens=0), cost=0.00751625, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0.00511625, 'upstream_inference_completions_cost': 0.0024}), provider='OpenAI')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc(\"Can you use tools? For example can you read https://llmstxt.org/index.md and tell me about it? Fetch it, don't store it, give the elevator pitch please.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01806637",
   "metadata": {},
   "source": [
    "**Prompt (openrouter/openai/gpt-5-codex):** Can you use tools? For example can you read https://llmstxt.org/index.md and tell me about it? Fetch it, don't store it, give the elevator pitch please.\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "<details class='tool-usage-details'>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"call_lAFPpDr5RciSYwMGKlzHua8Y\",\n",
    "  \"call\": {\n",
    "    \"function\": \"read_url\",\n",
    "    \"arguments\": {\n",
    "      \"url\": \"https://llmstxt.org/index.md\",\n",
    "      \"as_md\": true,\n",
    "      \"extract_section\": false,\n",
    "      \"selector\": \"\"\n",
    "    }\n",
    "  },\n",
    "  \"result\": \"# The /llms.txt file\\nJeremy Howard\\n2024-09-03\\n\\n## Background\\n\\nLarge language models increasingly rel<TRUNCATED>\"\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "`llms.txt` is a proposal for every website to publish a concise, LLM-friendly Markdown companion at `/llms.txt`. It gives models a curated overview, links to clean `.md` versions of key pages, and follows a predictable structure (H1 title, short summary, optional detail, then labeled link sections). The goal is to make it easy for language models to grab high-signal context without wrangling messy HTML, while staying compatible with existing web standards like `robots.txt` and sitemaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2873526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "`fhdaisy` is a thin Python layer that brings DaisyUIâ€™s Tailwind-based component set directly into FastHTML apps. Instead of sprinkling HTML strings like `<button class=\"btn btn-primary\">`, you work with Python-first components (`Btn`, `Card`, `Alert`, etc.) that automatically apply the right DaisyUI classes and underlying HTML tags. Modifiers are shortened (`-primary`, `-outline`, `-sm`), yet expand to the full DaisyUI class list under the hood, keeping your UI code concise and readable. Extras in `fhdaisy.xtras` offer ready-made helpers for recurring UI patterns (accordions, forms, â€¦), while still letting you compose or extend components as needed.\n",
       "\n",
       "Typical flow:\n",
       "\n",
       "```python\n",
       "pip install fhdaisy\n",
       "```\n",
       "\n",
       "then:\n",
       "\n",
       "```python\n",
       "from fasthtml.common import *\n",
       "from fhdaisy import *\n",
       "\n",
       "page = Div(\n",
       "    Card(\n",
       "        CardBody(\n",
       "            H2(\"Welcome to FastHTML + DaisyUI\"),\n",
       "            P(\"This card is built from Python components, not raw HTML.\"),\n",
       "            Btn(\"Click me\", cls='-primary -sm')\n",
       "        ),\n",
       "        cls='-compact -shadow'\n",
       "    )\n",
       ")\n",
       "\n",
       "print(page)\n",
       "```\n",
       "\n",
       "The snippet yields well-structured HTML where each element already carries the expected DaisyUI classes, so you can focus on layout and behavior rather than CSS glue code. For a deeper diveâ€”including advanced components, helper utilities, and live demosâ€”check the projectâ€™s documentation site linked from the README.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `gen-1763994357-VoP1BBTW593bbYjKHcTL`\n",
       "- model: `openai/gpt-5-codex`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=631, prompt_tokens=18980, total_tokens=19611, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=320, rejected_prediction_tokens=None, text_tokens=None, image_tokens=0), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=17664, text_tokens=None, image_tokens=None, video_tokens=0), cost=0.010163, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0.003853, 'upstream_inference_completions_cost': 0.00631})`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='gen-1763994357-VoP1BBTW593bbYjKHcTL', created=1763994357, model='openai/gpt-5-codex', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='`fhdaisy` is a thin Python layer that brings DaisyUIâ€™s Tailwind-based component set directly into FastHTML apps. Instead of sprinkling HTML strings like `<button class=\"btn btn-primary\">`, you work with Python-first components (`Btn`, `Card`, `Alert`, etc.) that automatically apply the right DaisyUI classes and underlying HTML tags. Modifiers are shortened (`-primary`, `-outline`, `-sm`), yet expand to the full DaisyUI class list under the hood, keeping your UI code concise and readable. Extras in `fhdaisy.xtras` offer ready-made helpers for recurring UI patterns (accordions, forms, â€¦), while still letting you compose or extend components as needed.\\n\\nTypical flow:\\n\\n```python\\npip install fhdaisy\\n```\\n\\nthen:\\n\\n```python\\nfrom fasthtml.common import *\\nfrom fhdaisy import *\\n\\npage = Div(\\n    Card(\\n        CardBody(\\n            H2(\"Welcome to FastHTML + DaisyUI\"),\\n            P(\"This card is built from Python components, not raw HTML.\"),\\n            Btn(\"Click me\", cls=\\'-primary -sm\\')\\n        ),\\n        cls=\\'-compact -shadow\\'\\n    )\\n)\\n\\nprint(page)\\n```\\n\\nThe snippet yields well-structured HTML where each element already carries the expected DaisyUI classes, so you can focus on layout and behavior rather than CSS glue code. For a deeper diveâ€”including advanced components, helper utilities, and live demosâ€”check the projectâ€™s documentation site linked from the README.', role='assistant', tool_calls=None, function_call=None, reasoning_content=\"**Summarizing fhdaisy Python wrapper**\\n\\nI'll capture that fhdaisy is a Python wrapper bridging DaisyUI with FastHTML, offering component classes like Btn and Card, plus shorthand modifiers and extra form helpers. I'll include an install note, a concise code example, and point to documentationâ€”all without naming the tool calls or being Socratic.\", provider_specific_fields={'refusal': None, 'reasoning': \"**Summarizing fhdaisy Python wrapper**\\n\\nI'll capture that fhdaisy is a Python wrapper bridging DaisyUI with FastHTML, offering component classes like Btn and Card, plus shorthand modifiers and extra form helpers. I'll include an install note, a concise code example, and point to documentationâ€”all without naming the tool calls or being Socratic.\", 'reasoning_details': [{'format': 'openai-responses-v1', 'index': 0, 'type': 'reasoning.summary', 'summary': \"**Summarizing fhdaisy Python wrapper**\\n\\nI'll capture that fhdaisy is a Python wrapper bridging DaisyUI with FastHTML, offering component classes like Btn and Card, plus shorthand modifiers and extra form helpers. I'll include an install note, a concise code example, and point to documentationâ€”all without naming the tool calls or being Socratic.\"}, {'id': 'rs_69246af674dc81908731bb08f95305d0', 'format': 'openai-responses-v1', 'index': 0, 'type': 'reasoning.encrypted', 'data': 'gAAAAABpJGsA9C8OBajBvroSS_weOnK99JM_woGZwp83AAVhcoqNhUnZMPSc0jwWWeb4yd4q5C_9-LjQsyzif5E11Etx4CEVL-MYuSrOEhzR4NXBmRMV84TqsQhl9D5fw6HqhYcded_4pdVW3ZWVTmyD-p0RV4qaYlzT70yxSvhdeBHPDXRRjrFhbvNWFB2-bEg9DbUN_O7A9o7xdELPfJ-j5iX2LSpRcDEsxXkfCWvVNt9N26AD3Vs0IcJlRDEvtrBnYRM3IHk4rs9JWyAc0XwU1rf0v0zIfAjVB97MTzKXiiLW6-p27y8b4pzzxQMBHuNt9PUYX4qefxXnCIZ3znUxOcCvJknTs99NdTKWUcxGLRVysY9eTAb1GJnLi8ibhT_RgDfp-dqShBmRjiGt2vkNaIJUv7A-TWOnxlZwbthPv-lszKpHm4C17cEEHsU5GI5MfGuUvfDgjW0aFyWQQeUnUcFAitxoul5n4A5m3dCUoemoIcnVm21CiQjfWNlzW6S5hWaLtL1ok5Dc86NiBLLv_vegeG9NC4FuSf31PcY9e3yJFLWNhfOzAnGnRfiYMwmdm3lzf7o82J8ITzevVMdvuQ47AsKX9Zd0Uc4SOR9-E5W56lPBiX24F4duCxy7Zf3ivA7WAEYOfxmP0Qa96q21_Ppx6yEx5dH-t5SqOEIvXEP23MKBfkGW_Q0YP31MUugvwi9cOjfA2Tzyh2YRuoaEs2ZiPoZetkhfwVcH1YfJs7JHrs2um9TNfQX_VzSM0NAS0EmOOmvBkyJkhyAHR3tDprXP_HxJHS4mQAXX22WkeH_-ofB8cYzU7E5KEYJtCRPZYpbIRmt06GBd5E6IJ8zkB_I7ofFp657e-z3BO6nQDa5yk1zn2A2jK-6W4G1PwcjIoLM1W3FiqmtuVuGf25f_ronPumacMy1OI5ZuX3R-VvyVcsE0JIA2POYC1MwnQey0UDzb5uuhqcAvNP-cLxlDDon7AljKlS9lwqbk8IS1djhWJAM9ZNiPiFoaprUcNLxRvKnYC4MePBplQbPq3wkgcoZSZuaanl4XxWcEPjCC612p-7rOPDjoeTyG1dFx7JXAcRFQ3w1U5jMMgqw1_-qU2i0TLS-z6U8MWvrOreghWTLXxLwK6LH_MuhMFR_I4MNtDp3Yp70ErIHReB9sTQ6ru7rZK_5vBboSk4KYf62ezqImLFxyPUQqTUpNoMJM7c6Ao3w6ZcPN7AhTiAuA9Cu-CwVGpuc9yZOhNEA9q6eaOe28_a3bHFjHhHpqmBDngDfW3TK_K_9zRqfrqoWQSa2BNgx9XygBRzp2c5SIgCfwYhHz64EdBFEr6_WRKlrIjXDzuR38BWix5yRPt35Gi4f4QUIAIvGuntiBtrCmJEtp1XqWK7iqL3ong_ZXtzH9sJguZcIgQcIMa6xPKcru_Th29obHP5sVcAWgaBSkm6y3mIQDtLYusUsTkbvhwPpjff7qm3671s_ls7TAtpq1r90EIQK11GLjDGKIjx3QHYRgCys1klMzxRkMVGzLqcfXyhvwE-JpFmf3wxxPLMMOscMKlOYfGQd9AXrA_KCaY6Hj47TFjXTKWNbJG3Xt_gkEykxwK7EqeAimGCll2kEzrLltXpDixGXH7AzBz2ObEG69QDVIlhuaX6YpzWxPzVSVlj_mQDajAfqfAEKKzLWN7nlwC05JRtLDTHASNquoAQkODMqlJ7IFltsTQbtUiwqnSGvoCQfp2FZNASdnafp73_94tYgMVplRh-rtwvI7dETl4fwcP33Kq9mHX6EXsHXCBnibJv7yY-Z7pjUdY68XXCaVftX8jk7enOb0gtGLAM9TXA9eSuoL6LJckyOO52Bf20SFNklZvnZ8z4DKSM9BD13eE4xdgMjHK3aTg2KAITsRnKXm4MCrUxcMZb7szXdw3DIZFnUycgOOgqBTUzNUhT6CzDaUKzepwxEAJWGDXPjkMu7Rve03rYtFJILj6IOuzZaCnFy3Fr2qvip25Zchgvu69Ze43JrIG5Rp7m4qfqvQM5EnOjXJeurpK9Y_iTBocghJhWydEidWEKSRcMsDkWweuovH_JYJedHwQCsJQBNjwiVrJDaJ8qHAv_HKaVPhz-4hZ1E4iEvX6NA-nSOCkl-AVm0QRwIUbyRY79t7ZimEziH1Rta1xEjxT4xeJ6V4reH9mL3WaYtAYMtpQmxwWB2X1qP3_rk9nLBh9RgJ4ikCYT6W5_98N2eS0CVSZAepxiVoP8CIc8qfgs4oRT0It7silVxyIj1NQxN1kUwD-i8HCOj1qJaw0i8DiB8XhECKS5JdSkccy0DqEzAaLVSrxD_t20bfSb5W_TAwd6MISE0Pi2cmN9o27bD21a8mpWbNrOGK2jA_TF0FYUQs0Eg6NdJyaBUu8YPfOSTBHSRA_ewuLyKgkzLUCkuiZwTJVv2WZnASo3mN8atkSR7q2PjJ7etYUbKEM_RyTAQgqiMnDJnoa64qoHI6ruRAl5H4DDjTqYY0G4yeBRGfFkee9Lg_YlIGDBXwL560pPG95qovOnqv8PpZgFv0ANRnGekTA4A3F7nTd9cnt_DQ0kFPnFMsssW-m448hBIY45-JEz0LO-zlY1rWHR0CW9TPf99uzHPS9zPS4gl5xVUBhCZjvn1Mu7MC0_Kp7VwPFgXnjgQJs_vTlJvkilYixNbKC3Ffhs7Mjj97gmcU8cn_wJ88J9AXmBSzoDO3MSIfh88EPjPOAimz_n89abPnzavyinwBsKxgh5R-T6sjRi_bQNDEfh5Y7JkeNRSz6wJ-D-fU8TJIqzreXEMWvXWMkoPgzfEwtrg9MU7w07MWwAJKwal1gnwVyl1FNQTESZaerfDEQCotKi2zmwiclyH7QknAAV3w6fASlPOc50EprcjZbNvSoLPItStX_c7JuAr-Jpw2YqW6_XNk3v4bKg04drMBaG0N0iD3r64UKRiTryZG-dLMMo3Yhqzoq-m9GlMi7g-ixs0a62qBO8wUvU93Hi9piQw1iuiuiaTQJJUkNKsk'}], 'reasoning_content': \"**Summarizing fhdaisy Python wrapper**\\n\\nI'll capture that fhdaisy is a Python wrapper bridging DaisyUI with FastHTML, offering component classes like Btn and Card, plus shorthand modifiers and extra form helpers. I'll include an install note, a concise code example, and point to documentationâ€”all without naming the tool calls or being Socratic.\"}), provider_specific_fields={'native_finish_reason': 'completed'})], usage=Usage(completion_tokens=631, prompt_tokens=18980, total_tokens=19611, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=320, rejected_prediction_tokens=None, text_tokens=None, image_tokens=0), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=17664, text_tokens=None, image_tokens=None, video_tokens=0), cost=0.010163, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0.003853, 'upstream_inference_completions_cost': 0.00631}), provider='OpenAI')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc(\"Now use your tool and summarize https://raw.githubusercontent.com/AnswerDotAI/fhdaisy/refs/heads/main/README.md please - give a code example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ecc513",
   "metadata": {},
   "source": [
    "**Prompt (openrouter/openai/gpt-5-codex):** Now use your tool and summarize https://raw.githubusercontent.com/AnswerDotAI/fhdaisy/refs/heads/main/README.md please - give a code example\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "<details class='tool-usage-details'>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"call_7FvqavATiA7YIfVsneBjiPQH\",\n",
    "  \"call\": {\n",
    "    \"function\": \"read_url\",\n",
    "    \"arguments\": {\n",
    "      \"url\": \"https://raw.githubusercontent.com/AnswerDotAI/fhdaisy/refs/heads/main/README.md\",\n",
    "      \"as_md\": true,\n",
    "      \"extract_section\": false,\n",
    "      \"selector\": \"\"\n",
    "    }\n",
    "  },\n",
    "  \"result\": \"# fhdaisy\\n\\n`fhdaisy` is a Python wrapper for [DaisyUI](https://daisyui.com/) that\\nbrings its compone<TRUNCATED>\"\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "`fhdaisy` is a thin Python layer that brings DaisyUIâ€™s Tailwind-based component set directly into FastHTML apps. Instead of sprinkling HTML strings like `<button class=\"btn btn-primary\">`, you work with Python-first components (`Btn`, `Card`, `Alert`, etc.) that automatically apply the right DaisyUI classes and underlying HTML tags. Modifiers are shortened (`-primary`, `-outline`, `-sm`), yet expand to the full DaisyUI class list under the hood, keeping your UI code concise and readable. Extras in `fhdaisy.xtras` offer ready-made helpers for recurring UI patterns (accordions, forms, â€¦), while still letting you compose or extend components as needed.\n",
    "\n",
    "Typical flow:\n",
    "\n",
    "```python\n",
    "pip install fhdaisy\n",
    "```\n",
    "\n",
    "then:\n",
    "\n",
    "```python\n",
    "from fasthtml.common import *\n",
    "from fhdaisy import *\n",
    "\n",
    "page = Div(\n",
    "    Card(\n",
    "        CardBody(\n",
    "            H2(\"Welcome to FastHTML + DaisyUI\"),\n",
    "            P(\"This card is built from Python components, not raw HTML.\"),\n",
    "            Btn(\"Click me\", cls='-primary -sm')\n",
    "        ),\n",
    "        cls='-compact -shadow'\n",
    "    )\n",
    ")\n",
    "\n",
    "print(page)\n",
    "```\n",
    "\n",
    "The snippet yields well-structured HTML where each element already carries the expected DaisyUI classes, so you can focus on layout and behavior rather than CSS glue code. For a deeper diveâ€”including advanced components, helper utilities, and live demosâ€”check the projectâ€™s documentation site linked from the README."
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
