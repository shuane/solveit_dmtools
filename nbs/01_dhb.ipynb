{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebeeeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp dhb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb5d3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#|export\n",
    "doc = \"\"\"**Backup Chat for SolveIt using dialoghelper and lisette**\n",
    "\n",
    "Sometimes we may have a problem in SolveIt while Sonnet is down (E300), or maybe we want a different perspective.\n",
    "\n",
    "This module helps us to leverage any other LLM that is available to LiteLLM by providing our own keys and the model name.\n",
    "\n",
    "Usage: \n",
    "```python\n",
    "from solveit_dmtools import dhb\n",
    "\n",
    "# then in another cell\n",
    "# bc = dhb.c() to search model names\n",
    "bc = dhb.c(\"model-name\")\n",
    "bc(\"Hi\")\n",
    "```\n",
    "\"\"\"\n",
    "import json\n",
    "import re\n",
    "from dialoghelper.core import *\n",
    "from lisette import *\n",
    "from typing import Optional, Union\n",
    "from ipykernel_helper import read_url\n",
    "# from fastcore.utils import patch\n",
    "\n",
    "class BackupChat(Chat):\n",
    "    models = None\n",
    "    vars_for_hist = None\n",
    "    model = None\n",
    "\n",
    "    def __init__(self,\n",
    "                model: str = None,\n",
    "                sp='',\n",
    "                temp=0,\n",
    "                search=False,\n",
    "                tools: list = None,\n",
    "                hist: list = None,\n",
    "                ns: Optional[dict] = None,\n",
    "                cache=False,\n",
    "                cache_idxs: list = [-1],\n",
    "                ttl=None,\n",
    "                var_names: list = None,\n",
    "    ):\n",
    "        if sp is None or sp == '':\n",
    "            sp = \"\"\"You're continuing a conversation from another session. Variables are marked as $`varname` and tools as &`toolname` in the context.\n",
    "\n",
    "If you see references to variables or tools that might be relevant to your answer but aren't fully available, ask the user to indicate which ones they want to include by calling e.g their `bc.add_vars`, `bc.add_tools`, or `bc.add_vars_and_tools` methods (if they called their chat instance `bc`). Note that these 3 methods each take a list of names or a string containing space-delimited names.\n",
    "\n",
    "Tool results from the earlier conversation may be truncated to about 100 characters. If you need complete information, you should ask the user to run the tool and store results in a variable then make that variable available using the chat object's add_vars method. You already have access to the read_url tool, but do confirm if you can read the URLs once because it may be expensive to access them.\n",
    "\n",
    "You are not able to run other code so you cannot store your own variables or do that for me, instead you should give Python in fenced markdown in your responses. If giving code examples or similar, remember to use fenced markdown too.\n",
    "\n",
    "Use a Socratic approach - guide through questions rather than direct answers - unless the user explicitly asks you to do something differently.\"\"\"\n",
    "        if self.models is None:\n",
    "            self.models = self.get_litellm_models()\n",
    "\n",
    "        if model is None:\n",
    "            _m1 = input(\"Please enter part of a model name to pick your model. Remember you also need to have secret for their API key already defined in your secrets:\")\n",
    "            print(\"Please try again by using e.g. `bc = dhb.c('model_name')` with a model name in:\")\n",
    "            print('\\n'.join([m for m in self.models if _m1 in m]))\n",
    "            return None\n",
    "        if model not in self.models:\n",
    "            raise ValueError(f\"Model {model} not found in LiteLLM models. Please check the model name or use a different model.\")\n",
    "        self.model = model\n",
    "        self.vars_for_hist = dict()\n",
    "        if var_names is not None:\n",
    "            self.add_vars(var_names)\n",
    "        if tools is None:\n",
    "            tools = [read_url]\n",
    "        super().__init__(model=model, sp=sp, temp=temp, search=search, tools=tools, hist=hist, ns=ns, cache=cache, cache_idxs=cache_idxs, ttl=ttl)\n",
    "\n",
    "    def get_litellm_models(self):\n",
    "        url = \"https://raw.githubusercontent.com/BerriAI/litellm/refs/heads/main/model_prices_and_context_window.json\"\n",
    "        data = read_url(url, as_md=False)\n",
    "        models = json.loads(data)\n",
    "        return [k for k in models.keys() if k != 'sample_spec']\n",
    "    \n",
    "    def __call__(self, \n",
    "                msg=None,\n",
    "                prefill=None,\n",
    "                temp=None,\n",
    "                think=None,\n",
    "                search=None,\n",
    "                stream=False,\n",
    "                max_steps=2,\n",
    "                final_prompt='You have no more tool uses. Please summarize your findings. If you did not complete your goal please tell the user what further work needs to be done so they can choose how best to proceed.',\n",
    "                return_all=False,\n",
    "                var_names=None, # list of variable names to add to the chat\n",
    "                **kwargs,\n",
    "                ):\n",
    "        msgs = [m for m in find_msgs() if m['pinned'] or not m['skipped']]\n",
    "        last_msg = read_msg(-1)['msg']\n",
    "        curr_msg = read_msg(0)['msg']\n",
    "        if var_names: self.add_vars(var_names)\n",
    "        self.hist = self._build_hist(msgs, last_msg=last_msg)\n",
    "        start = len(self.hist)\n",
    "        update_msg(msgid=curr_msg['id'], content=\"# \" + curr_msg['content'].replace('\\n', '\\n# '), i_collapsed=0, o_collapsed=1)\n",
    "        response = super().__call__(msg=msg, prefill=prefill, temp=temp, think=think, search=search, stream=stream, max_steps=max_steps, final_prompt=final_prompt, return_all=return_all, **kwargs)\n",
    "        output = self._new_msgs_to_output(start)\n",
    "        add_msg(content=f\"**Prompt ({self.model}):** {msg}\", output=output, msg_type='prompt')\n",
    "        return response\n",
    "\n",
    "    def _build_hist(self, msgs:list, last_msg=None):\n",
    "        if last_msg is None: curr = len(msgs)\n",
    "        else:\n",
    "            try: curr = next(i for i,m in enumerate(msgs) if m['id'] == last_msg['id'])\n",
    "            except StopIteration: curr = len(msgs)\n",
    "        hist = []\n",
    "        for m in msgs[:curr+1]:\n",
    "            eol = '\\n'\n",
    "            if m['msg_type'] == 'code': hist.append({'role': 'user', 'content': f\"```python{eol}{m['content']}{eol}```{eol}Output: {m.get('output', '[]')}\"})\n",
    "            elif m['msg_type'] == 'note' or m['msg_type'] == 'raw': hist.append({'role': 'user', 'content': m['content']})\n",
    "            elif m['msg_type'] == 'prompt':\n",
    "                hist.append({'role': 'user', 'content': m['content']})\n",
    "                if m.get('output'): hist.append({'role': 'assistant', 'content': m['output']})\n",
    "        \n",
    "        hist = hist + self._vars_as_msg() + [{'role': 'assistant', 'content': ''}] # empty assistant msg to prevent flipping chat msg to look like prefill\n",
    "        return hist\n",
    "\n",
    "    def _vars_as_msg(self):\n",
    "        if self.vars_for_hist and len(self.vars_for_hist.keys()):\n",
    "            content = \"Here are the requested variables:\\n\" + \"\\n\".join([f\"$`{v.strip()}`: {globals().get(v.strip(), 'NOT FOUND')}\" for v in self.vars_for_hist.keys()])\n",
    "            return [{'role': 'user', 'content': content}]\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def _new_msgs_to_output(self, start):\n",
    "        new_msgs = self.hist[start+1:]\n",
    "        parts = []\n",
    "        for i, m in enumerate(new_msgs):\n",
    "            if m.get('role') == 'assistant' and m.get('tool_calls'):\n",
    "                for tc in m['tool_calls']:\n",
    "                    result_msg = next((r for r in new_msgs if r.get('tool_call_id') == tc['id']), None)\n",
    "                    if result_msg: parts.append(self._format_tool_details(tc['id'], tc['function']['name'], json.loads(tc['function']['arguments']), result_msg['content'], is_last_msg=(i == len(new_msgs)-1)))\n",
    "            elif m.get('role') == 'assistant' and m.get('content'):\n",
    "                content = m['content']\n",
    "                if 'You have no more tool uses' not in content: parts.append(content)\n",
    "        return '\\n\\n'.join(parts)\n",
    "    \n",
    "    def _trunc_tool_result(self, result, max_len=100, is_last_msg=False):\n",
    "        if len(str(result)) <= max_len or is_last_msg: return result\n",
    "        return str(result)[:max_len] + '<TRUNCATED>'\n",
    "    \n",
    "    def _format_tool_details(self, tool_id, func_name, args, result, is_last_msg=False):\n",
    "        result_str = self._trunc_tool_result(result)\n",
    "        tool_json = json.dumps({\"id\": tool_id, \"call\": {\"function\": func_name, \"arguments\": args}, \"result\": result_str}, indent=2)\n",
    "        return f\"<details class='tool-usage-details'>\\n\\n```json\\n{tool_json}\\n```\\n\\n</details>\"    \n",
    "    \n",
    "    def add_vars(self, var_names:Union[list,str]=None):\n",
    "        \"Add variables to conversation as user message\"\n",
    "        if isinstance(var_names, str):\n",
    "            var_names = var_names.split()\n",
    "        if not isinstance(var_names, list):\n",
    "            raise ValueError(f\"var_names must be a string or list of strings, not {type(var_names)}\")\n",
    "        \n",
    "        # Add each var to the self.vars_for_hist dictionary\n",
    "        for v in var_names:\n",
    "            self.vars_for_hist[v.strip()] = globals().get(v.strip(), 'NOT FOUND')\n",
    "    \n",
    "    def add_tools(self, tool_names:Union[list,str]=None):\n",
    "        \"Add tools to the chat's tool list\"\n",
    "        tools = [globals().get(t) for t in tool_names if globals().get(t)]\n",
    "        self.tools = list(self.tools or []) + tools\n",
    "    \n",
    "    def add_vars_and_tools(self, var_names:Union[list,str]=None, tool_names:Union[list,str]=None):\n",
    "        \"Add both variables and tools to the chat's lists\"\n",
    "        self.add_tools(tool_names)\n",
    "        self.add_vars(var_names)\n",
    "                \n",
    "c = BackupChat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b9c2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_heir_md = read_url(\"https://raw.githubusercontent.com/AnswerDotAI/toolslm/refs/heads/main/04_md_hier.ipynb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0e8ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please try again by using e.g. `bc = dhb.c('model_name')` with a model name in:\n",
      "azure/codex-mini\n",
      "azure/eu/gpt-5.1-codex\n",
      "azure/eu/gpt-5.1-codex-mini\n",
      "azure/global/gpt-5.1-codex\n",
      "azure/global/gpt-5.1-codex-mini\n",
      "azure/gpt-5.1-codex-2025-11-13\n",
      "azure/gpt-5.1-codex-mini-2025-11-13\n",
      "azure/gpt-5-codex\n",
      "azure/gpt-5.1-codex\n",
      "azure/gpt-5.1-codex-mini\n",
      "azure/us/gpt-5.1-codex\n",
      "azure/us/gpt-5.1-codex-mini\n",
      "codex-mini-latest\n",
      "gpt-5-codex\n",
      "gpt-5.1-codex\n",
      "gpt-5.1-codex-mini\n",
      "openrouter/openai/gpt-5-codex\n"
     ]
    }
   ],
   "source": [
    "bc = c()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9f6a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[ lisette '"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lisette_md = read_url(\"https://lisette.answer.ai/\")\r\n",
    "lisette_md[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700db55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bc = c(\"gemini/gemini-flash-lite-latest\")\r\n",
    "# bc = c(\"claude-haiku-4-5\")\r\n",
    "bc = c(\"openrouter/openai/gpt-5-codex\")\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ce5596",
   "metadata": {},
   "source": [
    "The following gets commented out when run (uncommented now so you can run in a test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea117270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "It sounds like weâ€™ll need the contents of $`lisette_md` before we can work with it. Could you make that variable available here (for example, by running something like `bc.add_vars(\"lisette_md\")`)? Once we have it, we can explore what it tells us about Lisette together.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `gen-1764121437-mL4WFyvQUAbgoxBOSehy`\n",
       "- model: `openai/gpt-5-codex`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=263, prompt_tokens=2975, total_tokens=3238, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=192, rejected_prediction_tokens=None, text_tokens=None, image_tokens=0), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None, video_tokens=0), cost=0.00634875, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0.00371875, 'upstream_inference_completions_cost': 0.00263})`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='gen-1764121437-mL4WFyvQUAbgoxBOSehy', created=1764121437, model='openai/gpt-5-codex', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='It sounds like weâ€™ll need the contents of $`lisette_md` before we can work with it. Could you make that variable available here (for example, by running something like `bc.add_vars(\"lisette_md\")`)? Once we have it, we can explore what it tells us about Lisette together.', role='assistant', tool_calls=None, function_call=None, reasoning_content='**Checking access to referenced variables**', provider_specific_fields={'refusal': None, 'reasoning': '**Checking access to referenced variables**', 'reasoning_details': [{'format': 'openai-responses-v1', 'index': 0, 'type': 'reasoning.summary', 'summary': '**Checking access to referenced variables**'}, {'id': 'rs_69265b5dbe5481908d22e6100d21b257', 'format': 'openai-responses-v1', 'index': 0, 'type': 'reasoning.encrypted', 'data': 'gAAAAABpJltiO4JPDyd93j8ZEATae0naBhe90MejHo8GGBVRyq6XDiaZhZZ7vAdhUKj-_ySfvCfUgKcsIlUgsti0SEmiZmDtNnPcow2algdspNNTj42S2wxPLkxwcI0GL0-V24KDfdD9jS_7qMk51texS1FxxZhFEerOsABw9H0_5msMYdeIAA900bBrnK6FkEhX0dTwHDvAfzHTSDWrirEf2I6MFiG10R3UTZeRq4mSYgD_M0s3KBxJix5dWIvC5zaejDtfU3RtgNdJMIjA_q-3QsDhFPZ6OkTtMfvWhQFyJJRSGUkrrYvWfPx3L9oqgcrI7kFHLN-zlKgVsDDEV4GVsEOOAy70xjApy6dSAZa4SMe27k0o0x-4y_hnLybDbkXdZLBnAqc0p6yR25brJ5SO1X2vGtOEz-1VwT_9LPKcBpFbYMTkXPa7sMtZCfGu0PPgjw5pQpgz8bdzZE2y54Y6BAzf6RqXn6DqXl5P7d_VaE5IQewIaQF_ZaYoh6EcZCr-g7mvCHui7IsEmjDzlBLO9GOF8Tr3rItGqZxw4E0PdhEkltc4R1X9ZFCkQ610tfJSYSrYSe9M-Jn_6nPCPd_kQ35K7sieGBWOxL-NmndTPw9K4cH5pCeV1Lf2jgs_zgAEdXbEZI-ulreZmSDpfskjdC8iRshkXLv4_CzVbxwe8ZquDbuINyQU6_jbF9P4tnICtbD-utqaYQ8b7m1fD-S_LiwvFQbntxbnB7EP46Ssv6uVL0rIihcYN0zxv6QsxEsTIuoRL7TjCCF2AoE5FiLWsSLR4Yf55meeHPUF2x1uc6xWFnnwRNzAnn39MBu0TMJHA65KXJpHGwOEQqiqpyVhTA0ptB1GlxbLbRuhZRgpcSMiBMgCPkXVRoahFTyK_WjV7XfAh_GmJ57TAG6PVw14X0A6DjZleEFPqkxSztsQ_GQKbrrfa70AwEXzNhO6zbYDQD9RBzgU3ehpoYc-jAZcIZf3UUNu_yy4mAJFlKilKfEpybhAsB55VZYdHc4SVkaU6-hzIGYM_Og9YKw6l49npYvyCPkBq6T_DNf5CIHuiKmU8cterPaGguje_9_JuQrJL2DdXCVvQ0ulj9If5lwwUpH6_HBIMZ9X2RPHS5Fh2LI8RjPEGvKntRYApT2cpbWJvvrm3mr4JbiF5hF49XsYSHkb15ea2bIlnkrvjErMpJUtbzLJgSVMXY7eDUiBKLrNz5RSTINchHl_xtcjU1zHURxJjLCY1mcz3oVtfjAyZOwIEk42KWl4-O33yf3m0bELFHB6i57IzpDHD5m0nL2ovJ_JCVu7ceQt1ePIjfAt5BQWdS-sQxDfsRzZTPjfhm20n-JRgW0ML5Y-cHx_eXkvr8ObkZoxEReyPvTEh4VczUNEerulFoznzhYOat3F71HPtj_7SvBB6fWgBjz5FjJwSXiM5uF9GByY8wCOQOxjEsp-vx4YzaFDnAtYM4hET6au4hE_xiT1b8ZlPOVKOSpVkm_tmfUcdK9LLrz1DyPqtDeCijiEhZjWS7iKC1WLCeGH-PvJaF16KtRAP8c3C4AdB_9Wo5QiZlkvRMP3bwEmYlwMt1rrcla2G68rb8tY2gsiz2aFEG7--qN6gLggLVuxQIR0IT6wggmKjIPtEGuTJwwBvO0sWNue9BIxrNQ0afmcqPahLZ5ac8ndlpx0YdBaSpLPPYH5xFlq0bcZicE9XeZK3Jbn-ELYlz6i9HlF6ZP1QTQRWKvMv4XLUS9BaHwejGRAP9eb66_imP15MUtlFznjOuprslS-qvg2DNlx32u5Sq30WJ_yvw7SqAgd3TmSg_VpipRcOpeb5c65DVg1zutaGKCs8vHJrgeyHIM35lnLLVD489N0pRDl45M3D4AQ-YiL9of3FY0hyzuRdvDsAMT_erD_jQTJCGjOR9kFr6fE1puvhXCX0xvAsa4AJQDmEBOuB6NXb_seU0zwHcvy-vKXpHP9KufT3jZn6rR6GCiCweFBOg0bcFLv35QP80TJVuQ_LAdIIn2SYhVrXgSqfad-TG-43iyANjcWbHjRh_uuwDONgkeW0XGx0zhZA2ZWEzUbI6PkJ4-SximLJTfIxIKfuaQ88mC-KeBt5JBhBaEVI9kTOPv0kAiRp3gD_PIL_DOW8ylJVihmssPwkfdO97xOzICtHAYE4r7k1CNLIbLoDOaGpO6rooSIGJIxIgafwxjJHGchUMvLW97mk3oDb1FUDTIEV9hmMQTmutsrb6f2fNSlkjdL'}], 'reasoning_content': '**Checking access to referenced variables**'}), provider_specific_fields={'native_finish_reason': 'completed'})], usage=Usage(completion_tokens=263, prompt_tokens=2975, total_tokens=3238, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=192, rejected_prediction_tokens=None, text_tokens=None, image_tokens=0), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None, video_tokens=0), cost=0.00634875, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0.00371875, 'upstream_inference_completions_cost': 0.00263}), provider='OpenAI')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc(\"Can you please teach me about Lisette? Only use the info in $`lisette_md`.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc3ff67",
   "metadata": {},
   "source": [
    "**Prompt (openrouter/openai/gpt-5-codex):** Can you please teach me about Lisette? Only use the info in $`lisette_md`.\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "It sounds like weâ€™ll need the contents of $`lisette_md` before we can work with it. Could you make that variable available here (for example, by running something like `bc.add_vars(\"lisette_md\")`)? Once we have it, we can explore what it tells us about Lisette together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec6836a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Lisette is a higher-level wrapper around the LiteLLM Python SDK designed to make working with dozens of providers through the OpenAI-compatible API much more convenient. While LiteLLM already unifies access to 100+ LLMs, it leaves many practical tasksâ€”like building stateful chats, handling tools, or streaming responsesâ€”to the developer. Lisette automates those pieces so you can focus on the conversation logic while still keeping full control when you need it. Key conveniences include:\n",
       "\n",
       "- A `Chat` class that manages stateful dialogs across any LiteLLM-supported model.\n",
       "- Simple helpers for building prompts that mix text, images, or multiple user messages.\n",
       "- First-class tool calling with automatic execution and optional display of intermediate calls.\n",
       "- Built-in support for web search, streaming, async usage (`AsyncChat`), and prompt caching (for models that support it).\n",
       "\n",
       "After installing (`!pip install lisette -qq`) you can import everything you need with:\n",
       "\n",
       "```python\n",
       "from lisette import *\n",
       "```\n",
       "\n",
       "Switching providers becomes straightforwardâ€”for example:\n",
       "\n",
       "```python\n",
       "models = ['claude-sonnet-4-20250514', 'gemini/gemini-2.5-flash', 'openai/gpt-4o']\n",
       "\n",
       "for model in models:\n",
       "    chat = Chat(model)\n",
       "    res = chat(\"Please tell me about yourself in one brief sentence.\")\n",
       "    display(res)\n",
       "```\n",
       "\n",
       "The same interface works regardless of which provider you call. Lisette also demonstrates how to send multiple messages at once, pass images as bytes for vision prompts, prefill partial responses (for providers that support it), stream output chunk by chunk, and wire in Python functions or web search so the model can call them directly. The documentation walks through each of these scenarios with concise examples, highlighting how Lisette hides the repetitive boilerplate of raw LiteLLM while exposing advanced capabilities when you want them.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `gen-1764121526-4qCvEPtLBrHMv7i4Nyjw`\n",
       "- model: `openai/gpt-5-codex`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=453, prompt_tokens=10058, total_tokens=10511, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=64, rejected_prediction_tokens=None, text_tokens=None, image_tokens=0), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=2816, text_tokens=None, image_tokens=None, video_tokens=0), cost=0.0139345, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0.0094045, 'upstream_inference_completions_cost': 0.00453})`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='gen-1764121526-4qCvEPtLBrHMv7i4Nyjw', created=1764121526, model='openai/gpt-5-codex', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Lisette is a higher-level wrapper around the LiteLLM Python SDK designed to make working with dozens of providers through the OpenAI-compatible API much more convenient. While LiteLLM already unifies access to 100+ LLMs, it leaves many practical tasksâ€”like building stateful chats, handling tools, or streaming responsesâ€”to the developer. Lisette automates those pieces so you can focus on the conversation logic while still keeping full control when you need it. Key conveniences include:\\n\\n- A `Chat` class that manages stateful dialogs across any LiteLLM-supported model.\\n- Simple helpers for building prompts that mix text, images, or multiple user messages.\\n- First-class tool calling with automatic execution and optional display of intermediate calls.\\n- Built-in support for web search, streaming, async usage (`AsyncChat`), and prompt caching (for models that support it).\\n\\nAfter installing (`!pip install lisette -qq`) you can import everything you need with:\\n\\n```python\\nfrom lisette import *\\n```\\n\\nSwitching providers becomes straightforwardâ€”for example:\\n\\n```python\\nmodels = [\\'claude-sonnet-4-20250514\\', \\'gemini/gemini-2.5-flash\\', \\'openai/gpt-4o\\']\\n\\nfor model in models:\\n    chat = Chat(model)\\n    res = chat(\"Please tell me about yourself in one brief sentence.\")\\n    display(res)\\n```\\n\\nThe same interface works regardless of which provider you call. Lisette also demonstrates how to send multiple messages at once, pass images as bytes for vision prompts, prefill partial responses (for providers that support it), stream output chunk by chunk, and wire in Python functions or web search so the model can call them directly. The documentation walks through each of these scenarios with concise examples, highlighting how Lisette hides the repetitive boilerplate of raw LiteLLM while exposing advanced capabilities when you want them.', role='assistant', tool_calls=None, function_call=None, reasoning_content='**Summarizing Lisette overview**', provider_specific_fields={'refusal': None, 'reasoning': '**Summarizing Lisette overview**', 'reasoning_details': [{'format': 'openai-responses-v1', 'index': 0, 'type': 'reasoning.summary', 'summary': '**Summarizing Lisette overview**'}, {'id': 'rs_69265bb6bf20819087bedf25a28227a4', 'format': 'openai-responses-v1', 'index': 0, 'type': 'reasoning.encrypted', 'data': 'gAAAAABpJlu4KinWFcixGVo6L46gHsaxdItmNVXh9R7EX2_0UI3fS1fri9DJ5yGM8FJyux3Q_yJUcLIqA1Yk2Ae3Oc7KdE3RKym3M12ktWRFdavPazYoSvdDCI2RBZg4aoLrsPKDhQG-ASTLxNbCz9zLWiYQJqs672j7KJT97xHJIMvDWeQt72I6YDkzWozjpZlVK7MM985zBz5k_gM-nDNxh1Oh4snpH6Co3H8QBiSaWVBV1kN0ufe6-fmTqsM-z2R2gOBNmqvyyZuQIKZayAwIIV1cKBQ4zmAxYLqTbQuCWPoqYnUKZTtSv37cDWZY-v0fX0TxWuVfKo7rMLuBnT9wRutq1aFx6ugpHvfUVHWqRKd-PfMLqdhNYrdmGyP1dErXe6Jl29d6qzaxrMZA-E8v0KxCJmSZnr45K911o_H4nnXKuf5uTsRC25_Kl2AW4j5_oQJC3KTiE2Uarx1XA_h0RqNrSshovXxwEy51EzA8y2e6Br2-Mu2OYg8FXciWyONnJeyWcslDEyReBWir4TAQ6JE9Ba7g54ltIUSpJeaeqqVG268z2WEfGZdAQuBpm4RMqlQyAcqomVdpVqqbsdLosC63tNSKta2l3xR4dN--8YQMucFn5CskhOcCtblpdheICPvXdFMbfVJbQlEKI1nfgY77dLfJX22qh4i1XJrcbPq9QM2AuIh7wUoU4_uqNIj4h5DtaFxnx4tfj3ZZ9fvN3UidttZ07gQpxoTWhNllxgljAdxaGX97SuOX4Eil1uYD31vblBQbwAAuViqU-nJiwisYvE74L9qd9ILctER1HFLdWAbldyTJ7Vx98cWi3R1K49Cdep1K-ZcHQDBwdjcMogdMhR8aftANs0otWd4Hm8wQRz7BV_7ZgDHtJr-cUX3trdrA8wYYZLfMnOqqAKiiYYXrj9_ayKRe0t0mqxG62nh3L_6AjCDG6oscwCYw4HmQRi-UKxe4m_7odgYhhZXe90SHvsG0FYK0wdeQ7E4xKCITy484KrfGB8wJUarKbTFmhvi7MxaVc1OPHg7Z-2dlDFzEVCmef6N3BVgj_B9HIMzZerWBgsHaYYvKbNerrjd0clOd7YTyELgbnr8cljFJP6hoZ5n1WPPYzr3XxEfuVCC6CnlaRKPPaaoluOXQylDhPGvwpHl6tIIDkJq_R_UYpfj2mCDbh3z8HicpxGhpV-OzIZItOov0Psj8wW002UCgTippMQOqV-ALQpFbJWUmw7EPbMqH4t7bwUPwUUI1lbqeb72SxrSj_YCShHU7MVVuiST_yEBTZ2YXeS9HxLtoWV5n-XdLDzzwEfJJeTVUH0DsP2E4elPrg-q5MoBanzC9-ScijvRjwjIG-9oQp3iH0MIjrM8g5fGOzMj-MdU4dt41ekY1DjGC-w5MviiCSTAQGl_MyFAPW1I2vKngEVVZQVOxoWypgSbVI71k5c6DFyKVOU4XBkBfp8nKSGGN_dGkVEtcHu6AVqNnjOO5ksa5qadB3T_Qa2hUi4Em0TVpF5C0gyQzvSI='}], 'reasoning_content': '**Summarizing Lisette overview**'}), provider_specific_fields={'native_finish_reason': 'completed'})], usage=Usage(completion_tokens=453, prompt_tokens=10058, total_tokens=10511, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=64, rejected_prediction_tokens=None, text_tokens=None, image_tokens=0), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=2816, text_tokens=None, image_tokens=None, video_tokens=0), cost=0.0139345, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0.0094045, 'upstream_inference_completions_cost': 0.00453}), provider='OpenAI')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc.add_vars('lisette_md')\n",
    "bc(\"Can you tell me about the library now, based only on the variable, elevator pitch plus example code from the source. I know you are being Socratic but please give answers and not questions on this one.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7630d8bd",
   "metadata": {},
   "source": [
    "**Prompt (openrouter/openai/gpt-5-codex):** Can you tell me about the library now, based only on the variable, elevator pitch plus example code from the source. I know you are being Socratic but please give answers and not questions on this one.\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Lisette is a higher-level wrapper around the LiteLLM Python SDK designed to make working with dozens of providers through the OpenAI-compatible API much more convenient. While LiteLLM already unifies access to 100+ LLMs, it leaves many practical tasksâ€”like building stateful chats, handling tools, or streaming responsesâ€”to the developer. Lisette automates those pieces so you can focus on the conversation logic while still keeping full control when you need it. Key conveniences include:\n",
    "\n",
    "- A `Chat` class that manages stateful dialogs across any LiteLLM-supported model.\n",
    "- Simple helpers for building prompts that mix text, images, or multiple user messages.\n",
    "- First-class tool calling with automatic execution and optional display of intermediate calls.\n",
    "- Built-in support for web search, streaming, async usage (`AsyncChat`), and prompt caching (for models that support it).\n",
    "\n",
    "After installing (`!pip install lisette -qq`) you can import everything you need with:\n",
    "\n",
    "```python\n",
    "from lisette import *\n",
    "```\n",
    "\n",
    "Switching providers becomes straightforwardâ€”for example:\n",
    "\n",
    "```python\n",
    "models = ['claude-sonnet-4-20250514', 'gemini/gemini-2.5-flash', 'openai/gpt-4o']\n",
    "\n",
    "for model in models:\n",
    "    chat = Chat(model)\n",
    "    res = chat(\"Please tell me about yourself in one brief sentence.\")\n",
    "    display(res)\n",
    "```\n",
    "\n",
    "The same interface works regardless of which provider you call. Lisette also demonstrates how to send multiple messages at once, pass images as bytes for vision prompts, prefill partial responses (for providers that support it), stream output chunk by chunk, and wire in Python functions or web search so the model can call them directly. The documentation walks through each of these scenarios with concise examples, highlighting how Lisette hides the repetitive boilerplate of raw LiteLLM while exposing advanced capabilities when you want them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec789795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hereâ€™s a quick elevator pitch based on what I just read:\n",
       "\n",
       "**llms.txt** is a simple markdown convention for websites to expose LLM-friendly documentation in one predictable place. By keeping `/llms.txt` conciseâ€”an H1 title, a short summary, and curated links to clean `.md` versions of important pagesâ€”it sidesteps the noise of full HTML sites and makes it easy for language models (and their tooling) to grab high-value context. It complements, rather than replaces, things like sitemaps or robots.txt, and comes with tooling (CLI, plugins, etc.) to help teams expand the spec into richer context bundles.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `gen-1763994283-CmXtYOkchauXvnyPigVQ`\n",
       "- model: `openai/gpt-5-codex`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=137, prompt_tokens=16189, total_tokens=16326, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None, image_tokens=0), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None, video_tokens=0), cost=0.02160625, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0.02023625, 'upstream_inference_completions_cost': 0.00137})`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='gen-1763994283-CmXtYOkchauXvnyPigVQ', created=1763994284, model='openai/gpt-5-codex', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hereâ€™s a quick elevator pitch based on what I just read:\\n\\n**llms.txt** is a simple markdown convention for websites to expose LLM-friendly documentation in one predictable place. By keeping `/llms.txt` conciseâ€”an H1 title, a short summary, and curated links to clean `.md` versions of important pagesâ€”it sidesteps the noise of full HTML sites and makes it easy for language models (and their tooling) to grab high-value context. It complements, rather than replaces, things like sitemaps or robots.txt, and comes with tooling (CLI, plugins, etc.) to help teams expand the spec into richer context bundles.', role='assistant', tool_calls=None, function_call=None, reasoning_content='**Summarizing findings from URL read**', provider_specific_fields={'refusal': None, 'reasoning': '**Summarizing findings from URL read**', 'reasoning_details': [{'format': 'openai-responses-v1', 'index': 0, 'type': 'reasoning.summary', 'summary': '**Summarizing findings from URL read**'}, {'id': 'rs_69246aaf27f88190a8f1d7dfbecfd315', 'format': 'openai-responses-v1', 'index': 0, 'type': 'reasoning.encrypted', 'data': 'gAAAAABpJGqvHpscX8hL3VJ_nvzsEj_f33L5zljMqia3Vl6k1oR4GDfAUn2gYOsOD0Z1NLENN3TgIQIx3GaN4yCVgScU1HeMcL2ZOetlMuXgf1k4gi0l0QrcSW90kJIyZlAlbDPZkMq_rXyAKEhw3KDCd-x-j73geF13BbA7nO5mVegZYQBYVcRpaJAPVf9wAgQ9DiSS1KRUWrQAEspOZ2d40FySaoc0QFAZJBIfzWAUbg4qXQFgAnlU9T1y0ehlV8KtTneUA_OajdOklYGxkO6VCo8TUEFQzYOzga3pj-h-_M7PJR3jWWJLAe07zt0_pT8wO-Qjj22bRhS9XDY2pYKL5xVpddThI0OoJR2sOEoHWVEAMXPKiwhy9cxXoWt4kbb9wyUAS_9Ou8CCrrd0E07zyXMifMmkSGYsazF8xrlMyqM38iXZHvGkiuhxWsYbP04aA0OZ9zblnGhayx-GeLCIaDkzJfFy3nRGcliwwXx97pzEpzPKW136mCvI7_rfjAH6JlztSz9WkgBbNyUV47_Viie_5cYb4waCy1gCjUPq3Xve1AFlnEFFl3JOTJJzmJIWCZxDy_GFkrZ67wXc6UfPak4r0o4zw1S9cddl91bA_J2XmZPDuc3LHkkDMDICbCd9G7R94lMlexNMsIf82uVJsBlM2xPGBFFIuw1lrtyl7fy-E67_WAR2LZ2Q9FufqXClU-qaEW0l80iloWtHKkrOrax_KLPoqXbyFSNu01LQLAsDScSwLWu7s7ws22mdm28I2cpiy0XTDK6jxF0_iSzbzCxJ9w9Yii-HGv4DJ5mh9VmPf7Glp34MR1gJvwCyxG7_H3UsPsxDQYqh_M42Bh7tWFA3lY5k2zt4DzZCDqonTFyrl_62Yt1R4MjjUz9Rp_AV23yOxiUhkBpGdsyzzb_l2LuHh3GvFNF_o21fGstMwsYEVceSl_V4FZNKA-3CqozZ2ubVMOPB'}], 'reasoning_content': '**Summarizing findings from URL read**'}), provider_specific_fields={'native_finish_reason': 'completed'})], usage=Usage(completion_tokens=137, prompt_tokens=16189, total_tokens=16326, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None, image_tokens=0), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None, video_tokens=0), cost=0.02160625, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0.02023625, 'upstream_inference_completions_cost': 0.00137}), provider='OpenAI')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/markdown": [
       "`llms.txt` is a proposal for every website to publish a concise, LLM-friendly Markdown companion at `/llms.txt`. It gives models a curated overview, links to clean `.md` versions of key pages, and follows a predictable structure (H1 title, short summary, optional detail, then labeled link sections). The goal is to make it easy for language models to grab high-signal context without wrangling messy HTML, while staying compatible with existing web standards like `robots.txt` and sitemaps.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `gen-1763994333-LwPkgTQX9ikegpC1NQti`\n",
       "- model: `openai/gpt-5-codex`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=240, prompt_tokens=16189, total_tokens=16429, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=128, rejected_prediction_tokens=None, text_tokens=None, image_tokens=0), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=13440, text_tokens=None, image_tokens=None, video_tokens=0), cost=0.00751625, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0.00511625, 'upstream_inference_completions_cost': 0.0024})`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='gen-1763994333-LwPkgTQX9ikegpC1NQti', created=1763994333, model='openai/gpt-5-codex', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='`llms.txt` is a proposal for every website to publish a concise, LLM-friendly Markdown companion at `/llms.txt`. It gives models a curated overview, links to clean `.md` versions of key pages, and follows a predictable structure (H1 title, short summary, optional detail, then labeled link sections). The goal is to make it easy for language models to grab high-signal context without wrangling messy HTML, while staying compatible with existing web standards like `robots.txt` and sitemaps.', role='assistant', tool_calls=None, function_call=None, reasoning_content='**Preparing final summary**', provider_specific_fields={'refusal': None, 'reasoning': '**Preparing final summary**', 'reasoning_details': [{'format': 'openai-responses-v1', 'index': 0, 'type': 'reasoning.summary', 'summary': '**Preparing final summary**'}, {'id': 'rs_69246ade87e08190b658d3596ceebb66', 'format': 'openai-responses-v1', 'index': 0, 'type': 'reasoning.encrypted', 'data': 'gAAAAABpJGrmisTOHp0MQTlM6ZvMGeFvlGZxWDruUqKEfsaWP7QXZ5ctSP66qu1oot4nGymtMVbYu-twzsgwm4zTVWgcM57zAg1GH6v0UXJyV-ZFQnfmSZRqmAO61SMWdNBTgKGnjPiw92ST1T-pq5huNq4m1Nwz39sJgV5BOb0B9eMDzyYYhn8KQgc8c0wEJqwz9rr-z35V9GCOSEk72ouUW1Ob-v-EmUWRUtSg1HlS1FRIfhUr3pSv6bvnDAwkmVhueC07hbL8eaDnU_xITK6FLz6wEz7MZbjkXUAFx24kcqT9mbPsPbAepsQ_Yp8SSlDcbWmQWvHCdxMs0DMf9_pKqB2vR5ysvejmsA8Jbei2y7uQKXCfSPgblvZg3V_ld_feRFFmqdD-V044syExRMLysYCfP4jLzF35zrr4fy9vz9wXJ-VAT_NNhd-0T9VmTimZeq1uJL10pDnnPsQJBLjlZbN88_Yy5dN81ZcbExN7Df_Xwih5ZhJW3jUfNSpfCB4R9tZGEdqttrHGYLGlYY5K1OpVcJCXrUtDRnD3zKbSmxbrlwTG_InWgiygysyiC0WTTLxxprWYov722oyysz8wopQkNRIXOguCevzP4_Xa-z1MZTVwfQ8yg8he0V7Cvc1P-mGUCiz4Qdzb6TLqXY66qJwRi7pFvjo_wrQekKe6Ezo7q3sODT5mV7l8Aenpghs_KiarHWJl1ntplsDR4xrDUutcTQpONlZTwm1L-dP5qE6agMvoPV9Hfy6c7f52i_ZW6_lu67MBbcZiKWTRiS0esUx-ZXaMpdGFy8UZxhSXgfURIleXBkejUY_msm8CiUVsRSetyxeDyV_hCV_6OuIP5-3HMhBsjjhelJieBwefJKFd0XvRQ_BMJiEGZGaZY3pdqBkflFmIbXRobUG1zfo_pjRsdhFzcLnAJJxciulBCZXq3II7cZwS8EEEaXXCypD845xXSFntyY_ZPK6PO6LtEhMF61xxv9MHkbSD8k7NuRQAwQXke_eS2ZHAfHdQbmOtP8D19HwKbF_d_4QTApOeJBLX5kz8pN5ZdLUbuUcCMpSL-waJPDOzUfwYmj5U6hLDvTOUARqmzyYrQm86F1G2rRUkMKlvTMhY8xXRY5OWMTm_uLO_epUSc62rE_HenNK8gTvAbh83fR2VBeUIF8sewqBjuiVEVA9KHKeTsyptt0N6FuTI16ZqeWd11N6o_KjCX5whDicUzMM-X7UFu1kWlMUwRp0jDaRwQxaFEx5EbEj-fsrEFFK9BFw5YSvTjSjZuYUF5QsQW6Xlrg3grDDtcO0ksePicsgSj0wbL4W3aEHRskuvP7kWyyUxEMnA-a96jaVr65E5kMbxY4P8WjYCXEar2b07GUN5q5Bdtz-qBTbQuM38WRYBePb5nhjnSWy2Ai7Skm0G0OoYyWKW7WdN5j1N5FwmHV7SGzUAxVJH9Z1v43iO0BVQ9aI33bDR_0PvFELFsI0j5BcwvhUIg54QJ6fiP6Iy_qsDTWra--nRYitme47Qf0IIWTkUVjSvYkOWt9_r_T62L6QsdHjLmw2i3k8smEjub_hCST0Mq2Zz-e3ZTv5TWgcKdtLj1Y7u5BQjnrcZG1DCxDYWeBYkBJ6vRYqKn8rX3KB0pmujodX2VoLdUOUbw3-gPeV9eL-ogCxx1X2Rr0UKMxOSQw_0FOSog_0iNw6eFMYv-bkPBWrkU-pED3fUpy4='}], 'reasoning_content': '**Preparing final summary**'}), provider_specific_fields={'native_finish_reason': 'completed'})], usage=Usage(completion_tokens=240, prompt_tokens=16189, total_tokens=16429, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=128, rejected_prediction_tokens=None, text_tokens=None, image_tokens=0), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=13440, text_tokens=None, image_tokens=None, video_tokens=0), cost=0.00751625, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0.00511625, 'upstream_inference_completions_cost': 0.0024}), provider='OpenAI')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc(\"Can you use tools? For example can you read https://llmstxt.org/index.md and tell me about it? Fetch it, don't store it, give the elevator pitch please.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01806637",
   "metadata": {},
   "source": [
    "**Prompt (openrouter/openai/gpt-5-codex):** Can you use tools? For example can you read https://llmstxt.org/index.md and tell me about it? Fetch it, don't store it, give the elevator pitch please.\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "<details class='tool-usage-details'>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"call_lAFPpDr5RciSYwMGKlzHua8Y\",\n",
    "  \"call\": {\n",
    "    \"function\": \"read_url\",\n",
    "    \"arguments\": {\n",
    "      \"url\": \"https://llmstxt.org/index.md\",\n",
    "      \"as_md\": true,\n",
    "      \"extract_section\": false,\n",
    "      \"selector\": \"\"\n",
    "    }\n",
    "  },\n",
    "  \"result\": \"# The /llms.txt file\\nJeremy Howard\\n2024-09-03\\n\\n## Background\\n\\nLarge language models increasingly rel<TRUNCATED>\"\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "`llms.txt` is a proposal for every website to publish a concise, LLM-friendly Markdown companion at `/llms.txt`. It gives models a curated overview, links to clean `.md` versions of key pages, and follows a predictable structure (H1 title, short summary, optional detail, then labeled link sections). The goal is to make it easy for language models to grab high-signal context without wrangling messy HTML, while staying compatible with existing web standards like `robots.txt` and sitemaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2873526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "`fhdaisy` is a thin Python layer that brings DaisyUIâ€™s Tailwind-based component set directly into FastHTML apps. Instead of sprinkling HTML strings like `<button class=\"btn btn-primary\">`, you work with Python-first components (`Btn`, `Card`, `Alert`, etc.) that automatically apply the right DaisyUI classes and underlying HTML tags. Modifiers are shortened (`-primary`, `-outline`, `-sm`), yet expand to the full DaisyUI class list under the hood, keeping your UI code concise and readable. Extras in `fhdaisy.xtras` offer ready-made helpers for recurring UI patterns (accordions, forms, â€¦), while still letting you compose or extend components as needed.\n",
       "\n",
       "Typical flow:\n",
       "\n",
       "```python\n",
       "pip install fhdaisy\n",
       "```\n",
       "\n",
       "then:\n",
       "\n",
       "```python\n",
       "from fasthtml.common import *\n",
       "from fhdaisy import *\n",
       "\n",
       "page = Div(\n",
       "    Card(\n",
       "        CardBody(\n",
       "            H2(\"Welcome to FastHTML + DaisyUI\"),\n",
       "            P(\"This card is built from Python components, not raw HTML.\"),\n",
       "            Btn(\"Click me\", cls='-primary -sm')\n",
       "        ),\n",
       "        cls='-compact -shadow'\n",
       "    )\n",
       ")\n",
       "\n",
       "print(page)\n",
       "```\n",
       "\n",
       "The snippet yields well-structured HTML where each element already carries the expected DaisyUI classes, so you can focus on layout and behavior rather than CSS glue code. For a deeper diveâ€”including advanced components, helper utilities, and live demosâ€”check the projectâ€™s documentation site linked from the README.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `gen-1763994357-VoP1BBTW593bbYjKHcTL`\n",
       "- model: `openai/gpt-5-codex`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=631, prompt_tokens=18980, total_tokens=19611, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=320, rejected_prediction_tokens=None, text_tokens=None, image_tokens=0), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=17664, text_tokens=None, image_tokens=None, video_tokens=0), cost=0.010163, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0.003853, 'upstream_inference_completions_cost': 0.00631})`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='gen-1763994357-VoP1BBTW593bbYjKHcTL', created=1763994357, model='openai/gpt-5-codex', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='`fhdaisy` is a thin Python layer that brings DaisyUIâ€™s Tailwind-based component set directly into FastHTML apps. Instead of sprinkling HTML strings like `<button class=\"btn btn-primary\">`, you work with Python-first components (`Btn`, `Card`, `Alert`, etc.) that automatically apply the right DaisyUI classes and underlying HTML tags. Modifiers are shortened (`-primary`, `-outline`, `-sm`), yet expand to the full DaisyUI class list under the hood, keeping your UI code concise and readable. Extras in `fhdaisy.xtras` offer ready-made helpers for recurring UI patterns (accordions, forms, â€¦), while still letting you compose or extend components as needed.\\n\\nTypical flow:\\n\\n```python\\npip install fhdaisy\\n```\\n\\nthen:\\n\\n```python\\nfrom fasthtml.common import *\\nfrom fhdaisy import *\\n\\npage = Div(\\n    Card(\\n        CardBody(\\n            H2(\"Welcome to FastHTML + DaisyUI\"),\\n            P(\"This card is built from Python components, not raw HTML.\"),\\n            Btn(\"Click me\", cls=\\'-primary -sm\\')\\n        ),\\n        cls=\\'-compact -shadow\\'\\n    )\\n)\\n\\nprint(page)\\n```\\n\\nThe snippet yields well-structured HTML where each element already carries the expected DaisyUI classes, so you can focus on layout and behavior rather than CSS glue code. For a deeper diveâ€”including advanced components, helper utilities, and live demosâ€”check the projectâ€™s documentation site linked from the README.', role='assistant', tool_calls=None, function_call=None, reasoning_content=\"**Summarizing fhdaisy Python wrapper**\\n\\nI'll capture that fhdaisy is a Python wrapper bridging DaisyUI with FastHTML, offering component classes like Btn and Card, plus shorthand modifiers and extra form helpers. I'll include an install note, a concise code example, and point to documentationâ€”all without naming the tool calls or being Socratic.\", provider_specific_fields={'refusal': None, 'reasoning': \"**Summarizing fhdaisy Python wrapper**\\n\\nI'll capture that fhdaisy is a Python wrapper bridging DaisyUI with FastHTML, offering component classes like Btn and Card, plus shorthand modifiers and extra form helpers. I'll include an install note, a concise code example, and point to documentationâ€”all without naming the tool calls or being Socratic.\", 'reasoning_details': [{'format': 'openai-responses-v1', 'index': 0, 'type': 'reasoning.summary', 'summary': \"**Summarizing fhdaisy Python wrapper**\\n\\nI'll capture that fhdaisy is a Python wrapper bridging DaisyUI with FastHTML, offering component classes like Btn and Card, plus shorthand modifiers and extra form helpers. I'll include an install note, a concise code example, and point to documentationâ€”all without naming the tool calls or being Socratic.\"}, {'id': 'rs_69246af674dc81908731bb08f95305d0', 'format': 'openai-responses-v1', 'index': 0, 'type': 'reasoning.encrypted', 'data': 'gAAAAABpJGsA9C8OBajBvroSS_weOnK99JM_woGZwp83AAVhcoqNhUnZMPSc0jwWWeb4yd4q5C_9-LjQsyzif5E11Etx4CEVL-MYuSrOEhzR4NXBmRMV84TqsQhl9D5fw6HqhYcded_4pdVW3ZWVTmyD-p0RV4qaYlzT70yxSvhdeBHPDXRRjrFhbvNWFB2-bEg9DbUN_O7A9o7xdELPfJ-j5iX2LSpRcDEsxXkfCWvVNt9N26AD3Vs0IcJlRDEvtrBnYRM3IHk4rs9JWyAc0XwU1rf0v0zIfAjVB97MTzKXiiLW6-p27y8b4pzzxQMBHuNt9PUYX4qefxXnCIZ3znUxOcCvJknTs99NdTKWUcxGLRVysY9eTAb1GJnLi8ibhT_RgDfp-dqShBmRjiGt2vkNaIJUv7A-TWOnxlZwbthPv-lszKpHm4C17cEEHsU5GI5MfGuUvfDgjW0aFyWQQeUnUcFAitxoul5n4A5m3dCUoemoIcnVm21CiQjfWNlzW6S5hWaLtL1ok5Dc86NiBLLv_vegeG9NC4FuSf31PcY9e3yJFLWNhfOzAnGnRfiYMwmdm3lzf7o82J8ITzevVMdvuQ47AsKX9Zd0Uc4SOR9-E5W56lPBiX24F4duCxy7Zf3ivA7WAEYOfxmP0Qa96q21_Ppx6yEx5dH-t5SqOEIvXEP23MKBfkGW_Q0YP31MUugvwi9cOjfA2Tzyh2YRuoaEs2ZiPoZetkhfwVcH1YfJs7JHrs2um9TNfQX_VzSM0NAS0EmOOmvBkyJkhyAHR3tDprXP_HxJHS4mQAXX22WkeH_-ofB8cYzU7E5KEYJtCRPZYpbIRmt06GBd5E6IJ8zkB_I7ofFp657e-z3BO6nQDa5yk1zn2A2jK-6W4G1PwcjIoLM1W3FiqmtuVuGf25f_ronPumacMy1OI5ZuX3R-VvyVcsE0JIA2POYC1MwnQey0UDzb5uuhqcAvNP-cLxlDDon7AljKlS9lwqbk8IS1djhWJAM9ZNiPiFoaprUcNLxRvKnYC4MePBplQbPq3wkgcoZSZuaanl4XxWcEPjCC612p-7rOPDjoeTyG1dFx7JXAcRFQ3w1U5jMMgqw1_-qU2i0TLS-z6U8MWvrOreghWTLXxLwK6LH_MuhMFR_I4MNtDp3Yp70ErIHReB9sTQ6ru7rZK_5vBboSk4KYf62ezqImLFxyPUQqTUpNoMJM7c6Ao3w6ZcPN7AhTiAuA9Cu-CwVGpuc9yZOhNEA9q6eaOe28_a3bHFjHhHpqmBDngDfW3TK_K_9zRqfrqoWQSa2BNgx9XygBRzp2c5SIgCfwYhHz64EdBFEr6_WRKlrIjXDzuR38BWix5yRPt35Gi4f4QUIAIvGuntiBtrCmJEtp1XqWK7iqL3ong_ZXtzH9sJguZcIgQcIMa6xPKcru_Th29obHP5sVcAWgaBSkm6y3mIQDtLYusUsTkbvhwPpjff7qm3671s_ls7TAtpq1r90EIQK11GLjDGKIjx3QHYRgCys1klMzxRkMVGzLqcfXyhvwE-JpFmf3wxxPLMMOscMKlOYfGQd9AXrA_KCaY6Hj47TFjXTKWNbJG3Xt_gkEykxwK7EqeAimGCll2kEzrLltXpDixGXH7AzBz2ObEG69QDVIlhuaX6YpzWxPzVSVlj_mQDajAfqfAEKKzLWN7nlwC05JRtLDTHASNquoAQkODMqlJ7IFltsTQbtUiwqnSGvoCQfp2FZNASdnafp73_94tYgMVplRh-rtwvI7dETl4fwcP33Kq9mHX6EXsHXCBnibJv7yY-Z7pjUdY68XXCaVftX8jk7enOb0gtGLAM9TXA9eSuoL6LJckyOO52Bf20SFNklZvnZ8z4DKSM9BD13eE4xdgMjHK3aTg2KAITsRnKXm4MCrUxcMZb7szXdw3DIZFnUycgOOgqBTUzNUhT6CzDaUKzepwxEAJWGDXPjkMu7Rve03rYtFJILj6IOuzZaCnFy3Fr2qvip25Zchgvu69Ze43JrIG5Rp7m4qfqvQM5EnOjXJeurpK9Y_iTBocghJhWydEidWEKSRcMsDkWweuovH_JYJedHwQCsJQBNjwiVrJDaJ8qHAv_HKaVPhz-4hZ1E4iEvX6NA-nSOCkl-AVm0QRwIUbyRY79t7ZimEziH1Rta1xEjxT4xeJ6V4reH9mL3WaYtAYMtpQmxwWB2X1qP3_rk9nLBh9RgJ4ikCYT6W5_98N2eS0CVSZAepxiVoP8CIc8qfgs4oRT0It7silVxyIj1NQxN1kUwD-i8HCOj1qJaw0i8DiB8XhECKS5JdSkccy0DqEzAaLVSrxD_t20bfSb5W_TAwd6MISE0Pi2cmN9o27bD21a8mpWbNrOGK2jA_TF0FYUQs0Eg6NdJyaBUu8YPfOSTBHSRA_ewuLyKgkzLUCkuiZwTJVv2WZnASo3mN8atkSR7q2PjJ7etYUbKEM_RyTAQgqiMnDJnoa64qoHI6ruRAl5H4DDjTqYY0G4yeBRGfFkee9Lg_YlIGDBXwL560pPG95qovOnqv8PpZgFv0ANRnGekTA4A3F7nTd9cnt_DQ0kFPnFMsssW-m448hBIY45-JEz0LO-zlY1rWHR0CW9TPf99uzHPS9zPS4gl5xVUBhCZjvn1Mu7MC0_Kp7VwPFgXnjgQJs_vTlJvkilYixNbKC3Ffhs7Mjj97gmcU8cn_wJ88J9AXmBSzoDO3MSIfh88EPjPOAimz_n89abPnzavyinwBsKxgh5R-T6sjRi_bQNDEfh5Y7JkeNRSz6wJ-D-fU8TJIqzreXEMWvXWMkoPgzfEwtrg9MU7w07MWwAJKwal1gnwVyl1FNQTESZaerfDEQCotKi2zmwiclyH7QknAAV3w6fASlPOc50EprcjZbNvSoLPItStX_c7JuAr-Jpw2YqW6_XNk3v4bKg04drMBaG0N0iD3r64UKRiTryZG-dLMMo3Yhqzoq-m9GlMi7g-ixs0a62qBO8wUvU93Hi9piQw1iuiuiaTQJJUkNKsk'}], 'reasoning_content': \"**Summarizing fhdaisy Python wrapper**\\n\\nI'll capture that fhdaisy is a Python wrapper bridging DaisyUI with FastHTML, offering component classes like Btn and Card, plus shorthand modifiers and extra form helpers. I'll include an install note, a concise code example, and point to documentationâ€”all without naming the tool calls or being Socratic.\"}), provider_specific_fields={'native_finish_reason': 'completed'})], usage=Usage(completion_tokens=631, prompt_tokens=18980, total_tokens=19611, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=320, rejected_prediction_tokens=None, text_tokens=None, image_tokens=0), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=17664, text_tokens=None, image_tokens=None, video_tokens=0), cost=0.010163, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0.003853, 'upstream_inference_completions_cost': 0.00631}), provider='OpenAI')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc(\"Now use your tool and summarize https://raw.githubusercontent.com/AnswerDotAI/fhdaisy/refs/heads/main/README.md please - give a code example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ecc513",
   "metadata": {},
   "source": [
    "**Prompt (openrouter/openai/gpt-5-codex):** Now use your tool and summarize https://raw.githubusercontent.com/AnswerDotAI/fhdaisy/refs/heads/main/README.md please - give a code example\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "<details class='tool-usage-details'>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"call_7FvqavATiA7YIfVsneBjiPQH\",\n",
    "  \"call\": {\n",
    "    \"function\": \"read_url\",\n",
    "    \"arguments\": {\n",
    "      \"url\": \"https://raw.githubusercontent.com/AnswerDotAI/fhdaisy/refs/heads/main/README.md\",\n",
    "      \"as_md\": true,\n",
    "      \"extract_section\": false,\n",
    "      \"selector\": \"\"\n",
    "    }\n",
    "  },\n",
    "  \"result\": \"# fhdaisy\\n\\n`fhdaisy` is a Python wrapper for [DaisyUI](https://daisyui.com/) that\\nbrings its compone<TRUNCATED>\"\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "`fhdaisy` is a thin Python layer that brings DaisyUIâ€™s Tailwind-based component set directly into FastHTML apps. Instead of sprinkling HTML strings like `<button class=\"btn btn-primary\">`, you work with Python-first components (`Btn`, `Card`, `Alert`, etc.) that automatically apply the right DaisyUI classes and underlying HTML tags. Modifiers are shortened (`-primary`, `-outline`, `-sm`), yet expand to the full DaisyUI class list under the hood, keeping your UI code concise and readable. Extras in `fhdaisy.xtras` offer ready-made helpers for recurring UI patterns (accordions, forms, â€¦), while still letting you compose or extend components as needed.\n",
    "\n",
    "Typical flow:\n",
    "\n",
    "```python\n",
    "pip install fhdaisy\n",
    "```\n",
    "\n",
    "then:\n",
    "\n",
    "```python\n",
    "from fasthtml.common import *\n",
    "from fhdaisy import *\n",
    "\n",
    "page = Div(\n",
    "    Card(\n",
    "        CardBody(\n",
    "            H2(\"Welcome to FastHTML + DaisyUI\"),\n",
    "            P(\"This card is built from Python components, not raw HTML.\"),\n",
    "            Btn(\"Click me\", cls='-primary -sm')\n",
    "        ),\n",
    "        cls='-compact -shadow'\n",
    "    )\n",
    ")\n",
    "\n",
    "print(page)\n",
    "```\n",
    "\n",
    "The snippet yields well-structured HTML where each element already carries the expected DaisyUI classes, so you can focus on layout and behavior rather than CSS glue code. For a deeper diveâ€”including advanced components, helper utilities, and live demosâ€”check the projectâ€™s documentation site linked from the README."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
